"use client"

import { ChatbotUIContext } from "@/context/context"
import useHotkey from "@/lib/hooks/use-hotkey"
import { useVoiceRecorder } from "./chat-hooks/use-voice-recorder"
import { LLM_LIST } from "@/lib/models/llm/llm-list"
import { cn } from "@/lib/utils"
import { Tables } from "@/supabase/types"
import {
  IconCirclePlus,
  IconLoader2,
  IconMicrophone,
  IconPlayerRecordFilled,
  IconPlayerStopFilled,
  IconSend
} from "@tabler/icons-react"
import dynamic from "next/dynamic"
import { FC, useCallback, useContext, useEffect, useRef, useState } from "react"
import { toast } from "sonner"
import { Input } from "../ui/input"
import { TextareaAutosize } from "../ui/textarea-autosize"
import { useChatHandler } from "./chat-hooks/use-chat-handler"
import { usePromptAndCommand } from "./chat-hooks/use-prompt-and-command"
import { useSelectFileHandler } from "./chat-hooks/use-select-file-handler"

const ChatCommandInput = dynamic(() =>
  import("./chat-command-input").then(mod => mod.ChatCommandInput)
)
const ChatFilesDisplay = dynamic(() =>
  import("./chat-files-display").then(mod => mod.ChatFilesDisplay)
)
const SelectedAssistant = dynamic(() =>
  import("./selected-assistant").then(mod => mod.SelectedAssistant)
)
const SelectedTools = dynamic(() =>
  import("./selected-tools").then(mod => mod.SelectedTools)
)

interface ChatInputProps {}

export const ChatInput: FC<ChatInputProps> = ({}) => {
  const [isTyping, setIsTyping] = useState<boolean>(false)
  const [isTranscribing, setIsTranscribing] = useState<boolean>(false)
  const [isMobile, setIsMobile] = useState(false)
  useEffect(() => {
    // Ø§ÛŒÙ† Ú©Ø¯ Ø¯Ø± Ø²Ù…Ø§Ù† Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†ØªØŒ Ù†ÙˆØ¹ Ø¯Ø³ØªÚ¯Ø§Ù‡ Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
    const userAgent =
      typeof window.navigator === "undefined" ? "" : navigator.userAgent
    const mobile = /Mobi|Android|iPhone|iPad|iPod/i.test(userAgent)
    setIsMobile(mobile)
  }, [])
  const {
    userInput,
    chatMessages,
    isGenerating,
    selectedPreset,
    selectedAssistant,
    isPromptPickerOpen,
    setIsPromptPickerOpen,
    isFilePickerOpen,
    isToolPickerOpen,
    isAssistantPickerOpen,
    chatSettings,
    newMessageImages,
    newMessageFiles
  } = useContext(ChatbotUIContext)

  const { handleInputChange } = usePromptAndCommand()
  const {
    chatInputRef,
    handleSendMessage,
    handleStopMessage,
    handleFocusChatInput,
    setChatMessages
  } = useChatHandler()
  console.log("ChatInput render - chatSettings.model:", chatSettings?.model)
  // âœ¨ ØªØ§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯ Ùˆ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ùˆ Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ ØµÙˆØªÛŒ
  const handleVoiceSubmit = async (audioBlob: Blob) => {
    const selectedModel = chatSettings?.model
    setIsTranscribing(true)

    // Ø³Ù†Ø§Ø±ÛŒÙˆ Û²: Ø§Ú¯Ø± Ù…Ø¯Ù„ "Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ" Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯
    if (selectedModel === "gpt-4o-transcribe") {
      const audioUrl = URL.createObjectURL(audioBlob)

      const userAudioMessage: Tables<"messages"> = {
        id: crypto.randomUUID(),
        chat_id: chatMessages[0]?.message.chat_id || "",
        user_id: chatMessages[0]?.message.user_id || "",
        assistant_id: null,
        content: audioUrl,
        file_url: null,
        created_at: new Date().toISOString(),
        updated_at: new Date().toISOString(),
        image_paths: [],
        model: "user-audio",
        role: "user",
        sequence_number: chatMessages.length
      }
      setChatMessages(prev => [
        ...prev,
        { message: userAudioMessage, fileItems: [] }
      ])

      const formData = new FormData()
      formData.append("file", audioBlob, "recording.webm")
      const response = await fetch("/api/transcribe", {
        method: "POST",
        body: formData
      })
      const result = await response.json()

      if (response.ok && result.text) {
        const assistantTextMessage: Tables<"messages"> = {
          id: crypto.randomUUID(),
          chat_id: chatMessages[0]?.message.chat_id || "",
          file_url: null,
          user_id: chatMessages[0]?.message.user_id || "",
          assistant_id: null,
          content: result.text,
          created_at: new Date().toISOString(),
          image_paths: [],
          model: selectedModel,
          role: "assistant",
          sequence_number: chatMessages.length + 1,
          updated_at: new Date().toISOString()
        }
        setChatMessages(prev => [
          ...prev,
          { message: assistantTextMessage, fileItems: [] }
        ])
      } else {
        toast.error(result.message || "Ø®Ø·Ø§ Ø¯Ø± Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ ØµØ¯Ø§")
      }
    }
    // Ø³Ù†Ø§Ø±ÛŒÙˆ Û±: Ø¨Ø±Ø§ÛŒ Ø¨Ù‚ÛŒÙ‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ (Ø§Ø±Ø³Ø§Ù„ Ø®ÙˆØ¯Ú©Ø§Ø±)
    else {
      const formData = new FormData()
      formData.append("file", audioBlob, "recording.webm")
      const response = await fetch("/api/transcribe", {
        method: "POST",
        body: formData
      })
      const result = await response.json()

      if (response.ok && result.text) {
        handleSendMessage(result.text, chatMessages, false)
      } else {
        toast.error(result.message || "Ø®Ø·Ø§ Ø¯Ø± Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ ØµØ¯Ø§")
      }
    }
    setIsTranscribing(false)
  }

  const { isRecording, handleToggleRecording } =
    useVoiceRecorder(handleVoiceSubmit)

  useHotkey("l", () => handleFocusChatInput())
  const { handleSelectFile, filesToAccept } = useSelectFileHandler()
  const fileInputRef = useRef<HTMLInputElement>(null)

  useEffect(() => {
    // ðŸ‘‡ ==== Ø§ØµÙ„Ø§Ø­ Ø§ØµÙ„ÛŒ Ø§ÛŒÙ†Ø¬Ø§Ø³Øª ==== ðŸ‘‡
    // ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±ØªÛŒ ÙÙˆÚ©ÙˆØ³ Ú©Ù† Ú©Ù‡ Ø¹Ø±Ø¶ ØµÙØ­Ù‡ Ø¨Ø²Ø±Ú¯ØªØ± Ø§Ø² 768 Ù¾ÛŒÚ©Ø³Ù„ (Ø¯Ø³Ú©ØªØ§Ù¾) Ø¨Ø§Ø´Ø¯
    if (window.innerWidth > 768) {
      const timer = setTimeout(() => {
        handleFocusChatInput()
      }, 200)
      return () => clearTimeout(timer)
    }
    // ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ ØªØºÛŒÛŒØ± Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯
  }, [selectedPreset, selectedAssistant, handleFocusChatInput])

  const handleKeyDown = useCallback(
    (event: React.KeyboardEvent) => {
      if (
        !isMobile && // <--- Ø§ÛŒÙ† Ø´Ø±Ø· Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª
        !isTyping &&
        event.key === "Enter" &&
        !event.shiftKey &&
        !isRecording
      ) {
        event.preventDefault()
        setIsPromptPickerOpen(false)
        if (userInput) handleSendMessage(userInput, chatMessages, false)
      }
    },
    [
      isMobile, // <--- isMobile Ø±Ø§ Ø¨Ù‡ Ø§ÛŒÙ†Ø¬Ø§ Ù‡Ù… Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
      isTyping,
      userInput,
      chatMessages,
      isRecording,
      handleSendMessage,
      setIsPromptPickerOpen
    ]
  )

  const handlePaste = useCallback(
    (event: React.ClipboardEvent) => {
      const imagesAllowed = LLM_LIST.find(
        llm => llm.modelId === chatSettings?.model
      )?.imageInput

      const items = event.clipboardData.items
      for (const item of items) {
        if (item.type.indexOf("image") === 0) {
          if (!imagesAllowed) {
            toast.error(`Images are not supported for this model.`)
            return
          }
          const file = item.getAsFile()
          if (file) handleSelectFile(file)
        }
      }
    },
    [chatSettings?.model, handleSelectFile]
  )

  const showCommandInput =
    isPromptPickerOpen ||
    isFilePickerOpen ||
    isToolPickerOpen ||
    isAssistantPickerOpen

  return (
    <>
      <div className="flex flex-col flex-wrap justify-center gap-2">
        {(newMessageFiles.length > 0 || newMessageImages.length > 0) && (
          <ChatFilesDisplay />
        )}
        {selectedAssistant && <SelectedAssistant />}
      </div>

      <div className="border-input relative flex min-h-[60px] w-full items-center rounded-xl border-2 pb-[env(safe-area-inset-bottom)]">
        {showCommandInput && (
          <div className="absolute bottom-[76px] left-0 max-h-[300px] w-full overflow-auto rounded-xl dark:border-none">
            <ChatCommandInput />
          </div>
        )}

        <>
          <IconCirclePlus
            className="absolute bottom-2.5 left-3 cursor-pointer p-1 hover:opacity-50"
            size={32}
            onClick={() => fileInputRef.current?.click()}
          />
          <Input
            ref={fileInputRef}
            className="hidden"
            type="file"
            onChange={e => {
              if (!e.target.files) return
              const file = e.target.files[0]

              // Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø§Ø³Øª ÛŒØ§ Ù†Ù‡
              if (file.type.startsWith("audio/")) {
                // Ø§Ú¯Ø± ØµÙˆØªÛŒ Ø¨ÙˆØ¯ØŒ Ø¨Ø±Ø§ÛŒ Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯
                handleVoiceSubmit(file)
              } else {
                // Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±ØªØŒ Ù…Ø§Ù†Ù†Ø¯ Ù‚Ø¨Ù„ Ø¨Ø±Ø§ÛŒ Ø§Ù„ØµØ§Ù‚ ÙØ§ÛŒÙ„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´ÙˆØ¯
                handleSelectFile(file)
              }

              // input Ø±Ø§ Ø®Ø§Ù„ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø¨ØªÙˆØ§Ù† Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù‡Ù…Ø§Ù† ÙØ§ÛŒÙ„ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±Ø¯
              e.target.value = ""
            }}
            // Ù„ÛŒØ³Øª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§Ø² Ø±Ø§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú©Ù†ÛŒØ¯
            accept={filesToAccept + ",audio/*"}
          />
        </>

        <TextareaAutosize
          textareaRef={chatInputRef}
          className="font-vazir ring-offset-background placeholder:text-muted-foreground focus-visible:ring-ring text-md flex w-full min-w-0 resize-none rounded-md border-none bg-transparent py-3 pl-14 pr-24 focus-visible:outline-none disabled:cursor-not-allowed disabled:opacity-50"
          placeholder="Ù¾ÛŒØ§Ù… Ø®ÙˆØ¯ Ø±Ø§ ØªØ§ÛŒÙ¾ ÛŒØ§ Ø¶Ø¨Ø· Ú©Ù†ÛŒØ¯..."
          onValueChange={handleInputChange}
          value={userInput}
          minRows={1}
          maxRows={18}
          onKeyDown={handleKeyDown}
          onPaste={handlePaste}
          onCompositionStart={() => setIsTyping(true)}
          onCompositionEnd={() => setIsTyping(false)}
        />

        <div className="absolute bottom-2.5 right-12">
          {isTranscribing ? (
            <IconLoader2
              className="text-muted-foreground animate-spin"
              size={28}
            />
          ) : isRecording ? (
            <IconPlayerRecordFilled
              className="cursor-pointer rounded p-1 text-red-500 hover:opacity-80"
              size={28}
              onClick={handleToggleRecording}
            />
          ) : (
            <IconMicrophone
              className="bg-primary text-secondary cursor-pointer rounded p-1 hover:opacity-80"
              size={28}
              onClick={handleToggleRecording}
            />
          )}
        </div>

        <div className="absolute bottom-2.5 right-3">
          {isGenerating ? (
            <IconPlayerStopFilled
              className="hover:bg-background animate-pulse cursor-pointer rounded bg-transparent p-1"
              onClick={handleStopMessage}
              size={30}
            />
          ) : (
            <IconSend
              className={cn(
                "bg-primary text-secondary rounded p-1",
                !userInput || isRecording
                  ? "cursor-not-allowed opacity-50"
                  : "cursor-pointer"
              )}
              onClick={() => {
                if (!userInput || isRecording) return
                handleSendMessage(userInput, chatMessages, false)
              }}
              size={28}
            />
          )}
        </div>
      </div>
    </>
  )
}
