"use client"

import { FC, useState, useRef, useCallback, useEffect } from "react"
import { cn } from "@/lib/utils"
import { toast } from "sonner"
import { motion, AnimatePresence } from "framer-motion"
import { CircularAudioVisualizer } from "./CircularAudioVisualizer"
import { supabase } from "@/lib/supabase/client"

interface VoiceUIProps {
  chatSettings: any
}

// âœ¨ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ø®Ø·Ø§Ù‡Ø§ÛŒ import Ùˆ Ù…Ø³ØªÙ‚Ù„ Ø´Ø¯Ù† Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øª
const useAudioVisualizer = (stream: MediaStream | null) => {
  const [volume, setVolume] = useState(0)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyzerRef = useRef<AnalyserNode | null>(null)
  const dataArrayRef = useRef<Uint8Array | null>(null)

  useEffect(() => {
    if (!stream) {
      if (audioContextRef.current) {
        audioContextRef.current.close()
        audioContextRef.current = null
      }
      return
    }

    if (!audioContextRef.current) {
      const audioContext = new (window.AudioContext ||
        (window as any).webkitAudioContext)()
      const analyzer = audioContext.createAnalyser()
      analyzer.fftSize = 256
      const source = audioContext.createMediaStreamSource(stream)
      source.connect(analyzer)
      analyzerRef.current = analyzer
      dataArrayRef.current = new Uint8Array(analyzer.frequencyBinCount)
      audioContextRef.current = audioContext
    }
    const analyze = () => {
      const analyzer = analyzerRef.current
      const dataArray = dataArrayRef.current

      if (analyzer && dataArray) {
        // Ø§ÛŒØ¬Ø§Ø¯ Uint8Array ÙˆØ§Ù‚Ø¹ÛŒ Ø±ÙˆÛŒ ArrayBuffer Ø¬Ø¯ÛŒØ¯
        const buffer = new ArrayBuffer(dataArray.length)
        const typedArray = new Uint8Array(buffer)
        typedArray.set(dataArray) // Ú©Ù¾ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Uint8Array Ø¬Ø¯ÛŒØ¯

        analyzer.getByteFrequencyData(typedArray)

        const sum = typedArray.reduce((a, b) => a + b, 0)
        const avg = sum / typedArray.length
        setVolume(avg)
      }

      requestAnimationFrame(analyze)
    }

    analyze()

    return () => {
      if (audioContextRef.current) {
        audioContextRef.current.close()
        audioContextRef.current = null
      }
    }
  }, [stream])

  return volume
}

const getUserAccessToken = async (): Promise<string | null> => {
  try {
    // 2. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©Ù„Ø§ÛŒÙ†Øª Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ø´Ø¯Ù‡
    const {
      data: { session },
      error
    } = await supabase.auth.getSession()

    if (error) {
      console.error("Supabase getSession error:", error)
      return null
    }

    if (session) {
      return session.access_token
    }

    return null
  } catch (err) {
    console.error("Error fetching user token:", err)
    return null
  }
}
export const VoiceUI: FC<VoiceUIProps> = ({ chatSettings }) => {
  const [status, setStatus] = useState<"idle" | "connecting" | "connected">(
    "idle"
  )

  const dataChannelRef = useRef<RTCDataChannel | null>(null)

  const [userStream, setUserStream] = useState<MediaStream | null>(null)
  const [modelStream, setModelStream] = useState<MediaStream | null>(null)

  const peerConnectionRef = useRef<RTCPeerConnection | null>(null)

  const userVolume = useAudioVisualizer(userStream)
  const modelVolume = useAudioVisualizer(modelStream)
  const combinedVolume = Math.max(userVolume, modelVolume)

  const stopRealtime = useCallback(() => {
    if (
      dataChannelRef.current &&
      dataChannelRef.current.readyState === "open"
    ) {
      console.log("â¡ï¸ Sending session.terminate event to OpenAI...")
      dataChannelRef.current.send(JSON.stringify({ type: "session.terminate" }))
    }

    if (dataChannelRef.current) {
      dataChannelRef.current.close()
      dataChannelRef.current = null
    }

    if (peerConnectionRef.current) {
      peerConnectionRef.current.close()
      peerConnectionRef.current = null
    }

    if (userStream) {
      userStream.getTracks().forEach(track => track.stop())
      setUserStream(null)
    }

    if (modelStream) {
      modelStream.getTracks().forEach(track => track.stop())
      setModelStream(null)
    }

    setStatus("idle")
    console.log("ğŸ›‘ Realtime session stopped")
  }, [userStream, modelStream])

  const startRealtime = useCallback(
    async (model: string) => {
      setStatus("connecting")

      // âœ¨ Ù…ØªØºÛŒØ± sessionData Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø³Ø·Ø­ scope ØªØ§Ø¨Ø¹ ØªØ¹Ø±ÛŒÙ Ø´ÙˆØ¯
      // ØªØ§ Ù‡Ù… Ø¯Ø± Ø¨Ù„Ø§Ú© try Ùˆ Ù‡Ù… Ø¯Ø± dc.onmessage Ù‚Ø§Ø¨Ù„ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ø§Ø´Ø¯
      let sessionData: any = null

      try {
        // 1. Ø§ÙˆÙ„ ØªÙˆÚ©Ù† Ø±Ø§ Ø¨Ú¯ÛŒØ±
        const token = await getUserAccessToken()
        if (!token) {
          throw new Error("User not authenticated. Missing access token.")
        }

        // 2. Ø­Ø§Ù„Ø§ Ø¨Ø§ Ø³Ø±ÙˆØ± Ø®ÙˆØ¯Øª ØªÙ…Ø§Ø³ Ø¨Ú¯ÛŒØ± ØªØ§ session Ø±Ø§ Ø¨Ø³Ø§Ø²ÛŒ
        //    (Ø§ÛŒÙ† Ù‡Ù…Ø§Ù† Ú©Ø¯ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø´Ù…Ø§ Ø¨Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ Ù¾Ø§Ú© Ú©Ø±Ø¯Ù‡ Ø¨ÙˆØ¯ÛŒØ¯)
        const res = await fetch("/api/chat/openai", {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
            Authorization: `Bearer ${token}` // âœ¨ ØªÙˆÚ©Ù† Ø§ÛŒÙ†Ø¬Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
          },
          body: JSON.stringify({ chatSettings: { model } })
        })

        if (!res.ok) {
          const errorData = await res.json()
          throw new Error(errorData.message || "Failed to get ephemeral key.")
        }

        // 3. Ø­Ø§Ù„Ø§ sessionData Ø±Ø§ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ú©Ù†
        sessionData = await res.json()

        // 4. Ø­Ø§Ù„Ø§ Ú©Ù‡ sessionData Ø±Ø§ Ø¯Ø§Ø±ÛŒÙ…ØŒ WebRTC Ø±Ø§ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ù†
        const pc = new RTCPeerConnection()
        peerConnectionRef.current = pc

        pc.ontrack = e => {
          console.log("ğŸ”Š Remote audio track received:", e.streams)
          setModelStream(e.streams[0])

          const audioEl = document.createElement("audio")
          audioEl.srcObject = e.streams[0]
          audioEl.autoplay = true
          audioEl.setAttribute("playsinline", "true")

          document.body.appendChild(audioEl)

          audioEl
            .play()
            .then(() => {
              console.log("ğŸ”Š Model audio playing...")
            })
            .catch(err => {
              console.error("ğŸš¨ Autoplay blocked:", err)
            })
        }

        const dc = pc.createDataChannel("oai-events")
        dataChannelRef.current = dc
        dc.onopen = () => {
          console.log("ğŸ“¡ DataChannel opened:", dc.label)
        }

        const buffers = new Map<string, string>()

        // 5. Ø­Ø§Ù„Ø§ onmessage Ø±Ø§ ØªØ¹Ø±ÛŒÙ Ú©Ù†
        //    (Ú†ÙˆÙ† sessionData Ø¯Ø± scope Ø¨Ø§Ù„Ø§ØªØ± ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ØŒ Ø§ÛŒÙ†Ø¬Ø§ Ù‚Ø§Ø¨Ù„ Ø¯Ø³ØªØ±Ø³ÛŒ Ø§Ø³Øª)
        dc.onmessage = async msg => {
          const data = JSON.parse(msg.data)
          // console.log("ğŸ“© RAW event:", data)

          if (data.type === "response.function_call_arguments.delta") {
            // ... (Ú©Ø¯ Ø´Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…Ø´Ú©Ù„ÛŒ Ù†Ø¯Ø§Ø´Øª)
            const id = data.tool_call_id || data.item_id
            if (!id) {
              console.warn("âš ï¸ No tool_call_id or item_id in delta:", data)
              return
            }
            console.log("ğŸ†” Using buffer id:", id, " | delta:", data.delta)
            const prev = buffers.get(id) ?? ""
            buffers.set(id, prev + (data.delta ?? ""))
            console.log("âœï¸ Partial buffer for", id, ":", buffers.get(id))
          }

          if (data.type === "response.function_call_arguments.done") {
            // ... (Ú©Ø¯ Ø´Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…Ø´Ú©Ù„ÛŒ Ù†Ø¯Ø§Ø´Øª)
            const id = data.tool_call_id || data.item_id
            if (!id) {
              console.warn("âš ï¸ No tool_call_id or item_id in done:", data)
              return
            }
            console.log("ğŸ†” Finalizing buffer for id:", id)
            const buffer = buffers.get(id) ?? ""
            buffers.delete(id)
            console.log("âœ… Final buffer (raw):", buffer)
            if (!buffer.startsWith("{") || !buffer.endsWith("}")) {
              console.warn("âš ï¸ Incomplete JSON, skipping:", buffer)
              return
            }
            try {
              const args = JSON.parse(buffer)
              const query = args.query
              console.log("ğŸ” Search requested:", query)
              if (!query) return
              console.log("ğŸŒ Sending query to /api/chat/search ...")
              const searchRes = await fetch("/api/chat/search", {
                method: "POST",
                headers: { "Content-Type": "application/json" },
                body: JSON.stringify({ query })
              })
              console.log("ğŸŒ Got response, status:", searchRes.status)
              const data = await searchRes.json()
              console.log("ğŸ“¥ Search API raw response:", data)
              const textResult = data.output_text ?? "No result found."
              let payload
              if (data.tool_call_id) {
                payload = {
                  type: "response.create",
                  response: { conversation: "auto", instructions: textResult }
                }
              } else {
                payload = {
                  type: "response.create",
                  response: { conversation: "auto", instructions: textResult }
                }
              }
              console.log(
                "ğŸ“¦ Payload to realtime:",
                JSON.stringify(payload, null, 2)
              )
              dc.send(JSON.stringify(payload))
              console.log("âœ… Sent results back to model")
            } catch (err) {
              console.error("âŒ Error parsing JSON buffer:", buffer, err)
            }
          }

          // 6. Ø§ÛŒÙ† Ø¨Ù„Ø§Ú© "done" (Ø§Ø±Ø³Ø§Ù„ usage) Ø§Ø³Øª
          if (data.type === "response.done" && data.response?.usage) {
            const usage = data.response.usage // <-- 'usage' Ø§ÛŒÙ†Ø¬Ø§ ØªØ¹Ø±ÛŒÙ Ù…ÛŒâ€ŒØ´ÙˆØ¯
            console.log(`ğŸ” Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÙˆÚ©Ù† Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù¾Ø§Ø³Ø®:`)
            console.log(`- ÙˆØ±ÙˆØ¯ÛŒ: ${usage.input_tokens} ØªÙˆÚ©Ù†`)
            console.log(`- Ø®Ø±ÙˆØ¬ÛŒ: ${usage.output_tokens} ØªÙˆÚ©Ù†`)

            // âœ¨ 7. Ø§ÛŒÙ†Ø¬Ø§ Ø±Ø§Ù‡â€ŒØ­Ù„ Ù‚Ø¨Ù„ÛŒ (Ú¯Ø±ÙØªÙ† ØªÙˆÚ©Ù† ØªØ§Ø²Ù‡) Ø±Ø§ Ø§Ø¹Ù…Ø§Ù„ Ú©Ù†
            try {
              const currentToken = await getUserAccessToken() // <-- ØªÙˆÚ©Ù† ØªØ§Ø²Ù‡
              if (!currentToken) {
                console.error(
                  "âŒ Could not get user token before sending usage data."
                )
                throw new Error("Missing user token for usage report.")
              }

              // Ù†Ø§Ù… Ù…ØªØºÛŒØ± Ø±Ø§ Ø¹ÙˆØ¶ Ú©Ù† Ú©Ù‡ Ø¨Ø§ res Ø¨Ø§Ù„Ø§ ØªØ¯Ø§Ø®Ù„ Ù†Ú©Ù†Ø¯
              const webhookRes = await fetch("/api/webhooks/openai-realtime/", {
                method: "POST",
                headers: {
                  "Content-Type": "application/json",
                  Authorization: `Bearer ${currentToken}` // <-- ØªÙˆÚ©Ù† ØªØ§Ø²Ù‡
                },
                body: JSON.stringify({
                  openaiSessionId: sessionData.id, // <-- sessionData Ø­Ø§Ù„Ø§ Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª
                  modelId: chatSettings.model,
                  usage: usage // <-- usage Ø­Ø§Ù„Ø§ Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª
                })
              })

              if (!webhookRes.ok) {
                console.error("âŒ Error sending usage data to temporary API.")
              }
            } catch (error) {
              console.error("âŒ Network error sending usage data:", error)
            }
          }
        } // Ù¾Ø§ÛŒØ§Ù† dc.onmessage

        pc.onconnectionstatechange = () => {
          console.log("âš¡ Connection state:", pc.connectionState)
          if (
            ["disconnected", "failed", "closed"].includes(pc.connectionState)
          ) {
            stopRealtime()
          }
        }

        const ms = await navigator.mediaDevices.getUserMedia({
          audio: {
            noiseSuppression: true,
            echoCancellation: true
          }
        })
        console.log("ğŸ¤ Local stream obtained:", ms)
        setUserStream(ms)

        ms.getAudioTracks().forEach(track => {
          console.log("ğŸ¤ Sending audio track:", track.label, track.readyState)
          pc.addTrack(track, ms)
        })

        console.log("ğŸ¤ Local stream tracks:", ms.getTracks())

        const offer = await pc.createOffer()
        await pc.setLocalDescription(offer)

        // 8. Ø­Ø§Ù„Ø§ Ø§Ø² sessionData Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        const EPHEMERAL_KEY = sessionData.client_secret?.value
        const sdpResponse = await fetch(
          `https://api.openai.com/v1/realtime?model=${model}`,
          {
            method: "POST",
            body: offer.sdp,
            headers: {
              Authorization: `Bearer ${EPHEMERAL_KEY}`,
              "Content-Type": "application/sdp"
            }
          }
        )

        if (!sdpResponse.ok) {
          throw new Error(`SDP exchange failed: ${sdpResponse.statusText}`)
        }

        const answer: RTCSessionDescriptionInit = {
          type: "answer",
          sdp: await sdpResponse.text()
        }
        await pc.setRemoteDescription(answer)
        setStatus("connected")
        console.log("âœ… Realtime session started")
      } catch (error) {
        console.error(
          `âŒ Could not start voice chat: ${
            error instanceof Error ? error.message : "Unknown error"
          }`
        )
        stopRealtime()
      }
    },
    [stopRealtime, chatSettings]
  )

  const handleIconClick = () => {
    if (status === "idle") {
      startRealtime(chatSettings.model)
    } else {
      stopRealtime()
    }
  }

  return (
    <>
      <AnimatePresence>
        {status === "connected" && (
          <motion.div
            onClick={handleIconClick} // Ø¨Ø§ Ú©Ù„ÛŒÚ© Ø±ÙˆÛŒ Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ù‡Ù… Ø¨Ø³ØªÙ‡ Ø´ÙˆØ¯
            className="fixed inset-0 z-50 flex cursor-pointer flex-col items-center justify-center bg-gray-900/50 backdrop-blur-sm"
            initial={{ opacity: 0 }}
            animate={{ opacity: 1 }}
            exit={{ opacity: 0 }}
            transition={{ duration: 0.3 }}
          >
            {/* Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øª Ø¬Ø¯ÛŒØ¯ ÙˆÛŒÚ˜ÙˆØ§Ù„Ø§ÛŒØ²Ø± */}
            <CircularAudioVisualizer volume={combinedVolume} />

            <p className="font-vazir mt-12 text-lg text-white">
              Ø¯Ø± Ø­Ø§Ù„ Ø´Ù†ÛŒØ¯Ù†...
            </p>
            <p className="font-vazir mt-2 text-sm text-gray-400">
              Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ§Ù† Ø¯Ø§Ø¯Ù† Ø¨Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯
            </p>
          </motion.div>
        )}
      </AnimatePresence>

      {/* Ø¨Ø®Ø´ UI Ø¯Ú©Ù…Ù‡ Ø§ÙˆÙ„ÛŒÙ‡ (ÙˆÙ‚ØªÛŒ status !== 'connected') */}
      {status !== "connected" && (
        <div className="fixed bottom-12 left-1/2 z-50 flex -translate-x-1/2 flex-col items-center">
          <div
            onClick={handleIconClick}
            className={cn(
              "relative flex cursor-pointer items-center justify-center rounded-full transition-all duration-500",
              "bg-gradient-to-br from-[#4facfe] to-[#8e2de2] text-white",
              "shadow-[0_8px_16px_rgba(0,0,0,0.3)] dark:shadow-[0_8px_16px_rgba(0,0,0,0.5)]",
              "size-20"
            )}
          >
            {status === "connecting" ? (
              <svg /* ... SVG
                icon ... */
              ></svg>
            ) : (
              "â€¢â€¢â€¢â€¢"
            )}
          </div>
          <p className="font-vazir mt-3 text-sm text-white">
            {status === "idle" && "Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ ØµØ­Ø¨Øª Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯"}
            {status === "connecting" && "Ø¯Ø± Ø­Ø§Ù„ Ø§ØªØµØ§Ù„..."}
          </p>
        </div>
      )}
    </>
  )
}
