"use client"

import React, { FC, useState, useRef, useCallback, useEffect } from "react"
import { cn } from "@/lib/utils" // (cn) Ø´Ù…Ø§ Ø§Ø² Ù‚Ø¨Ù„ Ø¯Ø± Ù¾Ø±ÙˆÚ˜Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
import { toast, Toaster } from "sonner" // Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø®Ø·Ø§Ù‡Ø§
import { motion, AnimatePresence } from "framer-motion"

const remoteLog = (message: string) => {
  // Ù„Ø§Ú¯ Ø¯Ø± Ú©Ù†Ø³ÙˆÙ„ Ù…Ø­Ù„ÛŒ (Ø§Ú¯Ø± inspect Ø´Ø§Ù†Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø² Ø´Ø¯Ù† Ø¯Ø§Ø´Øª)
  console.log(message)

  // Ø§Ø±Ø³Ø§Ù„ Ù„Ø§Ú¯ Ø¨Ù‡ Ø³Ø±ÙˆØ± Vercel
  fetch("/api/log", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    // Ù…Ø§ Ù¾ÛŒØ§Ù… Ø±Ø§ Ø¨Ø§ [VoiceUI] Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø¯Ø± Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Vercel Ù…Ø´Ø®Øµ Ø¨Ø§Ø´Ø¯
    body: JSON.stringify({ message: `[VoiceUI] ${message}` })
  }).catch(err => console.error("Remote log failed:", err)) // Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„ Ù„Ø§Ú¯ Ø±Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
}

const CircularAudioVisualizer: FC<{ volume: number }> = ({ volume }) => {
  // Ø­Ø¬Ù… ØµØ¯Ø§ Ø±Ø§ Ø¨Ù‡ ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± Ù„Ú¯Ø§Ø±ÛŒØªÙ…ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ù†ÙˆØ³Ø§Ù† Ø²ÛŒØ¨Ø§ØªØ± Ø¨Ø§Ø´Ø¯
  const scale = Math.log(1 + volume * 2) * 0.5 + 1
  const opacity = Math.min(volume / 50, 0.5)

  return (
    <div className="relative flex size-64 items-center justify-center">
      {/* Ù‡Ø§Ù„Ù‡ Ø¨ÛŒØ±ÙˆÙ†ÛŒ */}
      <motion.div
        className="absolute size-full rounded-full border border-blue-500/30 bg-blue-500/10"
        animate={{
          scale: scale * 1.1,
          opacity: opacity * 0.8
        }}
        transition={{ duration: 0.1, ease: "easeOut" }}
      />
      {/* Ù‡Ø§Ù„Ù‡ Ù…ÛŒØ§Ù†ÛŒ */}
      <motion.div
        className="absolute size-48 rounded-full border border-blue-500/50 bg-blue-500/20"
        animate={{
          scale: scale,
          opacity: opacity
        }}
        transition={{ duration: 0.1, ease: "easeOut" }}
      />
      {/* Ø¯Ú©Ù…Ù‡ Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ† Ø¯Ø§Ø®Ù„ÛŒ */}
      <div className="flex size-32 items-center justify-center rounded-full bg-gradient-to-br from-blue-500 to-indigo-600 shadow-lg">
        <svg
          className="size-16 text-white"
          viewBox="0 0 24 24"
          fill="none"
          xmlns="http://www.w3.org/2000/svg"
        >
          <path
            d="M12 1a3 3 0 00-3 3v8a3 3 0 006 0V4a3 3 0 00-3-3z"
            fill="currentColor"
          />
          <path
            d="M19 10v2a7 7 0 01-14 0v-2H3v2a9 9 0 008 8.94V23h2v-2.06A9 9 0 0021 12v-2h-2z"
            fill="currentColor"
          />
        </svg>
      </div>
    </div>
  )
}

// ------------------------------------------------------------------
// Ù‡ÙˆÚ© ÙˆÛŒÚ˜ÙˆØ§Ù„Ø§ÛŒØ²Ø± (Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ø®Ø·Ø§ÛŒ TS(2345))
// ------------------------------------------------------------------
const useAudioVisualizer = (stream: MediaStream | null) => {
  const [volume, setVolume] = useState(0)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyzerRef = useRef<AnalyserNode | null>(null)
  const animationFrameRef = useRef<number | null>(null)

  useEffect(() => {
    if (!stream) {
      if (audioContextRef.current) {
        audioContextRef.current.close().catch(console.error)
        audioContextRef.current = null
      }
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current)
      }
      setVolume(0)
      return
    }

    if (!audioContextRef.current) {
      const audioContext = new (window.AudioContext ||
        (window as any).webkitAudioContext)()
      const analyzer = audioContext.createAnalyser()
      analyzer.fftSize = 256
      const source = audioContext.createMediaStreamSource(stream)
      source.connect(analyzer)
      analyzerRef.current = analyzer
      audioContextRef.current = audioContext
    }

    const analyze = () => {
      const analyzer = analyzerRef.current

      if (analyzer) {
        // âœ… [Ø§ØµÙ„Ø§Ø­ TS(2345)]
        // ÛŒÚ© Ø¢Ø±Ø§ÛŒÙ‡ Ù…ÙˆÙ‚Øª Ø¨Ø§ ØªØ§ÛŒÙ¾ ØµØ­ÛŒØ­ Ù…ÛŒâ€ŒØ³Ø§Ø²ÛŒÙ… ØªØ§ ØªØ§Ø¨Ø¹ Ø¨ØªÙˆØ§Ù†Ø¯ Ø¯Ø± Ø¢Ù† Ø¨Ù†ÙˆÛŒØ³Ø¯
        const tempArray = new Uint8Array(analyzer.frequencyBinCount)
        analyzer.getByteFrequencyData(tempArray)

        const sum = tempArray.reduce((a, b) => a + b, 0)
        const avg = sum / tempArray.length
        setVolume(avg)
      }
      animationFrameRef.current = requestAnimationFrame(analyze)
    }

    analyze()

    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current)
      }
      if (audioContextRef.current) {
        audioContextRef.current.close().catch(console.error)
        audioContextRef.current = null
      }
    }
  }, [stream])

  return volume
}

const useAudioActivityDetector = (
  stream: MediaStream | null,
  onActivityChange: (isActive: boolean) => void,
  options = { threshold: 10, silenceDelay: 500 } // Ø¢Ø³ØªØ§Ù†Ù‡ Ø­Ø³Ø§Ø³ÛŒØª Ùˆ ØªØ§Ø®ÛŒØ± Ø³Ú©ÙˆØª
) => {
  const contextRef = useRef<{
    audioContext: AudioContext
    analyser: AnalyserNode
    animationFrameId: number
    silenceTimerId: NodeJS.Timeout | null
    isSpeaking: boolean
  } | null>(null)

  useEffect(() => {
    // Ø§Ú¯Ø± Ø§Ø³ØªØ±ÛŒÙ… ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø±Ø§ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ú©Ù†
    if (!stream) {
      if (contextRef.current) {
        cancelAnimationFrame(contextRef.current.animationFrameId)
        if (contextRef.current.silenceTimerId)
          clearTimeout(contextRef.current.silenceTimerId)
        contextRef.current.audioContext.close().catch(console.error)
        if (contextRef.current.isSpeaking) {
          onActivityChange(false) // Ú¯Ø²Ø§Ø±Ø´ ØªÙˆÙ‚Ù ÙØ¹Ø§Ù„ÛŒØª
        }
        contextRef.current = null
      }
      return
    }

    // Ø§Ú¯Ø± Ø§Ø³ØªØ±ÛŒÙ… ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ø¢Ù†Ø§Ù„Ø§ÛŒØ²Ø± Ø±Ø§ Ø¨Ø³Ø§Ø²
    const audioContext = new (window.AudioContext ||
      (window as any).webkitAudioContext)()
    const analyser = audioContext.createAnalyser()
    analyser.fftSize = 256
    const source = audioContext.createMediaStreamSource(stream)
    source.connect(analyser)

    const dataArray = new Uint8Array(analyser.frequencyBinCount)

    contextRef.current = {
      audioContext,
      analyser,
      animationFrameId: 0,
      silenceTimerId: null,
      isSpeaking: false
    }

    const analyze = () => {
      const ctx = contextRef.current
      if (!ctx) return

      ctx.analyser.getByteFrequencyData(dataArray)
      const sum = dataArray.reduce((a, b) => a + b, 0)
      const avg = sum / dataArray.length

      if (avg > options.threshold) {
        // ØµØ¯Ø§ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯
        if (ctx.silenceTimerId) {
          clearTimeout(ctx.silenceTimerId)
          ctx.silenceTimerId = null
        }
        if (!ctx.isSpeaking) {
          ctx.isSpeaking = true
          onActivityChange(true) // Ú¯Ø²Ø§Ø±Ø´: Â«Ø´Ø±ÙˆØ¹ Ø¨Ù‡ ØµØ­Ø¨Øª Ú©Ø±Ø¯Â»
        }
      } else if (ctx.isSpeaking && !ctx.silenceTimerId) {
        // Ø³Ú©ÙˆØª ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ØŒ ØªØ§ÛŒÙ…Ø± Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´ Ø³Ú©ÙˆØª ÙØ¹Ø§Ù„ Ú©Ù†
        ctx.silenceTimerId = setTimeout(() => {
          ctx.isSpeaking = false
          onActivityChange(false) // Ú¯Ø²Ø§Ø±Ø´: Â«ØµØ­Ø¨Øª ØªÙ…Ø§Ù… Ø´Ø¯Â»
          ctx.silenceTimerId = null
        }, options.silenceDelay)
      }

      ctx.animationFrameId = requestAnimationFrame(analyze)
    }

    analyze()

    // ØªØ§Ø¨Ø¹ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
    return () => {
      if (contextRef.current) {
        cancelAnimationFrame(contextRef.current.animationFrameId)
        if (contextRef.current.silenceTimerId)
          clearTimeout(contextRef.current.silenceTimerId)
        contextRef.current.audioContext.close().catch(console.error)
        contextRef.current = null
      }
    }
  }, [stream, onActivityChange, options.threshold, options.silenceDelay])
}

const RealtimeVoicePage: FC = () => {
  const [status, setStatus] = useState<"idle" | "connecting" | "connected">(
    "idle"
  )
  const [model, setModel] = useState<string>("gpt-realtime-mini")

  // âœ… [Ø§ØµÙ„Ø§Ø­ Ø§ØµÙ„ÛŒ Û±]
  // ÛŒÚ© state Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ ØªÙˆÚ©Ù† Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
  const [supabaseToken, setSupabaseToken] = useState<string | null>(null)

  const dataChannelRef = useRef<RTCDataChannel | null>(null)
  const [userStream, setUserStream] = useState<MediaStream | null>(null)
  const [modelStream, setModelStream] = useState<MediaStream | null>(null)
  const peerConnectionRef = useRef<RTCPeerConnection | null>(null)
  const userAudioSenderRef = useRef<RTCRtpSender | null>(null)

  const userVolume = useAudioVisualizer(userStream)
  const modelVolume = useAudioVisualizer(modelStream)
  const combinedVolume = Math.max(userVolume, modelVolume)

  const handleModelSpeaking = useCallback((isSpeaking: boolean) => {
    // Ú†Ú© Ú©Ù† Ú©Ù‡ Ø¢ÛŒØ§ ÙØ±Ø³ØªÙ†Ø¯Ù‡ ØµØ¯Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø± (Ú©Ù‡ Ø¨Ù‡ OpenAI Ù…ÛŒâ€ŒØ±ÙˆØ¯) ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
    if (userAudioSenderRef.current && userAudioSenderRef.current.track) {
      if (isSpeaking) {
        // Ø§Ú¯Ø± Ù…Ø¯Ù„ ØµØ­Ø¨Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ† Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Mute Ú©Ù†
        remoteLog("ğŸ”‡ Model is speaking, MUTING user mic track.")
        userAudioSenderRef.current.track.enabled = false
      } else {
        // Ø§Ú¯Ø± Ù…Ø¯Ù„ Ø³Ø§Ú©Øª Ø´Ø¯ØŒ Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ† Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Unmute Ú©Ù†
        remoteLog("ğŸ¤ Model stopped speaking, UNMUTING user mic track.")
        userAudioSenderRef.current.track.enabled = true
      }
    }
  }, []) // ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø®Ø§Ù„ÛŒ Ø¯Ø±Ø³Øª Ø§Ø³ØªØŒ Ú†ÙˆÙ† Ù…Ø§ Ù‡Ù…ÛŒØ´Ù‡ Ø§Ø² Ref Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ…

  // Û². Ù‡ÙˆÚ© Ø¬Ø¯ÛŒØ¯ Ø±Ø§ ÙØ¹Ø§Ù„ Ú©Ù† ØªØ§ Ø¨Ù‡ ØµØ¯Ø§ÛŒ Ù…Ø¯Ù„ Ú¯ÙˆØ´ Ø¯Ù‡Ø¯
  useAudioActivityDetector(modelStream, handleModelSpeaking)

  useEffect(() => {
    remoteLog("Page component mounted. Adding global error listener.")

    const handleError = (event: ErrorEvent) => {
      // Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù‡Ø±Ú¯ÙˆÙ†Ù‡ Ú©Ø±Ø´ Ø¬Ø§ÙˆØ§ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¯Ø± ØµÙØ­Ù‡ Ø±Ø§ Ù„Ø§Ú¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
      remoteLog(
        `!!! GLOBAL CRASH !!! Message: ${event.message}, File: ${event.filename}, Line: ${event.lineno}`
      )
    }

    window.addEventListener("error", handleError)

    return () => {
      window.removeEventListener("error", handleError)
    }
  }, [])

  useEffect(() => {
    remoteLog("Token polling effect started.")
    const intervalId = setInterval(() => {
      const token = (window as any).SUPABASE_ACCESS_TOKEN
      remoteLog(
        `Polling... window token is: ${token ? token.substring(0, 10) + "..." : "null"}`
      )

      if (typeof window !== "undefined" && token) {
        remoteLog("SUCCESS! Token found on window object!")
        setSupabaseToken(token)
        delete (window as any).SUPABASE_ACCESS_TOKEN
        clearInterval(intervalId)
      }
    }, 250)
    return () => clearInterval(intervalId)
  }, [])

  // Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ø¯Ù„ Ø§Ø² URL (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
  useEffect(() => {
    // Û±. ÛŒÚ© Ø§ÛŒÙ†ØªØ±ÙˆØ§Ù„ Ø¨Ø±Ø§ÛŒ Ú†Ú© Ú©Ø±Ø¯Ù† Ù…ØªØºÛŒØ± Ø³Ø±Ø§Ø³Ø±ÛŒ Ø¨Ø³Ø§Ø²
    const intervalId = setInterval(() => {
      // âœ…âœ…âœ… Ù„Ø§Ú¯ Ø§Ø´Ú©Ø§Ù„â€ŒØ²Ø¯Ø§ÛŒÛŒ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯ âœ…âœ…âœ…
      console.log(
        "DEBUG: Polling for token... Current window token:",
        (window as any).SUPABASE_ACCESS_TOKEN
      )

      // Û². Ú†Ú© Ú©Ù† Ø¢ÛŒØ§ Ù…ØªØºÛŒØ± ØªÙˆØ³Ø· React Native ØªØ²Ø±ÛŒÙ‚ Ø´Ø¯Ù‡ Ø§Ø³ØªØŸ
      if (
        typeof window !== "undefined" &&
        (window as any).SUPABASE_ACCESS_TOKEN
      ) {
        const token = (window as any).SUPABASE_ACCESS_TOKEN
        console.log("âœ…âœ…âœ… [WebView] SUCCESS! Token found on window object!")
        setSupabaseToken(token)

        // Û³. Ù…ØªØºÛŒØ± Ø±Ø§ Ù¾Ø§Ú© Ú©Ù† (Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ø§Ù…Ø§ Ø§Ù…Ù†)
        delete (window as any).SUPABASE_ACCESS_TOKEN

        // Û´. Ø§ÛŒÙ†ØªØ±ÙˆØ§Ù„ Ø±Ø§ Ù…ØªÙˆÙ‚Ù Ú©Ù†
        clearInterval(intervalId)
      } else {
        // Ûµ. ØªØ§ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ ØªÙˆÚ©Ù† Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯Ù‡ØŒ Ù„Ø§Ú¯ Ø¨Ø²Ù†
        console.log(
          "âŒ›ï¸ [WebView] Polling: window.SUPABASE_ACCESS_TOKEN not found yet..."
        )
      }
    }, 250) // Ù‡Ø± 250 Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡ Ú†Ú© Ú©Ù†

    return () => {
      clearInterval(intervalId) // Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†ØªØ±ÙˆØ§Ù„ Ø¯Ø± Ø²Ù…Ø§Ù† unmount
    }
  }, [])
  // ØªØ§Ø¨Ø¹ Ø¨Ø³ØªÙ† Ùˆ Ø§Ø·Ù„Ø§Ø¹â€ŒØ±Ø³Ø§Ù†ÛŒ Ø¨Ù‡ Ø§Ù¾ Ù†ÛŒØªÛŒÙˆ
  const closeWebView = () => {
    if (typeof window !== "undefined" && (window as any).ReactNativeWebView) {
      // âœ… [Ø§ØµÙ„Ø§Ø­] Ù…Ø§ Ù‡Ù†ÙˆØ² Ø¨Ø±Ø§ÛŒ Ø¨Ø³ØªÙ† Ø¨Ù‡ postMessage Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ…
      ;(window as any).ReactNativeWebView.postMessage(
        JSON.stringify({ type: "close-webview" })
      )
    }
  }

  const stopRealtime = useCallback(() => {
    if (
      dataChannelRef.current &&
      dataChannelRef.current.readyState === "open"
    ) {
      console.log("â¡ï¸ Sending session.terminate event to OpenAI...")
      dataChannelRef.current.send(JSON.stringify({ type: "session.terminate" }))
    }

    if (dataChannelRef.current) {
      dataChannelRef.current.close()
      dataChannelRef.current = null
    }

    if (peerConnectionRef.current) {
      peerConnectionRef.current.close()
      peerConnectionRef.current = null
    }

    if (userStream) {
      userStream.getTracks().forEach(track => track.stop())
      setUserStream(null)
    }

    if (modelStream) {
      modelStream.getTracks().forEach(track => track.stop())
      setModelStream(null)
    }

    // Ø¨Ø³ØªÙ† ØªÙ…Ø§Ù… ØªÚ¯â€ŒÙ‡Ø§ÛŒ <audio> Ú©Ù‡ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯
    document.querySelectorAll("audio").forEach(el => el.remove())

    setStatus("idle")
    console.log("ğŸ›‘ Realtime session stopped")
    closeWebView() // <-- Ø¨Ù‡ Ø§Ù¾ Ù†ÛŒØªÛŒÙˆ Ø§Ø·Ù„Ø§Ø¹ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ø¨Ø³ØªÙ‡ Ø´ÙˆØ¯
  }, [userStream, modelStream])

  const startRealtime = useCallback(async () => {
    remoteLog("--- startRealtime function triggered ---")
    setStatus("connecting")
    let sessionData: any = null

    try {
      remoteLog("Reading token from window (using state)...")
      // â—ï¸â—ï¸â—ï¸ Ù…Ø§ Ù‡Ù…Ú†Ù†Ø§Ù† Ø§Ø² state Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ… Ú†ÙˆÙ† polling Ø¨Ø§ÛŒØ¯ Ø§ÙˆÙ„ ØªÙ…Ø§Ù… Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
      if (!supabaseToken) {
        remoteLog("FATAL: Token not found in state (supabaseToken is null).")
        throw new Error("Token not found in React state.")
      }

      remoteLog(`Token found in state. Calling /api/chat/openai...`)
      const res = await fetch("https://www.rhynoai.ir/api/chat/openai", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${supabaseToken}` // âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙˆÚ©Ù† state
        },
        body: JSON.stringify({ chatSettings: { model: model }, messages: [] })
      })

      remoteLog(`API call response status: ${res.status}`)
      if (!res.ok) {
        const errorData = await res.json()
        remoteLog(`API call failed: ${errorData.message}`)
        throw new Error(errorData.message || "Failed to get ephemeral key.")
      }

      sessionData = await res.json()
      remoteLog("Session data received from /api/chat.")

      // â—ï¸â—ï¸â—ï¸ [Ù„Ø§Ú¯ Û±: Ù¾Ø§Ø³Ø® Ú©Ø§Ù…Ù„ API Ø±Ø§ Ø¨Ø¨ÛŒÙ†ÛŒÙ…] â—ï¸â—ï¸â—ï¸
      console.log(
        "âœ… Session data received from /api/chat:",
        JSON.stringify(sessionData, null, 2)
      )

      // â—ï¸ Ø¨Ø± Ø§Ø³Ø§Ø³ route.ts Ø´Ù…Ø§ØŒ ØªÙˆÚ©Ù† Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯
      const EPHEMERAL_KEY = sessionData.client_secret?.value

      // â—ï¸â—ï¸â—ï¸ [Ù„Ø§Ú¯ Û²: Ø¨Ø¨ÛŒÙ†ÛŒÙ… ØªÙˆÚ©Ù† Ù¾ÛŒØ¯Ø§ Ø´Ø¯ ÛŒØ§ Ù†Ù‡] â—ï¸â—ï¸â—ï¸
      console.log(
        "ğŸ”‘ Extracted EPHEMERAL_KEY:",
        EPHEMERAL_KEY ? "Found" : "NOT FOUND"
      )

      if (!EPHEMERAL_KEY) {
        remoteLog("FATAL: client_secret not found in session data.")
        throw new Error("Invalid session data: client_secret.value is missing.")
      }
      remoteLog("Ephemeral key extracted. Setting up WebRTC...")
      const pc = new RTCPeerConnection()
      peerConnectionRef.current = pc

      pc.ontrack = e => {
        remoteLog("ğŸ”Š Remote audio track received!") // <-- Ù„Ø§Ú¯ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯
        setModelStream(e.streams[0])

        // ... (Ø­Ø°Ù <audio> Ù‚Ø¨Ù„ÛŒ)
        const audioEl = document.createElement("audio")
        audioEl.id = "model_audio"
        audioEl.srcObject = e.streams[0]
        audioEl.autoplay = true
        audioEl.setAttribute("playsinline", "true")
        document.body.appendChild(audioEl)

        remoteLog("Attempting to autoplay model audio...") // <-- Ù„Ø§Ú¯ Ù…Ù‡Ù…

        audioEl
          .play()
          .then(() => {
            // Ø§Ú¯Ø± ØµØ¯Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø®Ø´ Ø´ÙˆØ¯
            remoteLog("ğŸ”Š SUCCESS: Model audio playing.")
          })
          .catch(err => {
            // Ø§Ú¯Ø± Ù¾Ø®Ø´ Ø®ÙˆØ¯Ú©Ø§Ø± Ø´Ú©Ø³Øª Ø¨Ø®ÙˆØ±Ø¯
            console.error("ğŸš¨ Autoplay blocked:", err)
            remoteLog(
              `ğŸš¨ ERROR: Autoplay blocked: ${err instanceof Error ? err.message : String(err)}`
            ) // <-- Ù„Ø§Ú¯ Ø­ÛŒØ§ØªÛŒ
            toast.error("Ù…Ø±ÙˆØ±Ú¯Ø± Ø§Ø¬Ø§Ø²Ù‡ Ù¾Ø®Ø´ Ø®ÙˆØ¯Ú©Ø§Ø± ØµØ¯Ø§ Ø±Ø§ Ù†Ø¯Ø§Ø¯.")
          })
      }

      const dc = pc.createDataChannel("oai-events")
      dataChannelRef.current = dc
      dc.onopen = () => console.log("ğŸ“¡ DataChannel opened:", dc.label)

      const buffers = new Map<string, string>()

      // Ûµ. ØªØ¹Ø±ÛŒÙ onmessage (Ù…Ù†Ø·Ù‚ ÙØ§Ù†Ú©Ø´Ù† Ú©Ø§Ù„ Ø´Ù…Ø§)
      // Ûµ. ØªØ¹Ø±ÛŒÙ onmessage (Ù…Ù†Ø·Ù‚ ÙØ§Ù†Ú©Ø´Ù† Ú©Ø§Ù„ Ø´Ù…Ø§)
      dc.onmessage = async msg => {
        const data = JSON.parse(msg.data)
        remoteLog("raw data is logging :")
        remoteLog(JSON.stringify(data, null, 2))
        if (data.type === "response.function_call_arguments.delta") {
          const id = data.tool_call_id || data.item_id
          if (!id) return
          const prev = buffers.get(id) ?? ""
          buffers.set(id, prev + (data.delta ?? ""))
        }

        if (data.type === "response.function_call_arguments.done") {
          const id = data.tool_call_id || data.item_id
          if (!id) return
          const buffer = buffers.get(id) ?? ""
          buffers.delete(id)
          if (!buffer.startsWith("{") || !buffer.endsWith("}")) return

          try {
            const args = JSON.parse(buffer)
            const query = args.query
            console.log("ğŸ” Search requested:", query)
            if (!query) return
            console.log(
              `[AUTH DEBUG] Sending token: ${supabaseToken ? supabaseToken.substring(0, 10) + "..." : "TOKEN IS NULL!"}`
            )
            // â—ï¸â—ï¸â—ï¸ [Ø§ØµÙ„Ø§Ø­ Ø§ØµÙ„ÛŒ Ø§ÛŒÙ†Ø¬Ø§Ø³Øª] â—ï¸â—ï¸â—ï¸
            const searchRes = await fetch("/api/chat/search", {
              method: "POST",
              headers: {
                "Content-Type": "application/json",
                // âœ… Ø§ÛŒÙ† Ø®Ø· Ù…Ø´Ú©Ù„ Ø±Ø§ Ø¨Ø±Ø·Ø±Ù Ù…ÛŒâ€ŒÚ©Ù†Ø¯
                Authorization: `Bearer ${supabaseToken}`
              },
              body: JSON.stringify({ query })
            })

            // Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ø¢ÛŒØ§ Ø®ÙˆØ¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¬Ø³Øªâ€ŒÙˆØ¬Ùˆ Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯ ÛŒØ§ Ù†Ù‡
            if (!searchRes.ok) {
              const errorText = await searchRes.text()
              console.error(
                `Search API failed with status ${searchRes.status}: ${errorText}`
              )
              remoteLog(
                `Search API failed with status ${searchRes.status}: ${errorText}`
              )
              // Ù…Ø§ Ø®Ø·Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† JSON Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ… ØªØ§ Ø³ÛŒÙ†ØªÚ©Ø³ Ø§Ø±ÙˆØ± Ù†Ø¯Ù‡ÛŒÙ…
              // Ø§Ù…Ø§ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ú†ÙˆÙ† Ø³Ø±ÙˆØ± 401 Ø¯Ø§Ø¯Ù‡ØŒ Ù…Ø§ ÙÙ‚Ø· Ù„Ø§Ú¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ… Ùˆ Ù…ØªÙˆÙ‚Ù Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ…
              return
            }

            const searchData = await searchRes.json()
            const textResult = searchData.output_text ?? "No result found."

            const payload = {
              type: "response.create",
              response: { conversation: "auto", instructions: textResult }
            }
            dc.send(JSON.stringify(payload))
            console.log("âœ… Sent search results back to model")
          } catch (err: any) {
            console.error(
              "âŒ Error parsing JSON buffer or fetching search:",
              buffer,
              err
            )
            remoteLog(`âŒ Error in function call 'done': ${err.message}`)
          }
        }

        if (data.type === "response.done") {
          console.log("âœ… Response.done received from OpenAI.")
        }
      }

      pc.onconnectionstatechange = () => {
        console.log("âš¡ Connection state:", pc.connectionState)
        if (["disconnected", "failed", "closed"].includes(pc.connectionState)) {
          stopRealtime()
        }
      }

      // Û·. Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ† Ú©Ø§Ø±Ø¨Ø±
      try {
        remoteLog("Attempting to get user microphone (getUserMedia)...")
        const ms = await navigator.mediaDevices.getUserMedia({
          audio: {
            noiseSuppression: true,
            echoCancellation: true
          }
        })
        remoteLog("âœ… SUCCESS: User microphone stream obtained.")
        setUserStream(ms)
        remoteLog("ğŸš€ Sending 'audio-ready' message to React Native.")
        if (
          typeof window !== "undefined" &&
          (window as any).ReactNativeWebView
        ) {
          ;(window as any).ReactNativeWebView.postMessage(
            JSON.stringify({ type: "audio-ready" })
          )
        }
        ms.getAudioTracks().forEach(track => {
          remoteLog(`ğŸ¤ Sending audio track: ${track.label}`)
          const sender = pc.addTrack(track, ms)
          userAudioSenderRef.current = sender
        })
      } catch (micError: any) {
        // â—ï¸â—ï¸â—ï¸ Ø§ÛŒÙ† Ù„Ø§Ú¯ Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ø²ÛŒØ§Ø¯ Ø¯Ø± Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø² Ø¸Ø§Ù‡Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯ â—ï¸â—ï¸â—ï¸
        remoteLog(
          `ğŸš¨ FATAL MIC ERROR: getUserMedia failed: ${micError.message}`
        )
        toast.error(`Ø®Ø·Ø§ÛŒ Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ†: ${micError.message}`)
        stopRealtime() // Ø§Ú¯Ø± Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ† Ø±Ø§ Ù†Ú¯ÛŒØ±ÛŒÙ…ØŒ Ù…ØªÙˆÙ‚Ù Ø´Ùˆ
        return // Ø§Ø² ØªØ§Ø¨Ø¹ Ø®Ø§Ø±Ø¬ Ø´Ùˆ
      }

      const offer = await pc.createOffer()
      await pc.setLocalDescription(offer)

      // Û¸. ØªØ¨Ø§Ø¯Ù„ SDP Ø¨Ø§ OpenAI
      const sdpResponse = await fetch(
        `https://api.openai.com/v1/realtime?model=${model}`,
        {
          method: "POST",
          body: offer.sdp,
          headers: {
            Authorization: `Bearer ${EPHEMERAL_KEY}`, // â—ï¸ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙˆÚ©Ù† OpenAI
            "Content-Type": "application/sdp"
          }
        }
      )

      if (!sdpResponse.ok) {
        throw new Error(`SDP exchange failed: ${sdpResponse.statusText}`)
      }

      const answer: RTCSessionDescriptionInit = {
        type: "answer",
        sdp: await sdpResponse.text()
      }
      await pc.setRemoteDescription(answer)
      setStatus("connected")
    } catch (error: any) {
      remoteLog(`!!! CATCH block error in startRealtime !!!: ${error.message}`)
      toast.error(`Ø®Ø·Ø§: ${error.message}`)
      stopRealtime()
    }
  }, [stopRealtime, model, supabaseToken])

  const handleIconClick = () => {
    if (status === "idle" && supabaseToken) {
      remoteLog("Icon clicked to start.") // <-- Ù„Ø§Ú¯ Ú©Ù„ÛŒÚ©
      startRealtime()
    } else if (status !== "idle") {
      remoteLog("Icon clicked to stop.") // <-- Ù„Ø§Ú¯ Ú©Ù„ÛŒÚ©
      stopRealtime()
    } else {
      remoteLog("Icon clicked, but token is not ready yet.") // <-- Ù„Ø§Ú¯ Ú©Ù„ÛŒÚ©
      toast.error("Ø¯Ø± Ø­Ø§Ù„ Ù‡Ù…Ú¯Ø§Ù…â€ŒØ³Ø§Ø²ÛŒ... Ù„Ø·ÙØ§Ù‹ Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ ØµØ¨Ø± Ú©Ù†ÛŒØ¯.")
    }
  }
  // UI Ø±Ø§ Ø¯Ø± ÛŒÚ© div ØªÙ…Ø§Ù…â€ŒØµÙØ­Ù‡ Ø³ÛŒØ§Ù‡ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… ØªØ§ Ø¨Ø§ Ø§Ù¾ Ù†ÛŒØªÛŒÙˆ ÛŒÚ©Ø³Ø§Ù† Ø¨Ø§Ø´Ø¯
  return (
    <div className="font-vazir fixed inset-0 bg-black text-white">
      {/* â—ï¸ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø®Ø·Ø§Ù‡Ø§ÛŒ toastØŒ Ø§ÛŒÙ† Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øª Ù„Ø§Ø²Ù… Ø§Ø³Øª */}
      <Toaster position="top-center" richColors />

      <AnimatePresence>
        {status === "connected" && (
          <motion.div
            onClick={handleIconClick} // Ø¨Ø§ Ú©Ù„ÛŒÚ© Ø±ÙˆÛŒ Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ù‡Ù… Ø¨Ø³ØªÙ‡ Ø´ÙˆØ¯
            className="fixed inset-0 z-50 flex cursor-pointer flex-col items-center justify-center bg-black/80 backdrop-blur-sm"
            initial={{ opacity: 0 }}
            animate={{ opacity: 1 }}
            exit={{ opacity: 0 }}
            transition={{ duration: 0.3 }}
          >
            {/* Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øª ÙˆÛŒÚ˜ÙˆØ§Ù„Ø§ÛŒØ²Ø± */}
            <CircularAudioVisualizer volume={combinedVolume} />

            <p className="mt-12 text-lg text-white">Ø¯Ø± Ø­Ø§Ù„ Ø´Ù†ÛŒØ¯Ù†...</p>
            <p className="mt-2 text-sm text-gray-400">
              Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ§Ù† Ø¯Ø§Ø¯Ù† Ø¨Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯
            </p>
          </motion.div>
        )}
      </AnimatePresence>

      {/* Ø¨Ø®Ø´ UI Ø¯Ú©Ù…Ù‡ Ø§ÙˆÙ„ÛŒÙ‡ (ÙˆÙ‚ØªÛŒ status !== 'connected') */}
      {status !== "connected" && (
        <div className="fixed bottom-12 left-1/2 z-50 flex -translate-x-1/2 flex-col items-center">
          <div
            onClick={handleIconClick}
            className={cn(
              "relative flex size-20 cursor-pointer items-center justify-center rounded-full transition-all duration-500",
              "bg-gradient-to-br from-blue-500 to-indigo-600 text-white",
              "shadow-lg shadow-blue-500/30",
              // âœ… [Ø§ØµÙ„Ø§Ø­ Ø§ØµÙ„ÛŒ Û´]
              // Ø¯Ú©Ù…Ù‡ Ø±Ø§ ØªØ§ Ø²Ù…Ø§Ù† Ø¯Ø±ÛŒØ§ÙØª ØªÙˆÚ©Ù† ØºÛŒØ±ÙØ¹Ø§Ù„ Ù†Ø´Ø§Ù† Ø¨Ø¯Ù‡
              !supabaseToken && "cursor-not-allowed opacity-50"
            )}
          >
            {status === "connecting" ? (
              // Ø¢ÛŒÚ©ÙˆÙ† Ù„ÙˆØ¯ÛŒÙ†Ú¯
              <svg
                className="size-10 animate-spin text-white"
                xmlns="http://www.w3.org/2000/svg"
                fill="none"
                viewBox="0 0 24 24"
              >
                <circle
                  className="opacity-25"
                  cx="12"
                  cy="12"
                  r="10"
                  stroke="currentColor"
                  strokeWidth="4"
                ></circle>
                <path
                  className="opacity-75"
                  fill="currentColor"
                  d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"
                ></path>
              </svg>
            ) : (
              // Ø¢ÛŒÚ©ÙˆÙ† Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ†
              <svg
                className="size-10 text-white"
                viewBox="0 0 24 24"
                fill="none"
                xmlns="http://www.w3.org/2000/svg"
              >
                <path
                  d="M12 1a3 3 0 00-3 3v8a3 3 0 006 0V4a3 3 0 00-3-3z"
                  fill="currentColor"
                />
                <path
                  d="M19 10v2a7 7 0 01-14 0v-2H3v2a9 9 0 008 8.94V23h2v-2.06A9 9 0 0021 12v-2h-2z"
                  fill="currentColor"
                />
              </svg>
            )}
          </div>
          <p className="mt-3 text-sm text-white">
            {status === "idle" && !supabaseToken && "Ø¯Ø± Ø­Ø§Ù„ Ù‡Ù…Ú¯Ø§Ù…â€ŒØ³Ø§Ø²ÛŒ..."}
            {status === "idle" && supabaseToken && "Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ ØµØ­Ø¨Øª Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯"}
            {status === "connecting" && "Ø¯Ø± Ø­Ø§Ù„ Ø§ØªØµØ§Ù„..."}
          </p>
        </div>
      )}
    </div>
  )
}

// â—ï¸â—ï¸ [Ù…Ù‡Ù…] â—ï¸â—ï¸
// Ù…Ø§ Ø¨Ù‡ Ø§ÛŒÙ† Wrapper Ø¯ÛŒÚ¯Ø± Ù†ÛŒØ§Ø²ÛŒ Ù†Ø¯Ø§Ø±ÛŒÙ… Ú†ÙˆÙ† useSearchParams Ø­Ø°Ù Ø´Ø¯.
// Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øª Ø§ØµÙ„ÛŒ Ø±Ø§ export Ú©Ù†ÛŒØ¯.
export default RealtimeVoicePage
