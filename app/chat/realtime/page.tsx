"use client"

import React, { FC, useState, useRef, useCallback, useEffect } from "react"
import { cn } from "@/lib/utils" // (cn) Ø´Ù…Ø§ Ø§Ø² Ù‚Ø¨Ù„ Ø¯Ø± Ù¾Ø±ÙˆÚ˜Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
import { toast, Toaster } from "sonner" // Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø®Ø·Ø§Ù‡Ø§
import { motion, AnimatePresence } from "framer-motion"
import { supabase } from "@/lib/supabase/client" // Ú©Ù„Ø§ÛŒÙ†Øª Ø³ÙˆÙ¾Ø§Ø¨ÛŒØ³ Ø´Ù…Ø§

// ------------------------------------------------------------------
// Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øª ÙˆÛŒÚ˜ÙˆØ§Ù„Ø§ÛŒØ²Ø± ØµÙˆØªÛŒ (Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÙˆØ¨ Ø¨Ø§ Tailwind)
// ------------------------------------------------------------------
const CircularAudioVisualizer: FC<{ volume: number }> = ({ volume }) => {
  // Ø­Ø¬Ù… ØµØ¯Ø§ Ø±Ø§ Ø¨Ù‡ ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± Ù„Ú¯Ø§Ø±ÛŒØªÙ…ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ù†ÙˆØ³Ø§Ù† Ø²ÛŒØ¨Ø§ØªØ± Ø¨Ø§Ø´Ø¯
  const scale = Math.log(1 + volume * 2) * 0.5 + 1
  const opacity = Math.min(volume / 50, 0.5)

  return (
    <div className="relative flex size-64 items-center justify-center">
      {/* Ù‡Ø§Ù„Ù‡ Ø¨ÛŒØ±ÙˆÙ†ÛŒ */}
      <motion.div
        className="absolute size-full rounded-full border border-blue-500/30 bg-blue-500/10"
        animate={{
          scale: scale * 1.1,
          opacity: opacity * 0.8
        }}
        transition={{ duration: 0.1, ease: "easeOut" }}
      />
      {/* Ù‡Ø§Ù„Ù‡ Ù…ÛŒØ§Ù†ÛŒ */}
      <motion.div
        className="absolute size-48 rounded-full border border-blue-500/50 bg-blue-500/20"
        animate={{
          scale: scale,
          opacity: opacity
        }}
        transition={{ duration: 0.1, ease: "easeOut" }}
      />
      {/* Ø¯Ú©Ù…Ù‡ Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ† Ø¯Ø§Ø®Ù„ÛŒ */}
      <div className="flex size-32 items-center justify-center rounded-full bg-gradient-to-br from-blue-500 to-indigo-600 shadow-lg">
        <svg
          className="size-16 text-white"
          viewBox="0 0 24 24"
          fill="none"
          xmlns="http://www.w3.org/2000/svg"
        >
          <path
            d="M12 1a3 3 0 00-3 3v8a3 3 0 006 0V4a3 3 0 00-3-3z"
            fill="currentColor"
          />
          <path
            d="M19 10v2a7 7 0 01-14 0v-2H3v2a9 9 0 008 8.94V23h2v-2.06A9 9 0 0021 12v-2h-2z"
            fill="currentColor"
          />
        </svg>
      </div>
    </div>
  )
}

// ------------------------------------------------------------------
// Ù‡ÙˆÚ© ÙˆÛŒÚ˜ÙˆØ§Ù„Ø§ÛŒØ²Ø± (Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ø®Ø·Ø§ÛŒ TS(2345))
// ------------------------------------------------------------------
const useAudioVisualizer = (stream: MediaStream | null) => {
  const [volume, setVolume] = useState(0)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyzerRef = useRef<AnalyserNode | null>(null)
  const animationFrameRef = useRef<number | null>(null)

  useEffect(() => {
    if (!stream) {
      if (audioContextRef.current) {
        audioContextRef.current.close().catch(console.error)
        audioContextRef.current = null
      }
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current)
      }
      setVolume(0)
      return
    }

    if (!audioContextRef.current) {
      const audioContext = new (window.AudioContext ||
        (window as any).webkitAudioContext)()
      const analyzer = audioContext.createAnalyser()
      analyzer.fftSize = 256
      const source = audioContext.createMediaStreamSource(stream)
      source.connect(analyzer)
      analyzerRef.current = analyzer
      audioContextRef.current = audioContext
    }

    const analyze = () => {
      const analyzer = analyzerRef.current

      if (analyzer) {
        // âœ… [Ø§ØµÙ„Ø§Ø­ TS(2345)]
        // ÛŒÚ© Ø¢Ø±Ø§ÛŒÙ‡ Ù…ÙˆÙ‚Øª Ø¨Ø§ ØªØ§ÛŒÙ¾ ØµØ­ÛŒØ­ Ù…ÛŒâ€ŒØ³Ø§Ø²ÛŒÙ… ØªØ§ ØªØ§Ø¨Ø¹ Ø¨ØªÙˆØ§Ù†Ø¯ Ø¯Ø± Ø¢Ù† Ø¨Ù†ÙˆÛŒØ³Ø¯
        const tempArray = new Uint8Array(analyzer.frequencyBinCount)
        analyzer.getByteFrequencyData(tempArray)

        const sum = tempArray.reduce((a, b) => a + b, 0)
        const avg = sum / tempArray.length
        setVolume(avg)
      }
      animationFrameRef.current = requestAnimationFrame(analyze)
    }

    analyze()

    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current)
      }
      if (audioContextRef.current) {
        audioContextRef.current.close().catch(console.error)
        audioContextRef.current = null
      }
    }
  }, [stream])

  return volume
}

// ------------------------------------------------------------------
// ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ ØªÙˆÚ©Ù† (Ø®ÙˆØ§Ù†Ø¯Ù† Ø§Ø² localStorage)
// ------------------------------------------------------------------
const getSupabaseToken = (): string | null => {
  if (typeof window !== "undefined") {
    const token = localStorage.getItem("supabase-access-token")
    console.log(
      token
        ? "âœ… Token found in localStorage."
        : "âŒ Token not found in localStorage."
    )
    return token
  }
  return null
}

// ------------------------------------------------------------------
// Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øª Ø§ØµÙ„ÛŒ ØµÙØ­Ù‡
// ------------------------------------------------------------------
const RealtimeVoicePage: FC = () => {
  const [status, setStatus] = useState<"idle" | "connecting" | "connected">(
    "idle"
  )
  const [model, setModel] = useState<string>("gpt-4o-realtime-preview") // Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
  const dataChannelRef = useRef<RTCDataChannel | null>(null)
  const [userStream, setUserStream] = useState<MediaStream | null>(null)
  const [modelStream, setModelStream] = useState<MediaStream | null>(null)
  const peerConnectionRef = useRef<RTCPeerConnection | null>(null)

  const userVolume = useAudioVisualizer(userStream)
  const modelVolume = useAudioVisualizer(modelStream)
  const combinedVolume = Math.max(userVolume, modelVolume)

  // âœ… [Ø§ØµÙ„Ø§Ø­ `next/navigation`]
  // Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ø¯Ù„ Ø§Ø² URL Ø¯Ø± useEffect
  useEffect(() => {
    if (typeof window !== "undefined") {
      const searchParams = new URLSearchParams(window.location.search)
      const modelFromUrl = searchParams.get("model")
      if (modelFromUrl) {
        console.log("Model specified in URL:", modelFromUrl)
        setModel(modelFromUrl)
      }
    }
  }, [])

  // ØªØ§Ø¨Ø¹ Ø¨Ø³ØªÙ† Ùˆ Ø§Ø·Ù„Ø§Ø¹â€ŒØ±Ø³Ø§Ù†ÛŒ Ø¨Ù‡ Ø§Ù¾ Ù†ÛŒØªÛŒÙˆ
  const closeWebView = () => {
    if (typeof window !== "undefined" && (window as any).ReactNativeWebView) {
      console.log("â¡ï¸ Sending 'close-webview' message to native app...")
      ;(window as any).ReactNativeWebView.postMessage("close-webview")
    } else {
      console.log("Not in WebView, cannot send 'close-webview' message.")
    }
  }

  const stopRealtime = useCallback(() => {
    if (
      dataChannelRef.current &&
      dataChannelRef.current.readyState === "open"
    ) {
      console.log("â¡ï¸ Sending session.terminate event to OpenAI...")
      dataChannelRef.current.send(JSON.stringify({ type: "session.terminate" }))
    }

    if (dataChannelRef.current) {
      dataChannelRef.current.close()
      dataChannelRef.current = null
    }

    if (peerConnectionRef.current) {
      peerConnectionRef.current.close()
      peerConnectionRef.current = null
    }

    if (userStream) {
      userStream.getTracks().forEach(track => track.stop())
      setUserStream(null)
    }

    if (modelStream) {
      modelStream.getTracks().forEach(track => track.stop())
      setModelStream(null)
    }

    // Ø¨Ø³ØªÙ† ØªÙ…Ø§Ù… ØªÚ¯â€ŒÙ‡Ø§ÛŒ <audio> Ú©Ù‡ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯
    document.querySelectorAll("audio").forEach(el => el.remove())

    setStatus("idle")
    console.log("ğŸ›‘ Realtime session stopped")
    closeWebView() // <-- Ø¨Ù‡ Ø§Ù¾ Ù†ÛŒØªÛŒÙˆ Ø§Ø·Ù„Ø§Ø¹ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ø¨Ø³ØªÙ‡ Ø´ÙˆØ¯
  }, [userStream, modelStream])

  const startRealtime = useCallback(async () => {
    setStatus("connecting")

    let sessionData: any = null

    try {
      // Û±. ØªÙˆÚ©Ù† Ø±Ø§ Ø§Ø² localStorage (Ú©Ù‡ ØªÙˆØ³Ø· Ø§Ù¾ Ù†ÛŒØªÛŒÙˆ ØªØ²Ø±ÛŒÙ‚ Ø´Ø¯Ù‡) Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ…
      let token = getSupabaseToken()
      if (!token) {
        console.warn("Token not found, retrying in 1s...")
        await new Promise(resolve => setTimeout(resolve, 1000))
        token = getSupabaseToken()
        if (!token) {
          throw new Error(
            "User not authenticated. Missing access token in localStorage."
          )
        }
      }

      // Û². Ø¨Ø§ Ø³Ø±ÙˆØ± Ø§ØµÙ„ÛŒ (route.ts) ØªÙ…Ø§Ø³ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
      console.log(`ğŸš€ Calling /api/chat for model: ${model}`)
      const res = await fetch("/api/chat", {
        // â—ï¸â—ï¸ Ø¢Ø¯Ø±Ø³ API Ø§ØµÙ„ÛŒ Ø´Ù…Ø§
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${token}` // ØªÙˆÚ©Ù† Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        },
        body: JSON.stringify({ chatSettings: { model: model }, messages: [] })
      })

      if (!res.ok) {
        const errorData = await res.json()
        console.error("âŒ Error from /api/chat:", errorData)
        throw new Error(errorData.message || "Failed to get ephemeral key.")
      }

      // Û³. Ø­Ø§Ù„Ø§ sessionData Ø±Ø§ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ú©Ù†
      sessionData = await res.json()
      console.log("âœ… Session data received from /api/chat:", sessionData)

      // â—ï¸ Ø¨Ø± Ø§Ø³Ø§Ø³ route.ts Ø´Ù…Ø§ØŒ ØªÙˆÚ©Ù† Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯
      const EPHEMERAL_KEY = sessionData.client_secret?.value
      if (!EPHEMERAL_KEY) {
        throw new Error("Invalid session data: client_secret.value is missing.")
      }

      // Û´. Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ WebRTC
      const pc = new RTCPeerConnection()
      peerConnectionRef.current = pc

      pc.ontrack = e => {
        console.log("ğŸ”Š Remote audio track received:", e.streams)
        setModelStream(e.streams[0])

        // Ø­Ø°Ù <audio> Ù‚Ø¨Ù„ÛŒ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª
        document
          .querySelectorAll("audio#model_audio")
          .forEach(el => el.remove())

        const audioEl = document.createElement("audio")
        audioEl.id = "model_audio" // ÛŒÚ© Ø¢ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø¨Ù‡ØªØ±
        audioEl.srcObject = e.streams[0]
        audioEl.autoplay = true
        audioEl.setAttribute("playsinline", "true") // Ø¨Ø±Ø§ÛŒ iOS
        document.body.appendChild(audioEl)

        audioEl
          .play()
          .then(() => console.log("ğŸ”Š Model audio playing..."))
          .catch(err => {
            console.error("ğŸš¨ Autoplay blocked:", err)
            toast.error("Ù…Ø±ÙˆØ±Ú¯Ø± Ø§Ø¬Ø§Ø²Ù‡ Ù¾Ø®Ø´ Ø®ÙˆØ¯Ú©Ø§Ø± ØµØ¯Ø§ Ø±Ø§ Ù†Ø¯Ø§Ø¯.")
          })
      }

      const dc = pc.createDataChannel("oai-events")
      dataChannelRef.current = dc
      dc.onopen = () => console.log("ğŸ“¡ DataChannel opened:", dc.label)

      const buffers = new Map<string, string>()

      // Ûµ. ØªØ¹Ø±ÛŒÙ onmessage (Ù…Ù†Ø·Ù‚ ÙØ§Ù†Ú©Ø´Ù† Ú©Ø§Ù„ Ø´Ù…Ø§)
      dc.onmessage = async msg => {
        const data = JSON.parse(msg.data)

        if (data.type === "response.function_call_arguments.delta") {
          const id = data.tool_call_id || data.item_id
          if (!id) return
          const prev = buffers.get(id) ?? ""
          buffers.set(id, prev + (data.delta ?? ""))
        }

        if (data.type === "response.function_call_arguments.done") {
          const id = data.tool_call_id || data.item_id
          if (!id) return
          const buffer = buffers.get(id) ?? ""
          buffers.delete(id)
          if (!buffer.startsWith("{") || !buffer.endsWith("}")) return

          try {
            const args = JSON.parse(buffer)
            const query = args.query
            console.log("ğŸ” Search requested:", query)
            if (!query) return

            const searchRes = await fetch("/api/chat/search", {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({ query })
            })

            const searchData = await searchRes.json()
            const textResult = searchData.output_text ?? "No result found."

            const payload = {
              type: "response.create",
              response: { conversation: "auto", instructions: textResult }
            }
            dc.send(JSON.stringify(payload))
            console.log("âœ… Sent search results back to model")
          } catch (err) {
            console.error("âŒ Error parsing JSON buffer:", buffer, err)
          }
        }

        if (data.type === "response.done") {
          console.log("âœ… Response.done received from OpenAI.")
        }
      } // Ù¾Ø§ÛŒØ§Ù† dc.onmessage

      pc.onconnectionstatechange = () => {
        console.log("âš¡ Connection state:", pc.connectionState)
        if (["disconnected", "failed", "closed"].includes(pc.connectionState)) {
          stopRealtime()
        }
      }

      // Û·. Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ† Ú©Ø§Ø±Ø¨Ø±
      const ms = await navigator.mediaDevices.getUserMedia({
        audio: {
          noiseSuppression: true,
          echoCancellation: true
        }
      })
      console.log("ğŸ¤ Local stream obtained:", ms)
      setUserStream(ms)

      ms.getAudioTracks().forEach(track => {
        console.log("ğŸ¤ Sending audio track:", track.label)
        pc.addTrack(track, ms)
      })

      const offer = await pc.createOffer()
      await pc.setLocalDescription(offer)

      // Û¸. ØªØ¨Ø§Ø¯Ù„ SDP Ø¨Ø§ OpenAI
      const sdpResponse = await fetch(
        `https://api.openai.com/v1/realtime?model=${model}`,
        {
          method: "POST",
          body: offer.sdp,
          headers: {
            Authorization: `Bearer ${EPHEMERAL_KEY}`, // â—ï¸ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙˆÚ©Ù† OpenAI
            "Content-Type": "application/sdp"
          }
        }
      )

      if (!sdpResponse.ok) {
        throw new Error(`SDP exchange failed: ${sdpResponse.statusText}`)
      }

      const answer: RTCSessionDescriptionInit = {
        type: "answer",
        sdp: await sdpResponse.text()
      }
      await pc.setRemoteDescription(answer)
      setStatus("connected")
      console.log("âœ… Realtime session started")
    } catch (error) {
      console.error(
        `âŒ Could not start voice chat: ${
          error instanceof Error ? error.message : "Unknown error"
        }`
      )
      toast.error(
        `Ø®Ø·Ø§ Ø¯Ø± Ø´Ø±ÙˆØ¹ Ú†Øª ØµÙˆØªÛŒ: ${error instanceof Error ? error.message : "Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡"}`
      )
      stopRealtime() // Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ØŒ Ø¨Ø³ØªÙ‡ Ø´ÙˆØ¯
    }
  }, [stopRealtime, model]) // 'model' Ø¨Ù‡ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯

  const handleIconClick = () => {
    if (status === "idle") {
      startRealtime()
    } else {
      stopRealtime()
    }
  }

  // UI Ø±Ø§ Ø¯Ø± ÛŒÚ© div ØªÙ…Ø§Ù…â€ŒØµÙØ­Ù‡ Ø³ÛŒØ§Ù‡ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… ØªØ§ Ø¨Ø§ Ø§Ù¾ Ù†ÛŒØªÛŒÙˆ ÛŒÚ©Ø³Ø§Ù† Ø¨Ø§Ø´Ø¯
  return (
    <div className="font-vazir fixed inset-0 bg-black text-white">
      {/* â—ï¸ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø®Ø·Ø§Ù‡Ø§ÛŒ toastØŒ Ø§ÛŒÙ† Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øª Ù„Ø§Ø²Ù… Ø§Ø³Øª */}
      <Toaster position="top-center" richColors />

      <AnimatePresence>
        {status === "connected" && (
          <motion.div
            onClick={handleIconClick} // Ø¨Ø§ Ú©Ù„ÛŒÚ© Ø±ÙˆÛŒ Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ù‡Ù… Ø¨Ø³ØªÙ‡ Ø´ÙˆØ¯
            className="fixed inset-0 z-50 flex cursor-pointer flex-col items-center justify-center bg-black/80 backdrop-blur-sm"
            initial={{ opacity: 0 }}
            animate={{ opacity: 1 }}
            exit={{ opacity: 0 }}
            transition={{ duration: 0.3 }}
          >
            {/* Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øª ÙˆÛŒÚ˜ÙˆØ§Ù„Ø§ÛŒØ²Ø± */}
            <CircularAudioVisualizer volume={combinedVolume} />

            <p className="mt-12 text-lg text-white">Ø¯Ø± Ø­Ø§Ù„ Ø´Ù†ÛŒØ¯Ù†...</p>
            <p className="mt-2 text-sm text-gray-400">
              Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ§Ù† Ø¯Ø§Ø¯Ù† Ø¨Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯
            </p>
          </motion.div>
        )}
      </AnimatePresence>

      {/* Ø¨Ø®Ø´ UI Ø¯Ú©Ù…Ù‡ Ø§ÙˆÙ„ÛŒÙ‡ (ÙˆÙ‚ØªÛŒ status !== 'connected') */}
      {status !== "connected" && (
        <div className="fixed bottom-12 left-1/2 z-50 flex -translate-x-1/2 flex-col items-center">
          <div
            onClick={handleIconClick}
            className={cn(
              "relative flex size-20 cursor-pointer items-center justify-center rounded-full transition-all duration-500",
              "bg-gradient-to-br from-blue-500 to-indigo-600 text-white",
              "shadow-lg shadow-blue-500/30"
            )}
          >
            {status === "connecting" ? (
              // Ø¢ÛŒÚ©ÙˆÙ† Ù„ÙˆØ¯ÛŒÙ†Ú¯
              <svg
                className="size-10 animate-spin text-white"
                xmlns="http://www.w3.org/2000/svg"
                fill="none"
                viewBox="0 0 24 24"
              >
                <circle
                  className="opacity-25"
                  cx="12"
                  cy="12"
                  r="10"
                  stroke="currentColor"
                  strokeWidth="4"
                ></circle>
                <path
                  className="opacity-75"
                  fill="currentColor"
                  d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"
                ></path>
              </svg>
            ) : (
              // Ø¢ÛŒÚ©ÙˆÙ† Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ†
              <svg
                className="size-10 text-white"
                viewBox="0 0 24 24"
                fill="none"
                xmlns="http://www.w3.org/2000/svg"
              >
                <path
                  d="M12 1a3 3 0 00-3 3v8a3 3 0 006 0V4a3 3 0 00-3-3z"
                  fill="currentColor"
                />
                <path
                  d="M19 10v2a7 7 0 01-14 0v-2H3v2a9 9 0 008 8.94V23h2v-2.06A9 9 0 0021 12v-2h-2z"
                  fill="currentColor"
                />
              </svg>
            )}
          </div>
          <p className="mt-3 text-sm text-white">
            {status === "idle" && "Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ ØµØ­Ø¨Øª Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯"}
            {status === "connecting" && "Ø¯Ø± Ø­Ø§Ù„ Ø§ØªØµØ§Ù„..."}
          </p>
        </div>
      )}
    </div>
  )
}

// â—ï¸â—ï¸ [Ù…Ù‡Ù…] â—ï¸â—ï¸
// Ù…Ø§ Ø¨Ù‡ Ø§ÛŒÙ† Wrapper Ø¯ÛŒÚ¯Ø± Ù†ÛŒØ§Ø²ÛŒ Ù†Ø¯Ø§Ø±ÛŒÙ… Ú†ÙˆÙ† useSearchParams Ø­Ø°Ù Ø´Ø¯.
// Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øª Ø§ØµÙ„ÛŒ Ø±Ø§ export Ú©Ù†ÛŒØ¯.
export default RealtimeVoicePage
