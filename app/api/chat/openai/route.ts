import { checkApiKey, getServerProfile } from "@/lib/server/server-chat-helpers"
import { ChatSettings, LLMID } from "@/types"
import { ServerRuntime } from "next"
import OpenAI from "openai"
import type {
  ChatCompletionCreateParams,
  ChatCompletionCreateParamsStreaming
} from "openai/resources/chat/completions"
import { NextResponse } from "next/server"
import { MODEL_PROMPTS } from "@/lib/build-prompt"

// âœ¨ Ø§ÛŒÙ…Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø®Øª Ùˆ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª
import { createServerClient } from "@supabase/ssr"
import { cookies } from "next/headers"
import { OPENAI_LLM_LIST } from "@/lib/models/llm/openai-llm-list"
import { handleTTS } from "@/app/api/chat/handlers/tts"
import { modelsWithRial } from "@/app/checkout/pricing"
import { handleSTT } from "@/app/api/chat/handlers/stt"

// Ø§Ø² Node.js runtime Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
export const runtime: ServerRuntime = "nodejs"
const OPENROUTER_GEMINI_MODEL_ID = "google/gemini-2.5-flash-image"
function isImageRequest(prompt: string): boolean {
  const lowerCasePrompt = prompt.toLowerCase()

  // Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªØµÙˆÛŒØ±
  const imageNouns = [
    "Ø¹Ú©Ø³",
    "ØªØµÙˆÛŒØ±",
    "Ù†Ù‚Ø§Ø´ÛŒ",
    "Ø·Ø±Ø­",
    "Ù¾ÙˆØ³ØªØ±",
    "ÛŒÙ‡ Ø¹Ú©Ø³ Ø§Ø²",
    "ÛŒÙ‡ Ø¹Ú©Ø³",
    "ÛŒÚ© Ø¹Ú©Ø³"
  ]
  const createVerbs = [
    "Ø¨Ø³Ø§Ø²",
    "Ø¨Ú©Ø´",
    "Ø·Ø±Ø§Ø­ÛŒ Ú©Ù†",
    "Ø¯Ø±Ø³Øª Ú©Ù†",
    "Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†",
    "ÛŒÙ‡ Ø¹Ú©Ø³ Ø§Ø²"
  ]

  // Ø¢ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² Ø§Ø³Ù…â€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ± Ø¯Ø± Ù…ØªÙ† Ù‡Ø³ØªØŸ
  const hasImageNoun = imageNouns.some(noun => lowerCasePrompt.includes(noun))

  // Ø¢ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² ÙØ¹Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªÙ† Ø¯Ø± Ù…ØªÙ† Ù‡Ø³ØªØŸ
  const hasCreateVerb = createVerbs.some(verb => lowerCasePrompt.includes(verb))

  // Ø§Ú¯Ø± Ù‡Ø± Ø¯Ùˆ Ø´Ø±Ø· Ø¨Ø±Ù‚Ø±Ø§Ø± Ø¨ÙˆØ¯ØŒ ÛŒØ¹Ù†ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø³Ø§Ø®Øª ØªØµÙˆÛŒØ± Ø§Ø³Øª
  if (hasImageNoun && hasCreateVerb) {
    return true
  }

  // Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø±Ø§ Ù‡Ù… Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
  const englishKeywords = ["generate image", "create a picture of", "draw a"]
  if (englishKeywords.some(keyword => lowerCasePrompt.includes(keyword))) {
    return true
  }

  return false
}

// ØªØ§Ø¨Ø¹ ØªØ´Ø®ÛŒØµ ÙˆØ±ÙˆØ¯ÛŒ Ú¯ÙØªØ§Ø± Ø¨Ù‡ Ù…ØªÙ† (STT)
// Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù…ØŒ ÛŒÚ© ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±
function isSttRequest(messages: any[]): boolean {
  if (!messages || messages.length === 0) {
    return false
  }
  const lastMessage = messages[messages.length - 1]
  // Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ø¯ ÙØ±Ø§Ù†Øªâ€ŒØ§Ù†Ø¯ Ø´Ù…Ø§ØŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ ØµÙˆØªÛŒ Ú©Ø§Ø±Ø¨Ø± Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø±Ø§ Ø¯Ø§Ø±Ù†Ø¯
  return lastMessage.model === "user-audio"
}
function isMcpRequest(prompt: string): boolean {
  // Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Ù†Ø§Ù†Ùˆ Ø±Ø§ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯
  const keywords = ["mcp", "nano", "rhyno nano", "v5 nano"]
  const lowerCasePrompt = prompt.toLowerCase()

  // Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ø§ ÛŒÚ©ÛŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ (Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ : ÛŒØ§ ÙØ§ØµÙ„Ù‡) Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ÛŒØ§ Ø®ÛŒØ±
  // Ø§ÛŒÙ† Ú©Ø§Ø± Ø§Ø² ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
  return keywords.some(
    word =>
      lowerCasePrompt.startsWith(word + ":") ||
      lowerCasePrompt.startsWith(word + " ")
  )
}
function isDocgenRequest(prompt: string): boolean {
  const lowerCasePrompt = prompt.toLowerCase()

  // Ù„ÛŒØ³Øª Ø§Ù†ÙˆØ§Ø¹ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
  const docTypes = [
    "Ø§Ú©Ø³Ù„",
    "excel",
    "pdf",
    "Ù¾ÛŒ Ø¯ÛŒ Ø§Ù",
    "word",
    "ÙˆØ±Ø¯",
    "document",
    "Ø³Ù†Ø¯"
  ]

  // Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø³Ø§Ø®ØªÙ† ÛŒØ§ ØªØ¨Ø¯ÛŒÙ„
  const createKeywords = [
    "Ø¨Ø³Ø§Ø²",
    "Ú©Ù†",
    "ØªÙˆÙ„ÛŒØ¯ Ú©Ù†",
    "Ø¯Ø±Ø³Øª Ú©Ù†",
    "Ø®Ø±ÙˆØ¬ÛŒ",
    "output",
    "format",
    "Ø¯Ø± Ù‚Ø§Ù„Ø¨"
  ]

  // Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø¢ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² Ø§Ù†ÙˆØ§Ø¹ ÙØ§ÛŒÙ„ Ø¯Ø± Ù…ØªÙ† ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ
  const hasDocType = docTypes.some(doc => lowerCasePrompt.includes(doc))

  // Ùˆ Ø¢ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø³Ø§Ø®ØªÙ† Ø¯Ø± Ù…ØªÙ† ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ
  const hasCreateKeyword = createKeywords.some(keyword =>
    lowerCasePrompt.includes(keyword)
  )

  // Ø§Ú¯Ø± Ù‡Ø± Ø¯Ùˆ Ø´Ø±Ø· Ø¨Ø±Ù‚Ø±Ø§Ø± Ø¨ÙˆØ¯ØŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø³Ø§Ø®Øª ÙØ§ÛŒÙ„ Ø§Ø³Øª
  return hasDocType && hasCreateKeyword
}

type ChatCompletionUsage = {
  prompt_tokens: number
  completion_tokens: number
  total_tokens: number
}

type ExtendedChatSettings = ChatSettings & {
  maxTokens?: number
  max_tokens?: number
}

// âœ¨ Ø«Ø§Ø¨Øªâ€ŒÙ‡Ø§ Ùˆ ØªØ§Ø¨Ø¹ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡
const PROFIT_MARGIN = 1.4

function calculateUserCostUSD(
  modelId: string,
  usage: { prompt_tokens: number; completion_tokens: number }
): number {
  // Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„ Ø§Ø² modelsWithRial
  const model = modelsWithRial.find(m => m.id === modelId)
  if (!model) return 0

  const promptCost =
    (usage.prompt_tokens / 1_000_000) * model.inputPricePer1MTokenUSD
  const completionCost =
    (usage.completion_tokens / 1_000_000) * model.outputPricePer1MTokenUSD

  return (promptCost + completionCost) * PROFIT_MARGIN
}

const MODELS_NEED_MAX_COMPLETION = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-nano",
  "gpt-5-mini"
])
const MODELS_WITH_OPENAI_WEB_SEARCH = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-mini"
])
const MODELS_THAT_SHOULD_NOT_STREAM = new Set(["gpt-5", "gpt-5-mini"])
const MODELS_WITH_AUTO_SEARCH = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-mini"
])

const MODEL_MAX_TOKENS: Record<string, number> = {
  "gpt-4o": 8192,
  "gpt-4o-mini": 4096,
  "gpt-5": 12000,
  "gpt-5-mini": 12000,
  "gpt-3.5-turbo": 4096,
  "gpt-3.5-turbo-16k": 16384
  // Ø³Ø§ÛŒØ± Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
}
const MODELS_WITH_PRIORITY_TIER = new Set(["gpt-5", "gpt-5-mini", "gpt-5-nano"])

function pickMaxTokens(cs: ExtendedChatSettings, modelId: string): number {
  const requestedTokens = cs.maxTokens ?? cs.max_tokens ?? 4096
  const modelLimit = MODEL_MAX_TOKENS[modelId] ?? 4096
  // Ù…Ù‚Ø¯Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ù†Ø¨Ø§ÛŒØ¯ Ø§Ø² Ø³Ù‚Ù Ù…Ø¯Ù„ Ø¨ÛŒØ´ØªØ± Ø´ÙˆØ¯
  return Math.min(requestedTokens, modelLimit)
}

export async function POST(request: Request) {
  console.log("ğŸ”¥ğŸ”¥ğŸ”¥ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ API Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯! Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´... ğŸ”¥ğŸ”¥ğŸ”¥")
  try {
    const requestBody = await request.json()
    const { chatSettings, messages, enableWebSearch, input } = requestBody
    // console.log("--- RECEIVED MESSAGES ARRAY ---")
    // console.log(JSON.stringify(messages, null, 2))
    // console.log("-----------------------------")

    const authHeader = request.headers.get("Authorization")
    if (!authHeader || !authHeader.startsWith("Bearer ")) {
      console.error("âŒ Auth header missing or invalid")
      return new NextResponse("Unauthorized: Missing Bearer token", {
        status: 401
      })
    }
    const token = authHeader.split(" ")[1]

    // âœ¨ Û². Ú©Ù„Ø§ÛŒÙ†Øª Supabase Ø±Ø§ Ø¨Ø³Ø§Ø²ÛŒØ¯ (Ú©Ø¯ Ø´Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ú©Ù„Ø§ÛŒÙ†Øª Ø¯Ø±Ø³Øª Ø§Ø³Øª)
    const cookieStore = cookies()
    const supabase = createServerClient(
      process.env.NEXT_PUBLIC_SUPABASE_URL!,
      process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
      { cookies: { get: (name: string) => cookieStore.get(name)?.value } }
    )

    // âœ¨ Û³. Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙˆÚ©Ù† Ø¯Ø±ÛŒØ§ÙØªÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ú©Ù†ÛŒØ¯
    const {
      data: { user },
      error: authError
    } = await supabase.auth.getUser(token) // ğŸ‘ˆ **ØªØºÛŒÛŒØ± Ú©Ù„ÛŒØ¯ÛŒ Ø§ÛŒÙ†Ø¬Ø§Ø³Øª**

    if (authError || !user) {
      console.error("âŒ Supabase auth.getUser failed:", authError?.message)
      return new NextResponse("Unauthorized: Invalid token", { status: 401 })
    }

    // âœ… Ø§Ú¯Ø± Ú©Ø¯ Ø¨Ù‡ Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø±Ø³Ø¯ØŒ ÛŒØ¹Ù†ÛŒ Ú©Ø§Ø±Ø¨Ø± Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª
    const userId = user.id
    console.log(
      `âœ… User ${userId} successfully authenticated via Bearer token.`
    )

    const { data: wallet, error: walletError } = await supabase
      .from("wallets")
      .select("balance")
      .eq("user_id", userId)
      .single()

    if (walletError && walletError.code === "PGRST116") {
      return NextResponse.json(
        { message: "Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø´Ù…Ø§ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª." },
        { status: 402 }
      )
    } else if (walletError) {
      throw walletError
    }

    if (!wallet || wallet.balance <= 0) {
      return NextResponse.json(
        { message: "Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø´Ù…Ø§ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª." },
        { status: 402 }
      )
    }
    // âœ¨ Ù¾Ø§ÛŒØ§Ù† Ø¨Ø®Ø´ Ù¾Ø±Ø¯Ø§Ø®Øª Ùˆ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª

    const profile = await getServerProfile(userId)
    checkApiKey(profile.openai_api_key, "OpenAI")
    const openai = new OpenAI({
      apiKey: profile.openai_api_key || "",
      organization: profile.openai_organization_id
    })

    const selectedModel = (chatSettings.model || "gpt-4o-mini") as LLMID
    if (selectedModel === OPENROUTER_GEMINI_MODEL_ID) {
      // console.log(
      //   `ğŸ”„ Ù‡Ø¯Ø§ÛŒØª Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ ${selectedModel} Ø¨Ù‡ /api/chat/openrouter...`
      // )
      const openrouterUrl = new URL("/api/chat/openrouter", request.url)
      const openrouterResponse = await fetch(openrouterUrl, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Cookie: request.headers.get("Cookie") || ""
        },
        // âœ¨ [FIX] Ø§Ø² Ù…ØªØºÛŒØ± requestBody Ú©Ù‡ Ø¯Ø± Ø¨Ø§Ù„Ø§ Ø³Ø§Ø®ØªÛŒÙ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        body: JSON.stringify(requestBody)
      })
      return new Response(openrouterResponse.body, {
        status: openrouterResponse.status,
        headers: openrouterResponse.headers
      })
    }
    if (selectedModel === "gpt-4o-mini-tts") {
      // console.log("ğŸ”Š Ø¯Ø±Ø®ÙˆØ§Ø³Øª TTS Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯.")

      const ttsInput =
        input ||
        (messages && messages.length > 0
          ? messages[messages.length - 1]?.content
          : "") ||
        ""

      if (!ttsInput) {
        return NextResponse.json(
          { message: "Input text is required for TTS." },
          { status: 400 }
        )
      }
      const ttsBody = {
        input: ttsInput,
        voice: chatSettings.voice || "coral",
        speed: chatSettings.speed || 1.0,
        model: selectedModel
      }

      return await handleTTS({ body: ttsBody, user, supabase })
    }

    if (selectedModel.includes("realtime")) {
      const response = await fetch(
        "https://api.openai.com/v1/realtime/sessions",
        {
          method: "POST",
          headers: {
            Authorization: `Bearer ${profile.openai_api_key}`,
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            model: selectedModel,
            voice: "alloy",
            instructions: `
  You are Rhyno, a realtime Persian-speaking assistant.
  âœ… Always respond in Persian (Farsi).
  âœ… Only speak in voice (no text output).
  âœ… Introduce yourself as Rhyno when asked.
  âœ… Keep your answers short and concise. Do not over-explain.
`,

            tools: [
              {
                type: "function",
                name: "web_search",
                description: "Search the web for up-to-date information",
                parameters: {
                  type: "object",
                  properties: { query: { type: "string" } },
                  required: ["query"]
                }
              }
            ]
          })
        }
      )

      if (!response.ok) {
        const errorBody = await response.json()
        throw new Error(
          errorBody.error?.message || "Failed to create realtime session"
        )
      }
      const session = await response.json()
      // console.log("ğŸŒ Realtime session raw response:", session)
      // console.log("ğŸ”Š Session modalities:", session.modalities)
      // console.log("ğŸ”Š Session voice:", session.voice)
      // console.log("ğŸ”Š Session instructions:", session.instructions)

      const { error: insertError } = await supabase
        .from("realtime_sessions")
        .insert({
          user_id: userId,
          openai_session_id: session.id // ÛŒØ§ Ù‡Ø± ÙÛŒÙ„Ø¯ÛŒ Ú©Ù‡ ID Ø¬Ù„Ø³Ù‡ Ø¯Ø± Ø¢Ù† Ø§Ø³Øª
        })

      if (insertError) {
        console.error("Failed to save realtime session to DB:", insertError)
        // Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§ÛŒÙ†Ø¬Ø§ Ø®Ø·Ø§ Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†ÛŒØ¯
      }
      return NextResponse.json(session)
    }
    if (!messages) {
      return NextResponse.json(
        { message: "Missing 'messages' array for non-TTS request." },
        { status: 400 }
      )
    }
    function extractTextFromContent(content: any): string {
      if (!content && content !== 0) return ""
      if (typeof content === "string") return content
      // Ø§Ú¯Ø± content Ø¢Ø±Ø§ÛŒÙ‡ Ø§Ø² Ù¾Ø§Ø±Øªâ€ŒÙ‡Ø§Ø³Øª (Ù…Ø«Ù„ [{type:"input_text", text: "..."}])
      if (Array.isArray(content)) {
        return content
          .map(part => {
            if (typeof part === "string") return part
            if (part == null) return ""
            if (typeof part === "object") {
              // Ø§Ù†ÙˆØ§Ø¹ Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ú©Ù‡ Ù…Ù…Ú©Ù†Ù‡ Ø¯Ø§Ø®Ù„ Ø¨Ø§Ø´Ù†
              return (
                part.text ??
                part.content ??
                part.name ??
                JSON.stringify(part)
              ).toString()
            }
            return String(part)
          })
          .filter(Boolean)
          .join(" ")
      }
      // Ø§Ú¯Ø± Ø¢Ø¨Ø¬Ú©Øª Ø³Ø§Ø¯Ù‡â€ŒØ³Øª
      if (typeof content === "object") {
        return (
          content.text ??
          content.content ??
          JSON.stringify(content)
        ).toString()
      }
      return String(content)
    }

    // Ø³Ù¾Ø³
    const lastUserMessage = extractTextFromContent(
      messages[messages.length - 1]?.content
    )

    if (selectedModel === "gpt-4o-transcribe") {
      // console.log("ğŸ™ï¸ Ø¯Ø±Ø®ÙˆØ§Ø³Øª STT Ø¨Ù‡ Ù…Ø³ÛŒØ± Ø§Ø´ØªØ¨Ø§Ù‡ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯Ù‡ Ø§Ø³Øª.")
      // Ø§ÛŒÙ† Ø´Ø±Ø· Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø³Ø±Ø¯Ø±Ú¯Ù…ÛŒ Ø§Ø³Øª.
      // Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ STT Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø¨Ù‡ /api/transcribe Ø§Ø±Ø³Ø§Ù„ Ø´ÙˆÙ†Ø¯.
      return NextResponse.json(
        {
          message:
            "Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú¯ÙØªØ§Ø± Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ù…Ø³ÛŒØ± /api/transcribe Ø§Ø±Ø³Ø§Ù„ Ø´ÙˆÙ†Ø¯."
        },
        { status: 400 } // Bad Request
      )
    }

    // if (isDocgenRequest(lastUserMessage)) {
    //   // console.log("ğŸ“„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø³Ø§Ø®Øª ÙØ§ÛŒÙ„ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. Ù‡Ø¯Ø§ÛŒØª Ø¨Ù‡ Ù…Ø³ÛŒØ± DocGen...")

    //   // ØªÙˆØ¬Ù‡: ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø´Ù…Ø§ ÛŒÚ© Ù…Ø³ÛŒØ± API Ø¬Ø¯ÛŒØ¯ Ø¯Ø± /api/chat/docgen Ø³Ø§Ø®ØªÙ‡â€ŒØ§ÛŒØ¯
    //   const docgenUrl = new URL("/api/chat/mcp", request.url)

    //   const docgenResponse = await fetch(docgenUrl, {
    //     method: "POST",
    //     headers: {
    //       "Content-Type": "application/json",
    //       Cookie: request.headers.get("Cookie") || ""
    //     },
    //     body: JSON.stringify({ chatSettings, messages, enableWebSearch })
    //   })

    //   // Ù¾Ø§Ø³Ø® Ø§Ø² Ø§ÛŒÙ† Ù…Ø³ÛŒØ± Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ ÛŒÚ© Ù„ÛŒÙ†Ú© Ø¯Ø§Ù†Ù„ÙˆØ¯ ÛŒØ§ Ø®ÙˆØ¯ ÙØ§ÛŒÙ„ Ø¨Ø§Ø´Ø¯
    //   return new Response(docgenResponse.body, {
    //     status: docgenResponse.status,
    //     headers: docgenResponse.headers
    //   })
    // }

    // if (selectedModel === "gpt-5-nano") {
    //   console.log("ğŸš€ Ø¯Ø±Ø®ÙˆØ§Ø³Øª gpt-5-nano Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. Ù‡Ø¯Ø§ÛŒØª Ø¨Ù‡ /api/chat/mcp...")

    //   // Ø³Ø§Ø®Øª URL Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ù…Ø³ÛŒØ± Ø¬Ø¯ÛŒØ¯
    //   const mcpUrl = new URL("/api/chat/mcp", request.url)

    //   // Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ Ù…Ø³ÛŒØ± Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ Ù‡Ù…Ø§Ù† Ø¨Ø¯Ù†Ù‡ Ùˆ Ù‡Ø¯Ø±Ù‡Ø§
    //   const mcpResponse = await fetch(mcpUrl, {
    //     method: "POST",
    //     headers: {
    //       "Content-Type": "application/json",
    //       // Ø§Ø±Ø³Ø§Ù„ Ú©ÙˆÚ©ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª Ø¯Ø± Ù…Ø³ÛŒØ± Ø¬Ø¯ÛŒØ¯
    //       Cookie: request.headers.get("Cookie") || ""
    //     },
    //     // Ø§Ø±Ø³Ø§Ù„ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ú©Ù‡ Ø§Ø² Ø¨Ø¯Ù†Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø¨ÙˆØ¯ÛŒÙ…
    //     body: JSON.stringify({ chatSettings, messages, enableWebSearch })
    //   })

    //   // Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù…Ø³ØªÙ‚ÛŒÙ… Ù¾Ø§Ø³Ø® (Ø§Ø³ØªØ±ÛŒÙ… ÛŒØ§ ØºÛŒØ± Ø§Ø³ØªØ±ÛŒÙ…) Ø§Ø² Ù…Ø³ÛŒØ± MCP Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±
    //   return new Response(mcpResponse.body, {
    //     status: mcpResponse.status,
    //     headers: mcpResponse.headers
    //   })
    // }
    // Ø§Ú¯Ø± Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ú¯ÙØªØ§Ø± Ø§Ø³ØªØŒ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ú©Ù†ØªØ±Ù„â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ù…Ø±Ø¨ÙˆØ·Ù‡ Ø¨ÙØ±Ø³Øª

    // âœ¨ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ§Ù… Ø³ÛŒØ³ØªÙ…
    const finalMessages = [
      {
        role: "system",
        content:
          MODEL_PROMPTS[selectedModel] || "You are a helpful AI assistant."
      },
      ...(Array.isArray(messages) ? messages : [])
    ]

    if (selectedModel === "dall-e-3") {
      return NextResponse.json(
        {
          message:
            "DALL-E 3 requests should be sent to /api/chat/dalle/route.ts"
        },
        { status: 400 }
      )
    }

    const cs = chatSettings as ExtendedChatSettings
    const maxTokens = pickMaxTokens(cs, selectedModel)
    const temp = typeof cs.temperature === "number" ? cs.temperature : 1
    const hasImage = messages.some(
      (message: any) =>
        Array.isArray(message.content) &&
        message.content.some((part: any) => part.type === "image_url")
    )
    const useStream = !MODELS_THAT_SHOULD_NOT_STREAM.has(selectedModel)
    const enableSearch =
      typeof enableWebSearch === "boolean"
        ? enableWebSearch
        : MODELS_WITH_AUTO_SEARCH.has(selectedModel)
    const useOpenAIWebSearch =
      !!enableSearch &&
      MODELS_WITH_OPENAI_WEB_SEARCH.has(selectedModel) &&
      !hasImage // âœ¨

    // âœ¨ Ù…Ù†Ø·Ù‚ Web Search
    if (useOpenAIWebSearch) {
      // Ø¨Ø®Ø´ Û±: Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØºÛŒØ± Ø§Ø³ØªØ±ÛŒÙ… ÙˆØ¨â€ŒØ³Ø±Ú† (Ú©Ø¯ Ø§ØµÙ„ÛŒ Ø´Ù…Ø§)
      if (["gpt-5", "gpt-5-mini", "gpt-5-mini"].includes(selectedModel)) {
        // console.log(
        //   "ğŸš€ [WEB-SEARCH] Entering NON-streaming web search block for model:",
        //   selectedModel
        // )
        const webSearchPayload: any = {
          model: selectedModel,
          input: finalMessages.map(m =>
            m.role === "user"
              ? {
                  role: "user",
                  content: [{ type: "input_text", text: m.content as string }]
                }
              : m
          ) as any,
          tools: [{ type: "web_search" as any }],
          temperature: temp,
          max_output_tokens: maxTokens
        }

        if (MODELS_WITH_PRIORITY_TIER.has(selectedModel)) {
          webSearchPayload.service_tier = "default"
        }
        console.log(
          "ğŸš€ [PRIORITY-CHECK] Web Search Payload:",
          JSON.stringify(webSearchPayload, null, 2)
        )
        const response = await openai.responses.create(webSearchPayload)

        // âœ¨ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù†Ø·Ù‚ Ú©Ø³Ø± Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª ØºÛŒØ± Ø§Ø³ØªØ±ÛŒÙ… ÙˆØ¨â€ŒØ³Ø±Ú†
        const usage = response.usage
        if (usage) {
          const userCostUSD = calculateUserCostUSD(selectedModel, {
            prompt_tokens: usage.input_tokens, // <-- ØªØºÛŒÛŒØ± Ø§Ø² prompt_tokens
            completion_tokens: usage.output_tokens // <-- ØªØºÛŒÛŒØ± Ø§Ø² completion_tokens
          })
          // console.log(`ğŸ“Š [WEB-SEARCH] Usage data received:`, usage)
          if (userCostUSD > 0 && wallet) {
            // console.log(
            //   `ğŸ’° Ù‡Ø²ÛŒÙ†Ù‡: ${userCostUSD} | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø§ÙˆÙ„ÛŒÙ‡: ${wallet.balance}`
            // )
            await supabase.rpc("deduct_credits_and_log_usage", {
              p_user_id: userId,
              p_model_name: selectedModel,
              p_prompt_tokens: usage.input_tokens, // <-- ØªØºÛŒÛŒØ± Ø¨Ù‡ input_tokens
              p_completion_tokens: usage.output_tokens, // <-- ØªØºÛŒÛŒØ± Ø¨Ù‡ output_tokens
              p_cost: userCostUSD
            })
            const { data: updatedWallet } = await supabase
              .from("wallets")
              .select("balance")
              .eq("user_id", userId)
              .single()
            // console.log(
            //   `âœ… Ø¹Ù…Ù„ÛŒØ§Øª Ù…ÙˆÙÙ‚! | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯: ${updatedWallet?.balance}`
            // )
          }
        }

        return new Response(response.output_text ?? "", {
          headers: { "Content-Type": "text/plain; charset=utf-8" }
        })
      }

      // Ø¨Ø®Ø´ Û²: Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ±ÛŒÙ… ÙˆØ¨â€ŒØ³Ø±Ú† (Ù…Ø«Ù„ gpt-4o-mini)
      // Ø§ÛŒÙ† Ø¨Ø®Ø´ ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±ØªÛŒ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø´Ø±Ø· Ø¨Ø§Ù„Ø§ Ø¨Ø±Ù‚Ø±Ø§Ø± Ù†Ø¨Ø§Ø´Ø¯
      const encoder = new TextEncoder()
      const stream = new ReadableStream({
        async start(controller) {
          // console.log(
          //   "ğŸš€ [WEB-SEARCH] Entering STREAMING web search block for model:",
          //   selectedModel
          // )
          let usage: ChatCompletionUsage | undefined

          try {
            const transformedInput = finalMessages.map(m => {
              if (m.role === "user")
                return {
                  ...m,
                  content: [{ type: "input_text", text: m.content as string }]
                }
              if (m.role === "assistant" && typeof m.content === "string")
                return {
                  ...m,
                  content: [{ type: "output_text", text: m.content }]
                }
              return m
            })

            const oaiStream = await openai.responses.stream({
              model: selectedModel,
              input: transformedInput as any,
              tools: [{ type: "web_search" as any }],
              temperature: temp,
              max_output_tokens: maxTokens
            })

            for await (const event of oaiStream as AsyncIterable<any>) {
              // console.log("EVENT FROM OPENAI:", JSON.stringify(event, null, 2));
              if (event.type === "response.output_text.delta") {
                controller.enqueue(encoder.encode(String(event.delta || "")))
              } else if (
                event.type === "response.completed" &&
                event.response?.usage
              ) {
                const receivedUsage = event.response.usage
                usage = {
                  prompt_tokens: receivedUsage.input_tokens,
                  completion_tokens: receivedUsage.output_tokens,
                  total_tokens: receivedUsage.total_tokens
                }
                // Ø§ÛŒÙ† Ù„Ø§Ú¯ Ø­Ø§Ù„Ø§ Ø¨Ø§ÛŒØ¯ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯
                // console.log("ğŸ“Š [WEB-SEARCH] Usage data received:", usage)
              }
            }

            if (usage) {
              const userCostUSD = calculateUserCostUSD(selectedModel, usage)
              if (userCostUSD > 0 && wallet) {
                // console.log(
                //   `ğŸ’° Ù‡Ø²ÛŒÙ†Ù‡: ${userCostUSD} | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø§ÙˆÙ„ÛŒÙ‡: ${wallet.balance}`
                // )
                await supabase.rpc("deduct_credits_and_log_usage", {
                  p_user_id: userId,
                  p_model_name: selectedModel,
                  p_prompt_tokens: usage.prompt_tokens,
                  p_completion_tokens: usage.completion_tokens,
                  p_cost: userCostUSD
                })
                const { data: updatedWallet } = await supabase
                  .from("wallets")
                  .select("balance")
                  .eq("user_id", userId)
                  .single()
                // console.log(
                //   `âœ… Ø¹Ù…Ù„ÛŒØ§Øª Ù…ÙˆÙÙ‚! | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯: ${updatedWallet?.balance}`
                // )
              }
            }
          } catch (err: any) {
            console.error("âŒ [WEB-SEARCH] Error in stream:", err)
            controller.enqueue(
              encoder.encode(
                `âŒ Ø®Ø·Ø§ Ø¯Ø± ÙˆØ¨â€ŒØ³Ø±Ú†: ${err?.message || "Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡"}`
              )
            )
          } finally {
            // console.log("ğŸšª [WEB-SEARCH] Closing stream controller.")
            controller.close()
          }
        }
      })
      return new Response(stream, {
        headers: {
          "Content-Type": "text/plain; charset=utf-8",
          "Cache-Control": "no-cache, no-transform",
          "X-Accel-Buffering": "no"
        }
      })
    }
    const userPrompt = extractTextFromContent(
      finalMessages[finalMessages.length - 1]?.content
    )
    if (useStream) {
      const payload: ChatCompletionCreateParamsStreaming = {
        model: selectedModel,
        messages: finalMessages,
        stream: true,
        temperature: temp
        // ... (max_tokens, service_tier Ù…Ø«Ù„ Ù‚Ø¨Ù„)
      }
      if (MODELS_NEED_MAX_COMPLETION.has(selectedModel)) {
        ;(payload as any).max_completion_tokens = maxTokens
      } else {
        payload.max_tokens = maxTokens
      }
      if (MODELS_WITH_PRIORITY_TIER.has(selectedModel)) {
        ;(payload as any).service_tier = "priority" // ÛŒØ§ "default" Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÛŒØ§Ø²
      }

      const stream = await openai.chat.completions.create(payload)
      const encoder = new TextEncoder()
      let usage: ChatCompletionUsage | undefined // Ù…ØªØºÛŒØ± usage Ø¨ÛŒØ±ÙˆÙ† Ø­Ù„Ù‚Ù‡ ØªØ¹Ø±ÛŒÙ Ø´ÙˆØ¯

      const readableStream = new ReadableStream({
        async start(controller) {
          console.log(`ğŸš€ [STREAM-DEBUG] Stream started for user: ${userId}`)

          try {
            // --- ğŸ‘‡ Ø´Ø±ÙˆØ¹ Ø­Ù„Ù‚Ù‡ Stream ---
            for await (const chunk of stream) {
              // ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† usage Ø§Ø² Ù‡Ø± chunk (Ù…Ù…Ú©Ù† Ø§Ø³Øª null Ø¨Ø§Ø´Ø¯)
              if (chunk.usage) {
                usage = chunk.usage
                console.log("ğŸ“Š [STREAM-DEBUG] Potential Usage data:", usage)
              }

              const delta = chunk.choices[0]?.delta?.content || ""
              if (delta) {
                // Ø§Ø±Ø³Ø§Ù„ ØªÚ©Ù‡ Ù…ØªÙ† Ø¨Ù‡ Ú©Ù„Ø§ÛŒÙ†Øª
                console.log(`â¡ï¸ [STREAM-SENDING] Delta: "${delta}"`)
                controller.enqueue(encoder.encode(delta))
              }
            }
            // --- ğŸ‘† Ù¾Ø§ÛŒØ§Ù† Ø­Ù„Ù‚Ù‡ Stream ---

            console.log(
              "ğŸ [STREAM-DEBUG] Stream loop finished. Final check for usage data..."
            )

            // --- ğŸ‘‡ Ù…Ù†Ø·Ù‚ Fallback *Ø¨Ø¹Ø¯* Ø§Ø² Ø§ØªÙ…Ø§Ù… Stream ---
            if (!usage) {
              console.warn("âš ï¸ Usage data not found directly in stream chunks.")
              // Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ ØªØµÙ…ÛŒÙ… Ø¨Ú¯ÛŒØ±ÛŒØ¯:
              // 1. ÛŒÚ© Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØºÛŒØ±-Ø§Ø³ØªØ±ÛŒÙ… ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† usage Ø¨ÙØ±Ø³ØªÛŒØ¯ (Ø¨Ø¯ÙˆÙ† Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Ú©Ù„Ø§ÛŒÙ†Øª)
              // 2. Ù‡Ø²ÛŒÙ†Ù‡ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ®Ù…ÛŒÙ† Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†ÛŒØ¯
              // 3. ÙØ¹Ù„Ø§Ù‹ Ù‡ÛŒÚ† Ú©Ø§Ø±ÛŒ Ù†Ú©Ù†ÛŒØ¯ Ùˆ ÙÙ‚Ø· Ù„Ø§Ú¯ Ø¨Ø²Ù†ÛŒØ¯

              // Ù…Ø«Ø§Ù„ Ø¨Ø±Ø§ÛŒ Ú¯Ø²ÛŒÙ†Ù‡ Û± (Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ usage):
              try {
                console.log(
                  "ğŸ”„ Attempting non-stream call JUST for usage data..."
                )
                const usageResponsePayload: ChatCompletionCreateParams = {
                  // payload Ø´Ø¨ÛŒÙ‡ Ø¨Ù‡ Ø§Ø³ØªØ±ÛŒÙ… ÙˆÙ„ÛŒ stream: false
                  model: selectedModel,
                  messages: finalMessages,
                  temperature: temp,
                  // âŒ Ø®Ø· max_tokens: 1 Ø§Ø² Ø§ÛŒÙ†Ø¬Ø§ Ø­Ø°Ù Ø´Ø¯
                  stream: false
                }

                // âœ…âœ…âœ… Ù…Ù†Ø·Ù‚ ØµØ­ÛŒØ­ if/else âœ…âœ…âœ…
                if (MODELS_NEED_MAX_COMPLETION.has(selectedModel)) {
                  ;(usageResponsePayload as any).max_completion_tokens = 1
                } else {
                  // Ø§Ú¯Ø± Ù…Ø¯Ù„ Ø¨Ù‡ max_completion_tokens Ù†ÛŒØ§Ø² Ù†Ø¯Ø§Ø±Ø¯ØŒ Ø§Ø² max_tokens Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
                  usageResponsePayload.max_tokens = 1
                }
                // âœ…âœ…âœ… Ù¾Ø§ÛŒØ§Ù† Ø§ØµÙ„Ø§Ø­ÛŒÙ‡ âœ…âœ…âœ…

                if (MODELS_WITH_PRIORITY_TIER.has(selectedModel)) {
                  // Ø´Ù…Ø§ Ø§ÛŒÙ†Ø¬Ø§ "default" Ù†ÙˆØ´ØªÙ‡ Ø¨ÙˆØ¯ÛŒØ¯ØŒ Ø´Ø§ÛŒØ¯ Ø¨Ø§ÛŒØ¯ "priority" Ø¨Ø§Ø´Ø¯ØŸ
                  ;(usageResponsePayload as any).service_tier = "default"
                }

                const usageResponse =
                  await openai.chat.completions.create(usageResponsePayload)
                if (usageResponse.usage) {
                  usage = usageResponse.usage
                  console.log("ğŸ“Š Usage obtained via fallback request:", usage)
                } else {
                  console.error(
                    "âŒ Fallback request did not return usage data."
                  )
                }
              } catch (fallbackError: any) {
                console.error(
                  "âŒ Error during fallback request for usage:",
                  fallbackError
                )
              }
            }

            // --- ğŸ‘‡ Ú©Ø³Ø± Ù‡Ø²ÛŒÙ†Ù‡ *Ø¨Ø¹Ø¯* Ø§Ø² Ø§ØªÙ…Ø§Ù… Stream Ùˆ ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† usage ---
            if (usage) {
              console.log(
                "âœ… [STREAM-FINAL] Usage data available. Proceeding with deduction."
              )
              const userCostUSD = calculateUserCostUSD(selectedModel, usage)
              if (userCostUSD > 0 && wallet) {
                // ... (Ú©Ø¯ Ú©Ø³Ø± Ù‡Ø²ÛŒÙ†Ù‡ Ø´Ù…Ø§ Ù…Ø«Ù„ Ù‚Ø¨Ù„ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² usage) ...
                await supabase.rpc("deduct_credits_and_log_usage", {
                  p_user_id: userId,
                  p_model_name: selectedModel,
                  p_prompt_tokens: usage.prompt_tokens,
                  p_completion_tokens: usage.completion_tokens,
                  p_cost: userCostUSD
                })
                // ... (Ù„Ø§Ú¯ Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ú©Ø³Ø± Ù‡Ø²ÛŒÙ†Ù‡) ...
                console.log(
                  `âœ… Cost deducted: ${userCostUSD} for user ${userId}`
                )
              }
            } else {
              console.error(
                "âŒ CRITICAL: Could not determine usage data after stream and fallback."
              )
              // Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø§ÛŒØ¯ ØªØµÙ…ÛŒÙ… Ø¨Ú¯ÛŒØ±ÛŒØ¯ Ú†Ù‡ Ú©Ù†ÛŒØ¯ØŒ Ù…Ø«Ù„Ø§Ù‹ Ø®Ø·Ø§ Ù„Ø§Ú¯ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ù‡Ø²ÛŒÙ†Ù‡ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ú©Ù… Ú©Ù†ÛŒØ¯
            }
          } catch (err: any) {
            console.error("âŒ ERROR DURING STREAM PROCESSING:", err)
            controller.enqueue(
              encoder.encode(
                `\nâŒ Ø®Ø·Ø§ÛŒ Ø³Ø±ÙˆØ±: ${err.message || "Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡"}`
              )
            )
          } finally {
            console.log("ğŸšª [STREAM-DEBUG] Closing stream controller.")
            controller.close() // Ø¨Ø³ØªÙ† Stream Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§ÛŒÙ†Øª
          }
        }
      })
      // Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Stream Ø¨Ù‡ Ú©Ù„Ø§ÛŒÙ†Øª
      // Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Stream Ø¨Ù‡ Ú©Ù„Ø§ÛŒÙ†Øª
      return new Response(readableStream, {
        headers: {
          "Content-Type": "text/plain; charset=utf-8",
          "Cache-Control": "no-cache, no-transform",
          "X-Accel-Buffering": "no"
        }
      })
    } else {
      const payload: ChatCompletionCreateParams = {
        model: selectedModel,
        messages: finalMessages,
        stream: false,
        temperature: temp
      }
      if (MODELS_NEED_MAX_COMPLETION.has(selectedModel)) {
        ;(payload as any).max_completion_tokens = maxTokens
      } else {
        payload.max_tokens = maxTokens
      }
      if (MODELS_WITH_PRIORITY_TIER.has(selectedModel)) {
        ;(payload as any).service_tier = "priority"
      }
      console.log(
        "ğŸš€ [PRIORITY-CHECK] Non-Stream Payload:",
        JSON.stringify(payload, null, 2)
      )
      const response = await openai.chat.completions.create(payload)
      const content = response.choices[0].message.content ?? ""
      const usage = response.usage
      console.log("ğŸ’¡ Checking wallet and usage...")
      if (usage) {
        console.log("ğŸ’¡ Usage exists:", usage)
        const userCostUSD = calculateUserCostUSD(selectedModel, usage)
        console.log(
          `ğŸ’° Model: ${selectedModel}, UserID: ${userId}, CostUSD: ${userCostUSD}, Wallet balance before deduction: ${wallet?.balance}`
        )
        if (!wallet || wallet.balance < userCostUSD)
          return NextResponse.json(
            { message: "Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø´Ù…Ø§ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª." },
            { status: 402 }
          )
        if (userCostUSD > 0) {
          console.log("â³ Trying to deduct credits now...")
          await supabase.rpc("deduct_credits_and_log_usage", {
            p_user_id: userId,
            p_model_name: selectedModel,
            p_prompt_tokens: usage.prompt_tokens,
            p_completion_tokens: usage.completion_tokens,
            p_cost: userCostUSD
          })
          console.log(
            `âœ… Credits deducted for UserID: ${userId}, CostUSD: ${userCostUSD}`
          )
        }
      }
      return new Response(content, {
        headers: { "Content-Type": "text/plain; charset=utf-8" }
      })
    }
  } catch (error: any) {
    console.error("!!! FULL BACKEND ERROR CATCH !!!:", error)
    const errorMessage = error.message || "ÛŒÚ© Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø±Ø® Ø¯Ø§Ø¯"
    const status = error.status || 500
    return NextResponse.json({ message: errorMessage }, { status })
  }
}
