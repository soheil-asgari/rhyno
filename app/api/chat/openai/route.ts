import { checkApiKey, getServerProfile } from "@/lib/server/server-chat-helpers"
import { ChatSettings, LLMID } from "@/types"
import { ServerRuntime } from "next"
import OpenAI from "openai"
import type {
  ChatCompletionCreateParams,
  ChatCompletionCreateParamsStreaming
} from "openai/resources/chat/completions"
import { NextResponse } from "next/server"
import { MODEL_PROMPTS } from "@/lib/build-prompt"

// âœ¨ Ø§ÛŒÙ…Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø®Øª Ùˆ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª
import { createServerClient } from "@supabase/ssr"
import { cookies } from "next/headers"
import { OPENAI_LLM_LIST } from "@/lib/models/llm/openai-llm-list"
import { handleTTS } from "@/app/api/chat/handlers/tts"
import { modelsWithRial } from "@/app/checkout/pricing"
import { handleSTT } from "@/app/api/chat/handlers/stt"

// Ø§Ø² Node.js runtime Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
export const runtime: ServerRuntime = "nodejs"
function isImageRequest(prompt: string): boolean {
  const lowerCasePrompt = prompt.toLowerCase()

  // Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªØµÙˆÛŒØ±
  const imageNouns = [
    "Ø¹Ú©Ø³",
    "ØªØµÙˆÛŒØ±",
    "Ù†Ù‚Ø§Ø´ÛŒ",
    "Ø·Ø±Ø­",
    "Ù¾ÙˆØ³ØªØ±",
    "ÛŒÙ‡ Ø¹Ú©Ø³ Ø§Ø²",
    "ÛŒÙ‡ Ø¹Ú©Ø³",
    "ÛŒÚ© Ø¹Ú©Ø³"
  ]
  const createVerbs = [
    "Ø¨Ø³Ø§Ø²",
    "Ø¨Ú©Ø´",
    "Ø·Ø±Ø§Ø­ÛŒ Ú©Ù†",
    "Ø¯Ø±Ø³Øª Ú©Ù†",
    "Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†",
    "ÛŒÙ‡ Ø¹Ú©Ø³ Ø§Ø²"
  ]

  // Ø¢ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² Ø§Ø³Ù…â€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ± Ø¯Ø± Ù…ØªÙ† Ù‡Ø³ØªØŸ
  const hasImageNoun = imageNouns.some(noun => lowerCasePrompt.includes(noun))

  // Ø¢ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² ÙØ¹Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªÙ† Ø¯Ø± Ù…ØªÙ† Ù‡Ø³ØªØŸ
  const hasCreateVerb = createVerbs.some(verb => lowerCasePrompt.includes(verb))

  // Ø§Ú¯Ø± Ù‡Ø± Ø¯Ùˆ Ø´Ø±Ø· Ø¨Ø±Ù‚Ø±Ø§Ø± Ø¨ÙˆØ¯ØŒ ÛŒØ¹Ù†ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø³Ø§Ø®Øª ØªØµÙˆÛŒØ± Ø§Ø³Øª
  if (hasImageNoun && hasCreateVerb) {
    return true
  }

  // Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø±Ø§ Ù‡Ù… Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
  const englishKeywords = ["generate image", "create a picture of", "draw a"]
  if (englishKeywords.some(keyword => lowerCasePrompt.includes(keyword))) {
    return true
  }

  return false
}

// ØªØ§Ø¨Ø¹ ØªØ´Ø®ÛŒØµ ÙˆØ±ÙˆØ¯ÛŒ Ú¯ÙØªØ§Ø± Ø¨Ù‡ Ù…ØªÙ† (STT)
// Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù…ØŒ ÛŒÚ© ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±
function isSttRequest(messages: any[]): boolean {
  if (!messages || messages.length === 0) {
    return false
  }
  const lastMessage = messages[messages.length - 1]
  // Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ø¯ ÙØ±Ø§Ù†Øªâ€ŒØ§Ù†Ø¯ Ø´Ù…Ø§ØŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ ØµÙˆØªÛŒ Ú©Ø§Ø±Ø¨Ø± Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø±Ø§ Ø¯Ø§Ø±Ù†Ø¯
  return lastMessage.model === "user-audio"
}
function isMcpRequest(prompt: string): boolean {
  // Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Ù†Ø§Ù†Ùˆ Ø±Ø§ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯
  const keywords = ["mcp", "nano", "rhyno nano", "v5 nano"]
  const lowerCasePrompt = prompt.toLowerCase()

  // Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ø§ ÛŒÚ©ÛŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ (Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ : ÛŒØ§ ÙØ§ØµÙ„Ù‡) Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ÛŒØ§ Ø®ÛŒØ±
  // Ø§ÛŒÙ† Ú©Ø§Ø± Ø§Ø² ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
  return keywords.some(
    word =>
      lowerCasePrompt.startsWith(word + ":") ||
      lowerCasePrompt.startsWith(word + " ")
  )
}
function isDocgenRequest(prompt: string): boolean {
  const lowerCasePrompt = prompt.toLowerCase()

  // Ù„ÛŒØ³Øª Ø§Ù†ÙˆØ§Ø¹ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
  const docTypes = [
    "Ø§Ú©Ø³Ù„",
    "excel",
    "pdf",
    "Ù¾ÛŒ Ø¯ÛŒ Ø§Ù",
    "word",
    "ÙˆØ±Ø¯",
    "document",
    "Ø³Ù†Ø¯"
  ]

  // Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø³Ø§Ø®ØªÙ† ÛŒØ§ ØªØ¨Ø¯ÛŒÙ„
  const createKeywords = [
    "Ø¨Ø³Ø§Ø²",
    "Ú©Ù†",
    "ØªÙˆÙ„ÛŒØ¯ Ú©Ù†",
    "Ø¯Ø±Ø³Øª Ú©Ù†",
    "Ø®Ø±ÙˆØ¬ÛŒ",
    "output",
    "format",
    "Ø¯Ø± Ù‚Ø§Ù„Ø¨"
  ]

  // Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø¢ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² Ø§Ù†ÙˆØ§Ø¹ ÙØ§ÛŒÙ„ Ø¯Ø± Ù…ØªÙ† ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ
  const hasDocType = docTypes.some(doc => lowerCasePrompt.includes(doc))

  // Ùˆ Ø¢ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø³Ø§Ø®ØªÙ† Ø¯Ø± Ù…ØªÙ† ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ
  const hasCreateKeyword = createKeywords.some(keyword =>
    lowerCasePrompt.includes(keyword)
  )

  // Ø§Ú¯Ø± Ù‡Ø± Ø¯Ùˆ Ø´Ø±Ø· Ø¨Ø±Ù‚Ø±Ø§Ø± Ø¨ÙˆØ¯ØŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø³Ø§Ø®Øª ÙØ§ÛŒÙ„ Ø§Ø³Øª
  return hasDocType && hasCreateKeyword
}

type ChatCompletionUsage = {
  prompt_tokens: number
  completion_tokens: number
  total_tokens: number
}

type ExtendedChatSettings = ChatSettings & {
  maxTokens?: number
  max_tokens?: number
}

// âœ¨ Ø«Ø§Ø¨Øªâ€ŒÙ‡Ø§ Ùˆ ØªØ§Ø¨Ø¹ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡
const PROFIT_MARGIN = 1.4

function calculateUserCostUSD(
  modelId: string,
  usage: { prompt_tokens: number; completion_tokens: number }
): number {
  // Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„ Ø§Ø² modelsWithRial
  const model = modelsWithRial.find(m => m.id === modelId)
  if (!model) return 0

  const promptCost =
    (usage.prompt_tokens / 1_000_000) * model.inputPricePer1MTokenUSD
  const completionCost =
    (usage.completion_tokens / 1_000_000) * model.outputPricePer1MTokenUSD

  return (promptCost + completionCost) * PROFIT_MARGIN
}

const MODELS_NEED_MAX_COMPLETION = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-nano",
  "gpt-5-mini"
])
const MODELS_WITH_OPENAI_WEB_SEARCH = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-mini"
])
const MODELS_THAT_SHOULD_NOT_STREAM = new Set(["gpt-5", "gpt-5-mini"])
const MODELS_WITH_AUTO_SEARCH = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-mini"
])

const MODEL_MAX_TOKENS: Record<string, number> = {
  "gpt-4o": 8192,
  "gpt-4o-mini": 4096,
  "gpt-5": 12000,
  "gpt-5-mini": 12000,
  "gpt-3.5-turbo": 4096,
  "gpt-3.5-turbo-16k": 16384
  // Ø³Ø§ÛŒØ± Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
}
function pickMaxTokens(cs: ExtendedChatSettings, modelId: string): number {
  const requestedTokens = cs.maxTokens ?? cs.max_tokens ?? 4096
  const modelLimit = MODEL_MAX_TOKENS[modelId] ?? 4096
  // Ù…Ù‚Ø¯Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ù†Ø¨Ø§ÛŒØ¯ Ø§Ø² Ø³Ù‚Ù Ù…Ø¯Ù„ Ø¨ÛŒØ´ØªØ± Ø´ÙˆØ¯
  return Math.min(requestedTokens, modelLimit)
}

export async function POST(request: Request) {
  console.log("ğŸ”¥ğŸ”¥ğŸ”¥ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ API Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯! Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´... ğŸ”¥ğŸ”¥ğŸ”¥")
  try {
    const { chatSettings, messages, enableWebSearch } = await request.json()

    console.log("--- RECEIVED MESSAGES ARRAY ---")
    console.log(JSON.stringify(messages, null, 2))
    console.log("-----------------------------")

    // âœ¨ Ø´Ø±ÙˆØ¹ Ø¨Ø®Ø´ Ù¾Ø±Ø¯Ø§Ø®Øª Ùˆ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª
    const cookieStore = cookies()
    const supabase = createServerClient(
      process.env.NEXT_PUBLIC_SUPABASE_URL!,
      process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
      { cookies: { get: (name: string) => cookieStore.get(name)?.value } }
    )

    const {
      data: { user }
    } = await supabase.auth.getUser()
    if (!user) return new NextResponse("Unauthorized", { status: 401 })
    const userId = user.id

    const { data: wallet, error: walletError } = await supabase
      .from("wallets")
      .select("balance")
      .eq("user_id", userId)
      .single()

    if (walletError && walletError.code === "PGRST116") {
      return NextResponse.json(
        { message: "Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø´Ù…Ø§ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª." },
        { status: 402 }
      )
    } else if (walletError) {
      throw walletError
    }

    if (!wallet || wallet.balance <= 0) {
      return NextResponse.json(
        { message: "Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø´Ù…Ø§ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª." },
        { status: 402 }
      )
    }
    // âœ¨ Ù¾Ø§ÛŒØ§Ù† Ø¨Ø®Ø´ Ù¾Ø±Ø¯Ø§Ø®Øª Ùˆ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª

    const profile = await getServerProfile()
    checkApiKey(profile.openai_api_key, "OpenAI")
    const openai = new OpenAI({
      apiKey: profile.openai_api_key || "",
      organization: profile.openai_organization_id
    })
    const lastUserMessage = messages[messages.length - 1]?.content || ""

    const selectedModel = (chatSettings.model || "gpt-4o-mini") as LLMID

    if (selectedModel === "gpt-4o-transcribe") {
      console.log("ğŸ™ï¸ Ø¯Ø±Ø®ÙˆØ§Ø³Øª STT Ø¨Ù‡ Ù…Ø³ÛŒØ± Ø§Ø´ØªØ¨Ø§Ù‡ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯Ù‡ Ø§Ø³Øª.")
      // Ø§ÛŒÙ† Ø´Ø±Ø· Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø³Ø±Ø¯Ø±Ú¯Ù…ÛŒ Ø§Ø³Øª.
      // Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ STT Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø¨Ù‡ /api/transcribe Ø§Ø±Ø³Ø§Ù„ Ø´ÙˆÙ†Ø¯.
      return NextResponse.json(
        {
          message:
            "Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú¯ÙØªØ§Ø± Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ù…Ø³ÛŒØ± /api/transcribe Ø§Ø±Ø³Ø§Ù„ Ø´ÙˆÙ†Ø¯."
        },
        { status: 400 } // Bad Request
      )
    }
    if (isImageRequest(lastUserMessage)) {
      console.log("ğŸ¨ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø³Ø§Ø®Øª ØªØµÙˆÛŒØ± Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. Ù‡Ø¯Ø§ÛŒØª Ø¨Ù‡ Ù…Ø³ÛŒØ± DALL-E...")

      // Ø³Ø§Ø®Øª URL Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ù…Ø³ÛŒØ± DALL-E
      const dalleUrl = new URL("/api/chat/dalle", request.url)

      // Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ Ù…Ø³ÛŒØ± DALL-E Ø¨Ø§ Ù‡Ù…Ø§Ù† Ø¨Ø¯Ù†Ù‡
      // ÙÙ‚Ø· prompt Ø±Ø§ Ø¨Ù‡ Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø± Ù…Ø­Ø¯ÙˆØ¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø§Ø´Ø¯
      const dalleResponse = await fetch(dalleUrl, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          // Ø§Ø±Ø³Ø§Ù„ Ú©ÙˆÚ©ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª Ø¯Ø± Ù…Ø³ÛŒØ± DALL-E Ù‡Ù… Ú©Ø§Ø± Ú©Ù†Ø¯
          Cookie: request.headers.get("Cookie") || ""
        },
        body: JSON.stringify({
          chatSettings, // ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú†Øª Ø±Ø§ Ù‡Ù… Ù…ÛŒâ€ŒÙØ±Ø³ØªÛŒÙ…
          prompt: lastUserMessage // ÙÙ‚Ø· Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù… Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù¾Ø±Ø§Ù…Ù¾Øª Ù…ÛŒâ€ŒÙØ±Ø³ØªÛŒÙ…
        })
      })
      // Ø¨Ù„Ø§Ú© Û²: Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª MCP (Nano)

      // Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù…Ø³ØªÙ‚ÛŒÙ… Ù¾Ø§Ø³Ø® Ø§Ø² Ù…Ø³ÛŒØ± DALL-E Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±
      // Ø§ÛŒÙ† Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø´Ø§Ù…Ù„ URL Ø¹Ú©Ø³ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
      return new Response(dalleResponse.body, {
        status: dalleResponse.status,
        headers: dalleResponse.headers
      })
    }
    if (isMcpRequest(lastUserMessage)) {
      console.log("ğŸ”¹ Ø¯Ø±Ø®ÙˆØ§Ø³Øª MCP (Nano) Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. Ù‡Ø¯Ø§ÛŒØª Ø¨Ù‡ Ù…Ø³ÛŒØ± MCP...")
      const mcpUrl = new URL("/api/chat/mcp", request.url)

      // Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø±Ø§ Ø¨Ø§ ØªÙ…Ø§Ù… Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ù‡ Ù…Ø³ÛŒØ± MCP ÙÙˆØ±ÙˆØ§Ø±Ø¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
      const mcpResponse = await fetch(mcpUrl, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Cookie: request.headers.get("Cookie") || ""
        },
        // Ù…Ø³ÛŒØ± MCP Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø¨Ù‡ ØªÙ…Ø§Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ø¯ØŒ Ù†Ù‡ ÙÙ‚Ø· Ù¾Ø±Ø§Ù…Ù¾Øª
        body: JSON.stringify({ chatSettings, messages, enableWebSearch })
      })

      return new Response(mcpResponse.body, {
        status: mcpResponse.status,
        headers: mcpResponse.headers
      })
    }

    if (isDocgenRequest(lastUserMessage)) {
      console.log("ğŸ“„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø³Ø§Ø®Øª ÙØ§ÛŒÙ„ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. Ù‡Ø¯Ø§ÛŒØª Ø¨Ù‡ Ù…Ø³ÛŒØ± DocGen...")

      // ØªÙˆØ¬Ù‡: ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø´Ù…Ø§ ÛŒÚ© Ù…Ø³ÛŒØ± API Ø¬Ø¯ÛŒØ¯ Ø¯Ø± /api/chat/docgen Ø³Ø§Ø®ØªÙ‡â€ŒØ§ÛŒØ¯
      const docgenUrl = new URL("/api/chat/mcp", request.url)

      const docgenResponse = await fetch(docgenUrl, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Cookie: request.headers.get("Cookie") || ""
        },
        body: JSON.stringify({ chatSettings, messages, enableWebSearch })
      })

      // Ù¾Ø§Ø³Ø® Ø§Ø² Ø§ÛŒÙ† Ù…Ø³ÛŒØ± Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ ÛŒÚ© Ù„ÛŒÙ†Ú© Ø¯Ø§Ù†Ù„ÙˆØ¯ ÛŒØ§ Ø®ÙˆØ¯ ÙØ§ÛŒÙ„ Ø¨Ø§Ø´Ø¯
      return new Response(docgenResponse.body, {
        status: docgenResponse.status,
        headers: docgenResponse.headers
      })
    }

    if (selectedModel === "gpt-5-nano") {
      console.log("ğŸš€ Ø¯Ø±Ø®ÙˆØ§Ø³Øª gpt-5-nano Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. Ù‡Ø¯Ø§ÛŒØª Ø¨Ù‡ /api/chat/mcp...")

      // Ø³Ø§Ø®Øª URL Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ù…Ø³ÛŒØ± Ø¬Ø¯ÛŒØ¯
      const mcpUrl = new URL("/api/chat/mcp", request.url)

      // Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ Ù…Ø³ÛŒØ± Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ Ù‡Ù…Ø§Ù† Ø¨Ø¯Ù†Ù‡ Ùˆ Ù‡Ø¯Ø±Ù‡Ø§
      const mcpResponse = await fetch(mcpUrl, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          // Ø§Ø±Ø³Ø§Ù„ Ú©ÙˆÚ©ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª Ø¯Ø± Ù…Ø³ÛŒØ± Ø¬Ø¯ÛŒØ¯
          Cookie: request.headers.get("Cookie") || ""
        },
        // Ø§Ø±Ø³Ø§Ù„ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ú©Ù‡ Ø§Ø² Ø¨Ø¯Ù†Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø¨ÙˆØ¯ÛŒÙ…
        body: JSON.stringify({ chatSettings, messages, enableWebSearch })
      })

      // Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù…Ø³ØªÙ‚ÛŒÙ… Ù¾Ø§Ø³Ø® (Ø§Ø³ØªØ±ÛŒÙ… ÛŒØ§ ØºÛŒØ± Ø§Ø³ØªØ±ÛŒÙ…) Ø§Ø² Ù…Ø³ÛŒØ± MCP Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±
      return new Response(mcpResponse.body, {
        status: mcpResponse.status,
        headers: mcpResponse.headers
      })
    }
    // Ø§Ú¯Ø± Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ú¯ÙØªØ§Ø± Ø§Ø³ØªØŒ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ú©Ù†ØªØ±Ù„â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ù…Ø±Ø¨ÙˆØ·Ù‡ Ø¨ÙØ±Ø³Øª
    if (selectedModel === "gpt-4o-mini-tts") {
      console.log("ğŸ”Š Ø¯Ø±Ø®ÙˆØ§Ø³Øª TTS Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ handleTTS...")

      // Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±
      const input = messages[messages.length - 1]?.content || ""
      if (!input) {
        return NextResponse.json(
          { message: "Input text is required for TTS." },
          { status: 400 }
        )
      }

      // Ø¨Ø¯Ù†Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø±Ø§ Ø¨Ø±Ø§ÛŒ handleTTS Ø¨Ø³Ø§Ø²
      const ttsBody = {
        input,
        voice: chatSettings.voice || "coral",
        speed: chatSettings.speed || 1.0, // Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØµØ¯Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯
        model: selectedModel
      }

      // Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø±Ø§ Ø¨Ù‡ Ú©Ù†ØªØ±Ù„â€ŒÚ©Ù†Ù†Ø¯Ù‡ TTS Ø§Ø±Ø³Ø§Ù„ Ú©Ø±Ø¯Ù‡ Ùˆ Ù†ØªÛŒØ¬Ù‡ Ø±Ø§ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†
      return await handleTTS({
        body: ttsBody,
        user,
        supabase
      })
    }
    // âœ¨ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ§Ù… Ø³ÛŒØ³ØªÙ…
    const finalMessages = [
      {
        role: "system",
        content:
          MODEL_PROMPTS[selectedModel] || "You are a helpful AI assistant."
      },
      ...(Array.isArray(messages) ? messages : [])
    ]

    if (selectedModel === "dall-e-3") {
      return NextResponse.json(
        {
          message:
            "DALL-E 3 requests should be sent to /api/chat/dalle/route.ts"
        },
        { status: 400 }
      )
    }

    if (selectedModel.includes("realtime")) {
      const response = await fetch(
        "https://api.openai.com/v1/realtime/sessions",
        {
          method: "POST",
          headers: {
            Authorization: `Bearer ${profile.openai_api_key}`,
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            model: selectedModel,
            voice: "alloy",
            instructions: `
  You are Rhyno, a realtime Persian-speaking assistant.
  âœ… Always respond in Persian (Farsi).
  âœ… Only speak in voice (no text output).
  âœ… Introduce yourself as Rhyno when asked.
  âœ… Keep your answers short and concise. Do not over-explain.
`,

            tools: [
              {
                type: "function",
                name: "web_search",
                description: "Search the web for up-to-date information",
                parameters: {
                  type: "object",
                  properties: { query: { type: "string" } },
                  required: ["query"]
                }
              }
            ]
          })
        }
      )

      if (!response.ok) {
        const errorBody = await response.json()
        throw new Error(
          errorBody.error?.message || "Failed to create realtime session"
        )
      }
      const session = await response.json()
      console.log("ğŸŒ Realtime session raw response:", session)
      console.log("ğŸ”Š Session modalities:", session.modalities)
      console.log("ğŸ”Š Session voice:", session.voice)
      console.log("ğŸ”Š Session instructions:", session.instructions)

      const { error: insertError } = await supabase
        .from("realtime_sessions")
        .insert({
          user_id: userId,
          openai_session_id: session.id // ÛŒØ§ Ù‡Ø± ÙÛŒÙ„Ø¯ÛŒ Ú©Ù‡ ID Ø¬Ù„Ø³Ù‡ Ø¯Ø± Ø¢Ù† Ø§Ø³Øª
        })

      if (insertError) {
        console.error("Failed to save realtime session to DB:", insertError)
        // Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§ÛŒÙ†Ø¬Ø§ Ø®Ø·Ø§ Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†ÛŒØ¯
      }
      return NextResponse.json(session)
    }

    const cs = chatSettings as ExtendedChatSettings
    const maxTokens = pickMaxTokens(cs, selectedModel)
    const temp = typeof cs.temperature === "number" ? cs.temperature : 1
    const hasImage = messages.some(
      (message: any) =>
        Array.isArray(message.content) &&
        message.content.some((part: any) => part.type === "image_url")
    )
    const useStream = !MODELS_THAT_SHOULD_NOT_STREAM.has(selectedModel)
    const enableSearch =
      typeof enableWebSearch === "boolean"
        ? enableWebSearch
        : MODELS_WITH_AUTO_SEARCH.has(selectedModel)
    const useOpenAIWebSearch =
      !!enableSearch &&
      MODELS_WITH_OPENAI_WEB_SEARCH.has(selectedModel) &&
      !hasImage // âœ¨

    // âœ¨ Ù…Ù†Ø·Ù‚ Web Search
    if (useOpenAIWebSearch) {
      // Ø¨Ø®Ø´ Û±: Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØºÛŒØ± Ø§Ø³ØªØ±ÛŒÙ… ÙˆØ¨â€ŒØ³Ø±Ú† (Ú©Ø¯ Ø§ØµÙ„ÛŒ Ø´Ù…Ø§)
      if (["gpt-5", "gpt-5-mini"].includes(selectedModel)) {
        console.log(
          "ğŸš€ [WEB-SEARCH] Entering NON-streaming web search block for model:",
          selectedModel
        )
        const response = await openai.responses.create({
          model: selectedModel,
          input: finalMessages.map(m =>
            m.role === "user"
              ? {
                  role: "user",
                  content: [{ type: "input_text", text: m.content as string }]
                }
              : m
          ) as any,
          tools: [{ type: "web_search" as any }],
          temperature: temp,
          max_output_tokens: maxTokens
        })

        // âœ¨ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù†Ø·Ù‚ Ú©Ø³Ø± Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª ØºÛŒØ± Ø§Ø³ØªØ±ÛŒÙ… ÙˆØ¨â€ŒØ³Ø±Ú†
        const usage = response.usage
        if (usage) {
          const userCostUSD = calculateUserCostUSD(selectedModel, {
            prompt_tokens: usage.input_tokens, // <-- ØªØºÛŒÛŒØ± Ø§Ø² prompt_tokens
            completion_tokens: usage.output_tokens // <-- ØªØºÛŒÛŒØ± Ø§Ø² completion_tokens
          })
          console.log(`ğŸ“Š [WEB-SEARCH] Usage data received:`, usage)
          if (userCostUSD > 0 && wallet) {
            console.log(
              `ğŸ’° Ù‡Ø²ÛŒÙ†Ù‡: ${userCostUSD} | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø§ÙˆÙ„ÛŒÙ‡: ${wallet.balance}`
            )
            await supabase.rpc("deduct_credits_and_log_usage", {
              p_user_id: userId,
              p_model_name: selectedModel,
              p_prompt_tokens: usage.input_tokens, // <-- ØªØºÛŒÛŒØ± Ø¨Ù‡ input_tokens
              p_completion_tokens: usage.output_tokens, // <-- ØªØºÛŒÛŒØ± Ø¨Ù‡ output_tokens
              p_cost: userCostUSD
            })
            const { data: updatedWallet } = await supabase
              .from("wallets")
              .select("balance")
              .eq("user_id", userId)
              .single()
            console.log(
              `âœ… Ø¹Ù…Ù„ÛŒØ§Øª Ù…ÙˆÙÙ‚! | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯: ${updatedWallet?.balance}`
            )
          }
        }

        return new Response(response.output_text ?? "", {
          headers: { "Content-Type": "text/plain; charset=utf-8" }
        })
      }

      // Ø¨Ø®Ø´ Û²: Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ±ÛŒÙ… ÙˆØ¨â€ŒØ³Ø±Ú† (Ù…Ø«Ù„ gpt-4o-mini)
      // Ø§ÛŒÙ† Ø¨Ø®Ø´ ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±ØªÛŒ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø´Ø±Ø· Ø¨Ø§Ù„Ø§ Ø¨Ø±Ù‚Ø±Ø§Ø± Ù†Ø¨Ø§Ø´Ø¯
      const encoder = new TextEncoder()
      const stream = new ReadableStream({
        async start(controller) {
          console.log(
            "ğŸš€ [WEB-SEARCH] Entering STREAMING web search block for model:",
            selectedModel
          )
          let usage: ChatCompletionUsage | undefined

          try {
            const transformedInput = finalMessages.map(m => {
              if (m.role === "user")
                return {
                  ...m,
                  content: [{ type: "input_text", text: m.content as string }]
                }
              if (m.role === "assistant" && typeof m.content === "string")
                return {
                  ...m,
                  content: [{ type: "output_text", text: m.content }]
                }
              return m
            })

            const oaiStream = await openai.responses.stream({
              model: selectedModel,
              input: transformedInput as any,
              tools: [{ type: "web_search" as any }],
              temperature: temp,
              max_output_tokens: maxTokens
            })

            for await (const event of oaiStream as AsyncIterable<any>) {
              // console.log("EVENT FROM OPENAI:", JSON.stringify(event, null, 2));
              if (event.type === "response.output_text.delta") {
                controller.enqueue(encoder.encode(String(event.delta || "")))
              } else if (
                event.type === "response.completed" &&
                event.response?.usage
              ) {
                const receivedUsage = event.response.usage
                usage = {
                  prompt_tokens: receivedUsage.input_tokens,
                  completion_tokens: receivedUsage.output_tokens,
                  total_tokens: receivedUsage.total_tokens
                }
                // Ø§ÛŒÙ† Ù„Ø§Ú¯ Ø­Ø§Ù„Ø§ Ø¨Ø§ÛŒØ¯ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯
                console.log("ğŸ“Š [WEB-SEARCH] Usage data received:", usage)
              }
            }

            if (usage) {
              const userCostUSD = calculateUserCostUSD(selectedModel, usage)
              if (userCostUSD > 0 && wallet) {
                console.log(
                  `ğŸ’° Ù‡Ø²ÛŒÙ†Ù‡: ${userCostUSD} | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø§ÙˆÙ„ÛŒÙ‡: ${wallet.balance}`
                )
                await supabase.rpc("deduct_credits_and_log_usage", {
                  p_user_id: userId,
                  p_model_name: selectedModel,
                  p_prompt_tokens: usage.prompt_tokens,
                  p_completion_tokens: usage.completion_tokens,
                  p_cost: userCostUSD
                })
                const { data: updatedWallet } = await supabase
                  .from("wallets")
                  .select("balance")
                  .eq("user_id", userId)
                  .single()
                console.log(
                  `âœ… Ø¹Ù…Ù„ÛŒØ§Øª Ù…ÙˆÙÙ‚! | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯: ${updatedWallet?.balance}`
                )
              }
            }
          } catch (err: any) {
            console.error("âŒ [WEB-SEARCH] Error in stream:", err)
            controller.enqueue(
              encoder.encode(
                `âŒ Ø®Ø·Ø§ Ø¯Ø± ÙˆØ¨â€ŒØ³Ø±Ú†: ${err?.message || "Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡"}`
              )
            )
          } finally {
            console.log("ğŸšª [WEB-SEARCH] Closing stream controller.")
            controller.close()
          }
        }
      })
      return new Response(stream, {
        headers: {
          "Content-Type": "text/plain; charset=utf-8",
          "Cache-Control": "no-cache, no-transform",
          "X-Accel-Buffering": "no"
        }
      })
    }
    const userPrompt = finalMessages[finalMessages.length - 1]
      ?.content as string

    // âœ¨ Ù…Ù†Ø·Ù‚ Ø§Ø³ØªØ±ÛŒÙ… Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ
    if (useStream) {
      const payload: ChatCompletionCreateParamsStreaming = {
        model: selectedModel,
        messages: finalMessages,
        stream: true,
        temperature: temp
      }

      if (MODELS_NEED_MAX_COMPLETION.has(selectedModel)) {
        ;(payload as any).max_completion_tokens = maxTokens
      } else {
        payload.max_tokens = maxTokens
      }

      const stream = await openai.chat.completions.create(payload)
      const encoder = new TextEncoder()
      const readableStream = new ReadableStream({
        async start(controller) {
          console.log(`ğŸš€ [STREAM-DEBUG] Stream started for user: ${userId}`)

          let usage: ChatCompletionUsage | undefined
          try {
            for await (const chunk of stream) {
              if (chunk.usage) usage = chunk.usage
              console.log("ğŸ“Š [STREAM-DEBUG] Usage data received:", usage)
              const delta = chunk.choices[0]?.delta?.content || ""
              if (delta) controller.enqueue(encoder.encode(delta))
            }
            console.log(
              "ğŸ [STREAM-DEBUG] Stream loop finished. Checking for usage data..."
            )
            if (usage) {
              console.log(
                "âœ… [STREAM-DEBUG] Usage data found. Proceeding with deduction logic."
              )

              const userCostUSD = calculateUserCostUSD(selectedModel, usage)
              console.log(
                `ğŸ’° Model: ${selectedModel}, UserID: ${userId}, CostUSD: ${userCostUSD}, Wallet balance before deduction: ${wallet?.balance}`
              )
              if (userCostUSD > 0 && wallet) {
                await supabase.rpc("deduct_credits_and_log_usage", {
                  p_user_id: userId,
                  p_model_name: selectedModel,
                  p_prompt_tokens: usage.prompt_tokens,
                  p_completion_tokens: usage.completion_tokens,
                  p_cost: userCostUSD
                })
                const { data: updatedWallet } = await supabase
                  .from("wallets")
                  .select("balance")
                  .eq("user_id", userId)
                  .single()

                console.log(
                  `âœ… Ø¹Ù…Ù„ÛŒØ§Øª Ù…ÙˆÙÙ‚! | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯: ${updatedWallet?.balance}`
                )
                console.log(
                  "ğŸ’° Credits deducted:",
                  userCostUSD,
                  "for user:",
                  userId
                )
              }
            } else {
              // ğŸ“Œ fallback ÙˆÙ‚ØªÛŒ usage Ø§Ø² Ø§Ø³ØªØ±ÛŒÙ… Ù†ÛŒØ§Ø¯
              console.log(
                "âš ï¸ No usage data from stream. Trying fallback non-stream request..."
              )
              const usageResponsePayload: ChatCompletionCreateParams = {
                model: selectedModel,
                messages: finalMessages,
                temperature: temp,
                stream: false
              }

              if (MODELS_NEED_MAX_COMPLETION.has(selectedModel)) {
                ;(usageResponsePayload as any).max_completion_tokens = maxTokens
              } else {
                usageResponsePayload.max_tokens = maxTokens
              }

              const usageResponse =
                await openai.chat.completions.create(usageResponsePayload)

              console.log(
                "âœ… FALLBACK RESPONSE:",
                JSON.stringify(usageResponse, null, 2)
              )
              const content = usageResponse.choices[0]?.message?.content
              // if (content) {
              //   controller.enqueue(encoder.encode(content))
              // }
              if (usageResponse.usage) {
                const userCostUSD = calculateUserCostUSD(
                  selectedModel,
                  usageResponse.usage
                )
                console.log(
                  "ğŸ“Š Usage from fallback request:",
                  usageResponse.usage
                )
                console.log(
                  `ğŸ’° [FALLBACK] Model: ${selectedModel}, UserID: ${userId}, CostUSD: ${userCostUSD}, Wallet balance before deduction: ${wallet?.balance}`
                )

                if (userCostUSD > 0 && wallet) {
                  await supabase.rpc("deduct_credits_and_log_usage", {
                    p_user_id: userId,
                    p_model_name: selectedModel,
                    p_prompt_tokens: usageResponse.usage.prompt_tokens,
                    p_completion_tokens: usageResponse.usage.completion_tokens,
                    p_cost: userCostUSD
                  })
                  console.log(
                    `âœ… Credits deducted with fallback | User: ${userId}`
                  )

                  const { data: updatedWallet } = await supabase
                    .from("wallets")
                    .select("balance")
                    .eq("user_id", userId)
                    .single()

                  console.log(
                    `âœ… [FALLBACK] Ø¹Ù…Ù„ÛŒØ§Øª Ù…ÙˆÙÙ‚! | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù‡Ø²ÛŒÙ†Ù‡: ${userCostUSD} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯: ${updatedWallet?.balance}`
                  )
                }
              }
            }
          } catch (err: any) {
            // ++ Ø§ÛŒÙ† Ø¨Ù„ÙˆÚ© CATCH Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª ++
            console.error("âŒ ERROR INSIDE STREAM/FALLBACK:", err)
            const errorMessage = `Ø®Ø·Ø§ÛŒ Ø³Ø±ÙˆØ±: ${err.message || "Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡"}`
            controller.enqueue(encoder.encode(errorMessage))
          } finally {
            // Ø´Ø±ÙˆØ¹ FINALLY
            console.log("ğŸšª [STREAM-DEBUG] Closing stream controller.")
            controller.close()
          }
        }
      })
      return new Response(readableStream, {
        headers: { "Content-Type": "text/event-stream; charset=utf-8" }
      })
    } else {
      const payload: ChatCompletionCreateParams = {
        model: selectedModel,
        messages: finalMessages,
        stream: false,
        temperature: temp
      }
      if (MODELS_NEED_MAX_COMPLETION.has(selectedModel)) {
        ;(payload as any).max_completion_tokens = maxTokens
      } else {
        payload.max_tokens = maxTokens
      }
      const response = await openai.chat.completions.create(payload)
      const content = response.choices[0].message.content ?? ""
      const usage = response.usage
      console.log("ğŸ’¡ Checking wallet and usage...")
      if (usage) {
        console.log("ğŸ’¡ Usage exists:", usage)
        const userCostUSD = calculateUserCostUSD(selectedModel, usage)
        console.log(
          `ğŸ’° Model: ${selectedModel}, UserID: ${userId}, CostUSD: ${userCostUSD}, Wallet balance before deduction: ${wallet?.balance}`
        )
        if (!wallet || wallet.balance < userCostUSD)
          return NextResponse.json(
            { message: "Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø´Ù…Ø§ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª." },
            { status: 402 }
          )
        if (userCostUSD > 0) {
          console.log("â³ Trying to deduct credits now...")
          await supabase.rpc("deduct_credits_and_log_usage", {
            p_user_id: userId,
            p_model_name: selectedModel,
            p_prompt_tokens: usage.prompt_tokens,
            p_completion_tokens: usage.completion_tokens,
            p_cost: userCostUSD
          })
          console.log(
            `âœ… Credits deducted for UserID: ${userId}, CostUSD: ${userCostUSD}`
          )
        }
      }
      return new Response(content, {
        headers: { "Content-Type": "text/plain; charset=utf-8" }
      })
    }
  } catch (error: any) {
    console.error("!!! FULL BACKEND ERROR CATCH !!!:", error)
    const errorMessage = error.message || "ÛŒÚ© Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø±Ø® Ø¯Ø§Ø¯"
    const status = error.status || 500
    return NextResponse.json({ message: errorMessage }, { status })
  }
}
