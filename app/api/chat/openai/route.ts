import { checkApiKey, getServerProfile } from "@/lib/server/server-chat-helpers"
import { ChatSettings } from "@/types"
import { ServerRuntime } from "next"
import OpenAI from "openai"
import type {
  ChatCompletionCreateParams,
  ChatCompletionCreateParamsStreaming
} from "openai/resources/chat/completions"
import { NextResponse } from "next/server"
import { MODEL_PROMPTS } from "@/lib/build-prompt"

export const runtime: ServerRuntime = "edge"

type ExtendedChatSettings = ChatSettings & {
  maxTokens?: number
  max_tokens?: number
}

// ØªØ¹Ø±ÛŒÙ SetÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
const MODELS_NEED_MAX_COMPLETION = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-mini"
])
const MODELS_WITH_OPENAI_WEB_SEARCH = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-mini"
])
const MODELS_THAT_SHOULD_NOT_STREAM = new Set(["gpt-5", "gpt-5-mini"])
const MODELS_WITH_AUTO_SEARCH = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-mini"
])

function pickMaxTokens(cs: ExtendedChatSettings): number {
  return Math.min(cs.maxTokens ?? cs.max_tokens ?? 10000, 12000)
}

export async function POST(request: Request) {
  try {
    const { chatSettings, messages, enableWebSearch } = await request.json()

    const profile = await getServerProfile()
    checkApiKey(profile.openai_api_key, "OpenAI")

    const openai = new OpenAI({
      apiKey: profile.openai_api_key || "",
      organization: profile.openai_organization_id
    })

    const selectedModel = chatSettings.model || "gpt-4o-mini"

    // Ù‡Ø¯Ø§ÛŒØª Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ DALL-E 3 Ø¨Ù‡ /api/dalle
    if (selectedModel === "dall-e-3") {
      return NextResponse.json(
        { message: "DALL-E 3 requests should be sent to /api/dalle/route.ts" },
        { status: 400 }
      )
    }

    // âœ… Ù…Ù†Ø·Ù‚ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Realtime Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ÛŒ Ø§ØµÙˆÙ„ÛŒ
    if (selectedModel.includes("realtime")) {
      console.log(`Creating realtime session for model: ${selectedModel}`)

      // Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² MODEL_PROMPTS Ø¨Ø®ÙˆÙ†
      const systemPrompt =
        MODEL_PROMPTS[selectedModel] ?? "You are Rhyno realtime assistant."

      const response = await fetch(
        "https://api.openai.com/v1/realtime/sessions",
        {
          method: "POST",
          headers: {
            Authorization: `Bearer ${profile.openai_api_key}`,
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            model: selectedModel,
            voice: "alloy",
            instructions:
              "You are Rhyno realtime assistant. Use the web_search function when needed.",
            tools: [
              {
                type: "function",
                name: "web_search",
                description: "Search the web for up-to-date information",
                parameters: {
                  type: "object",
                  properties: {
                    query: {
                      type: "string",
                      description: "The search query to look up online"
                    }
                  },
                  required: ["query"]
                }
              }
            ]
          })
        }
      )

      // âœ¨ Ø§ØµÙ„Ø§Ø­ Ú©Ù„ÛŒØ¯ÛŒ: Ø§Ú¯Ø± OpenAI Ø®Ø·Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯ØŒ Ø¢Ù† Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†
      if (!response.ok) {
        const errorBody = await response.json()
        console.error("Error creating OpenAI realtime session:", errorBody)
        // Ù¾Ø±ØªØ§Ø¨ Ø®Ø·Ø§ Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø¨Ù‡ Ø¨Ù„Ø§Ú© catch Ø§ØµÙ„ÛŒ Ø¨Ø±ÙˆÛŒÙ… Ùˆ Ù¾Ø§Ø³Ø® Ø®Ø·Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø§Ø±Ø³Ø§Ù„ Ø´ÙˆØ¯
        throw new Error(
          errorBody.error?.message || "Failed to create realtime session"
        )
      }

      const session = await response.json()
      return NextResponse.json(session)
    }

    // Ù…Ù†Ø·Ù‚ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØºÛŒØ± DALL-E 3
    const cs = chatSettings as ExtendedChatSettings
    const maxTokens = pickMaxTokens(cs)

    let enableSearch: boolean
    if (typeof enableWebSearch === "boolean") {
      enableSearch = enableWebSearch
    } else {
      enableSearch = MODELS_WITH_AUTO_SEARCH.has(selectedModel)
    }

    const temp =
      typeof cs.temperature === "number"
        ? cs.temperature
        : ["gpt-4o", "gpt-4o-mini", "gpt-5", "gpt-5-mini"].includes(
              selectedModel
            )
          ? 1
          : 0.7

    const useStream = !MODELS_THAT_SHOULD_NOT_STREAM.has(selectedModel)
    const useOpenAIWebSearch =
      !!enableSearch && MODELS_WITH_OPENAI_WEB_SEARCH.has(selectedModel)

    if (useOpenAIWebSearch) {
      // Ø§Ú¯Ø± Ù…Ø¯Ù„ gpt-5 ÛŒØ§ gpt-5-mini Ø¨Ø§Ø´Ù‡ â†’ Ø§Ø³ØªØ±ÛŒÙ… Ù†Ú©Ù†
      if (["gpt-5", "gpt-5-mini"].includes(selectedModel)) {
        const response = await openai.responses.create({
          model: selectedModel,
          input: messages.map((m: any) =>
            m.role === "user"
              ? {
                  role: "user",
                  content: [{ type: "input_text", text: m.content }]
                }
              : m
          ),
          tools: [{ type: "web_search" as any }],
          temperature: temp,
          max_output_tokens: maxTokens
        })

        return new Response(response.output_text ?? "", {
          headers: { "Content-Type": "text/plain; charset=utf-8" }
        })
      }

      // Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ù‡ (Ù…Ø«Ù„Ø§Ù‹ gpt-4o) Ù‡Ù…Ú†Ù†Ø§Ù† Ø§Ø³ØªØ±ÛŒÙ… Ú©Ù†
      const encoder = new TextEncoder()
      const stream = new ReadableStream({
        async start(controller) {
          try {
            const transformedInput = (messages ?? []).map((m: any) => {
              if (m.role === "user") {
                return {
                  ...m,
                  content: [{ type: "input_text", text: m.content }]
                }
              }
              if (m.role === "assistant" && typeof m.content === "string") {
                return {
                  ...m,
                  content: [{ type: "output_text", text: m.content }]
                }
              }
              return m
            })

            const oaiStream = await openai.responses.stream({
              model: selectedModel,
              input: transformedInput,
              tools: [{ type: "web_search" as any }], // ğŸ‘ˆ ØªØºÛŒÛŒØ± Ø¨Ù‡ web_search
              temperature: temp,
              max_output_tokens: maxTokens
            })

            for await (const event of oaiStream as AsyncIterable<any>) {
              if (event.type === "response.output_text.delta") {
                controller.enqueue(encoder.encode(String(event.delta || "")))
              } else if (event.type === "response.error" && "error" in event) {
                const msg = String(event.error?.message || "Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡")
                controller.enqueue(encoder.encode(`\nâŒ ${msg}`))
              }
            }
          } catch (err: any) {
            controller.enqueue(
              encoder.encode(
                `âŒ Ø®Ø·Ø§ Ø¯Ø± ÙˆØ¨â€ŒØ³Ø±Ú†: ${err?.message || "Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡"}`
              )
            )
          } finally {
            controller.close()
          }
        }
      })

      return new Response(stream, {
        headers: {
          "Content-Type": "text/plain; charset=utf-8",
          "Cache-Control": "no-cache, no-transform",
          "X-Accel-Buffering": "no"
        }
      })
    }

    if (useStream) {
      console.log(`ğŸ“¡ Model "${selectedModel}" is using streaming.`)
      const payload: ChatCompletionCreateParamsStreaming = {
        model: selectedModel,
        messages,
        stream: true,
        temperature: temp,
        presence_penalty: 0,
        frequency_penalty: 0,
        max_tokens: maxTokens
      }

      if (MODELS_NEED_MAX_COMPLETION.has(selectedModel)) {
        ;(payload as any).max_completion_tokens = maxTokens
      } else {
        payload.max_tokens = maxTokens
      }

      const stream = await openai.chat.completions.create(payload)

      return new Response(stream.toReadableStream(), {
        headers: {
          "Content-Type": "text/event-stream; charset=utf-8",
          "Cache-Control": "no-cache, no-transform",
          "X-Accel-Buffering": "no"
        }
      })
    } else {
      const payload: ChatCompletionCreateParams = {
        model: selectedModel,
        messages,
        stream: false,
        temperature: temp,
        presence_penalty: 0,
        frequency_penalty: 0
      }

      if (MODELS_NEED_MAX_COMPLETION.has(selectedModel)) {
        ;(payload as any).max_completion_tokens = maxTokens
      } else {
        payload.max_tokens = maxTokens
      }

      const response = await openai.chat.completions.create(payload)
      const content = response.choices[0].message.content ?? ""

      return new Response(content, {
        headers: {
          "Content-Type": "text/plain; charset=utf-8"
        }
      })
    }
  } catch (error: any) {
    console.error("!!! FULL BACKEND ERROR CATCH !!!:", error)
    const errorMessage = error.message || "ÛŒÚ© Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø±Ø® Ø¯Ø§Ø¯"
    const status = error.status || 500
    return new Response(JSON.stringify({ message: errorMessage }), {
      status: status,
      headers: { "Content-Type": "application/json" }
    })
  }
}
