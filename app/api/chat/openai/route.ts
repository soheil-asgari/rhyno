import { checkApiKey, getServerProfile } from "@/lib/server/server-chat-helpers"
import { ChatSettings, LLMID } from "@/types"
import { ServerRuntime } from "next"
import OpenAI from "openai"
import type {
  ChatCompletionCreateParams,
  ChatCompletionCreateParamsStreaming
} from "openai/resources/chat/completions"
import { NextResponse } from "next/server"
import { MODEL_PROMPTS } from "@/lib/build-prompt"

// âœ¨ Ø§ÛŒÙ…Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø®Øª Ùˆ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª
import { createServerClient } from "@supabase/ssr"
import { cookies } from "next/headers"
import { OPENAI_LLM_LIST } from "@/lib/models/llm/openai-llm-list"
import { handleTTS } from "@/app/api/chat/handlers/tts"

// Ø§Ø² Node.js runtime Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
export const runtime: ServerRuntime = "nodejs"

function isImageRequest(prompt: string): boolean {
  const keywords = ["Ø¹Ú©Ø³", "ØªØµÙˆÛŒØ±", "picture", "image", "generate image"]
  return keywords.some(word => prompt.toLowerCase().includes(word))
}

type ChatCompletionUsage = {
  prompt_tokens: number
  completion_tokens: number
  total_tokens: number
}

type ExtendedChatSettings = ChatSettings & {
  maxTokens?: number
  max_tokens?: number
}

// âœ¨ Ø«Ø§Ø¨Øªâ€ŒÙ‡Ø§ Ùˆ ØªØ§Ø¨Ø¹ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡
const pricingMap = new Map(
  OPENAI_LLM_LIST.map(llm => [llm.modelId, llm.pricing])
)
const PROFIT_MARGIN = 1.4

function calculateUserCostUSD(
  modelId: LLMID,
  usage: { prompt_tokens: number; completion_tokens: number }
): number {
  const pricing = pricingMap.get(modelId)
  if (!pricing?.inputCost || !pricing.outputCost) return 0
  const promptCost = (usage.prompt_tokens / 1_000_000) * pricing.inputCost
  const completionCost =
    (usage.completion_tokens / 1_000_000) * pricing.outputCost
  return (promptCost + completionCost) * PROFIT_MARGIN
}

const MODELS_NEED_MAX_COMPLETION = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-nano",
  "gpt-5-mini"
])
const MODELS_WITH_OPENAI_WEB_SEARCH = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-mini"
])
const MODELS_THAT_SHOULD_NOT_STREAM = new Set(["gpt-5", "gpt-5-mini"])
const MODELS_WITH_AUTO_SEARCH = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-mini"
])

const MODEL_MAX_TOKENS: Record<string, number> = {
  "gpt-4o": 8192,
  "gpt-4o-mini": 4096,
  "gpt-5": 12000,
  "gpt-5-mini": 12000,
  "gpt-3.5-turbo": 4096,
  "gpt-3.5-turbo-16k": 16384
  // Ø³Ø§ÛŒØ± Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
}
function pickMaxTokens(cs: ExtendedChatSettings, modelId: string): number {
  const requestedTokens = cs.maxTokens ?? cs.max_tokens ?? 4096
  const modelLimit = MODEL_MAX_TOKENS[modelId] ?? 4096
  // Ù…Ù‚Ø¯Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ù†Ø¨Ø§ÛŒØ¯ Ø§Ø² Ø³Ù‚Ù Ù…Ø¯Ù„ Ø¨ÛŒØ´ØªØ± Ø´ÙˆØ¯
  return Math.min(requestedTokens, modelLimit)
}

export async function POST(request: Request) {
  console.log("ğŸ”¥ğŸ”¥ğŸ”¥ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ API Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯! Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´... ğŸ”¥ğŸ”¥ğŸ”¥")
  try {
    const { chatSettings, messages, enableWebSearch } = await request.json()

    // âœ¨ Ø´Ø±ÙˆØ¹ Ø¨Ø®Ø´ Ù¾Ø±Ø¯Ø§Ø®Øª Ùˆ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª
    const cookieStore = cookies()
    const supabase = createServerClient(
      process.env.NEXT_PUBLIC_SUPABASE_URL!,
      process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
      { cookies: { get: (name: string) => cookieStore.get(name)?.value } }
    )

    const {
      data: { user }
    } = await supabase.auth.getUser()
    if (!user) return new NextResponse("Unauthorized", { status: 401 })
    const userId = user.id

    const { data: wallet, error: walletError } = await supabase
      .from("wallets")
      .select("balance")
      .eq("user_id", userId)
      .single()

    if (walletError && walletError.code === "PGRST116") {
      return NextResponse.json(
        { message: "Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø´Ù…Ø§ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª." },
        { status: 402 }
      )
    } else if (walletError) {
      throw walletError
    }

    if (!wallet || wallet.balance <= 0) {
      return NextResponse.json(
        { message: "Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø´Ù…Ø§ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª." },
        { status: 402 }
      )
    }
    // âœ¨ Ù¾Ø§ÛŒØ§Ù† Ø¨Ø®Ø´ Ù¾Ø±Ø¯Ø§Ø®Øª Ùˆ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª

    const profile = await getServerProfile()
    checkApiKey(profile.openai_api_key, "OpenAI")
    const openai = new OpenAI({
      apiKey: profile.openai_api_key || "",
      organization: profile.openai_organization_id
    })

    const selectedModel = (chatSettings.model || "gpt-4o-mini") as LLMID
    // Ø§Ú¯Ø± Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ú¯ÙØªØ§Ø± Ø§Ø³ØªØŒ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ú©Ù†ØªØ±Ù„â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ù…Ø±Ø¨ÙˆØ·Ù‡ Ø¨ÙØ±Ø³Øª
    if (selectedModel === "gpt-4o-mini-tts") {
      console.log("ğŸ”Š Ø¯Ø±Ø®ÙˆØ§Ø³Øª TTS Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ handleTTS...")

      // Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±
      const input = messages[messages.length - 1]?.content || ""
      if (!input) {
        return NextResponse.json(
          { message: "Input text is required for TTS." },
          { status: 400 }
        )
      }

      // Ø¨Ø¯Ù†Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø±Ø§ Ø¨Ø±Ø§ÛŒ handleTTS Ø¨Ø³Ø§Ø²
      const ttsBody = {
        input,
        voice: chatSettings.voice || "coral",
        speed: chatSettings.speed || 1.0, // Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØµØ¯Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯
        model: selectedModel
      }

      // Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø±Ø§ Ø¨Ù‡ Ú©Ù†ØªØ±Ù„â€ŒÚ©Ù†Ù†Ø¯Ù‡ TTS Ø§Ø±Ø³Ø§Ù„ Ú©Ø±Ø¯Ù‡ Ùˆ Ù†ØªÛŒØ¬Ù‡ Ø±Ø§ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†
      return await handleTTS({
        body: ttsBody,
        user,
        supabase
      })
    }
    // âœ¨ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ§Ù… Ø³ÛŒØ³ØªÙ…
    const finalMessages = [
      {
        role: "system",
        content:
          MODEL_PROMPTS[selectedModel] || "You are a helpful AI assistant."
      },
      ...(Array.isArray(messages) ? messages : [])
    ]

    if (selectedModel === "dall-e-3") {
      return NextResponse.json(
        { message: "DALL-E 3 requests should be sent to /api/dalle/route.ts" },
        { status: 400 }
      )
    }

    if (selectedModel.includes("realtime")) {
      const response = await fetch(
        "https://api.openai.com/v1/realtime/sessions",
        {
          method: "POST",
          headers: {
            Authorization: `Bearer ${profile.openai_api_key}`,
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            model: selectedModel,
            voice: "alloy",
            instructions: `
  You are Rhyno, a realtime Persian-speaking assistant.
  âœ… Always respond in Persian (Farsi).
  âœ… Only speak in voice (no text output).
  âœ… Introduce yourself as Rhyno when asked.
  âœ… Keep your answers short and concise. Do not over-explain.
`,

            tools: [
              {
                type: "function",
                name: "web_search",
                description: "Search the web for up-to-date information",
                parameters: {
                  type: "object",
                  properties: { query: { type: "string" } },
                  required: ["query"]
                }
              }
            ]
          })
        }
      )

      if (!response.ok) {
        const errorBody = await response.json()
        throw new Error(
          errorBody.error?.message || "Failed to create realtime session"
        )
      }
      const session = await response.json()
      console.log("ğŸŒ Realtime session raw response:", session)
      console.log("ğŸ”Š Session modalities:", session.modalities)
      console.log("ğŸ”Š Session voice:", session.voice)
      console.log("ğŸ”Š Session instructions:", session.instructions)

      const { error: insertError } = await supabase
        .from("realtime_sessions")
        .insert({
          user_id: userId,
          openai_session_id: session.id // ÛŒØ§ Ù‡Ø± ÙÛŒÙ„Ø¯ÛŒ Ú©Ù‡ ID Ø¬Ù„Ø³Ù‡ Ø¯Ø± Ø¢Ù† Ø§Ø³Øª
        })

      if (insertError) {
        console.error("Failed to save realtime session to DB:", insertError)
        // Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§ÛŒÙ†Ø¬Ø§ Ø®Ø·Ø§ Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†ÛŒØ¯
      }
      return NextResponse.json(session)
    }

    const cs = chatSettings as ExtendedChatSettings
    const maxTokens = pickMaxTokens(cs, selectedModel)
    const temp = typeof cs.temperature === "number" ? cs.temperature : 1
    const hasImage = messages.some(
      (message: any) =>
        Array.isArray(message.content) &&
        message.content.some((part: any) => part.type === "image_url")
    )
    const useStream = !MODELS_THAT_SHOULD_NOT_STREAM.has(selectedModel)
    const enableSearch =
      typeof enableWebSearch === "boolean"
        ? enableWebSearch
        : MODELS_WITH_AUTO_SEARCH.has(selectedModel)
    const useOpenAIWebSearch =
      !!enableSearch &&
      MODELS_WITH_OPENAI_WEB_SEARCH.has(selectedModel) &&
      !hasImage // âœ¨

    // âœ¨ Ù…Ù†Ø·Ù‚ Web Search
    if (useOpenAIWebSearch) {
      // Ø¨Ø®Ø´ Û±: Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØºÛŒØ± Ø§Ø³ØªØ±ÛŒÙ… ÙˆØ¨â€ŒØ³Ø±Ú† (Ú©Ø¯ Ø§ØµÙ„ÛŒ Ø´Ù…Ø§)
      if (["gpt-5", "gpt-5-mini"].includes(selectedModel)) {
        console.log(
          "ğŸš€ [WEB-SEARCH] Entering NON-streaming web search block for model:",
          selectedModel
        )
        const response = await openai.responses.create({
          model: selectedModel,
          input: finalMessages.map(m =>
            m.role === "user"
              ? {
                  role: "user",
                  content: [{ type: "input_text", text: m.content as string }]
                }
              : m
          ) as any,
          tools: [{ type: "web_search" as any }],
          temperature: temp,
          max_output_tokens: maxTokens
        })

        // âœ¨ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù†Ø·Ù‚ Ú©Ø³Ø± Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª ØºÛŒØ± Ø§Ø³ØªØ±ÛŒÙ… ÙˆØ¨â€ŒØ³Ø±Ú†
        const usage = response.usage
        if (usage) {
          const userCostUSD = calculateUserCostUSD(selectedModel, {
            prompt_tokens: usage.input_tokens, // <-- ØªØºÛŒÛŒØ± Ø§Ø² prompt_tokens
            completion_tokens: usage.output_tokens // <-- ØªØºÛŒÛŒØ± Ø§Ø² completion_tokens
          })
          console.log(`ğŸ“Š [WEB-SEARCH] Usage data received:`, usage)
          if (userCostUSD > 0 && wallet) {
            console.log(
              `ğŸ’° Ù‡Ø²ÛŒÙ†Ù‡: ${userCostUSD} | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø§ÙˆÙ„ÛŒÙ‡: ${wallet.balance}`
            )
            await supabase.rpc("deduct_credits_and_log_usage", {
              p_user_id: userId,
              p_model_name: selectedModel,
              p_prompt_tokens: usage.input_tokens, // <-- ØªØºÛŒÛŒØ± Ø¨Ù‡ input_tokens
              p_completion_tokens: usage.output_tokens, // <-- ØªØºÛŒÛŒØ± Ø¨Ù‡ output_tokens
              p_cost: userCostUSD
            })
            const { data: updatedWallet } = await supabase
              .from("wallets")
              .select("balance")
              .eq("user_id", userId)
              .single()
            console.log(
              `âœ… Ø¹Ù…Ù„ÛŒØ§Øª Ù…ÙˆÙÙ‚! | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯: ${updatedWallet?.balance}`
            )
          }
        }

        return new Response(response.output_text ?? "", {
          headers: { "Content-Type": "text/plain; charset=utf-8" }
        })
      }

      // Ø¨Ø®Ø´ Û²: Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ±ÛŒÙ… ÙˆØ¨â€ŒØ³Ø±Ú† (Ù…Ø«Ù„ gpt-4o-mini)
      // Ø§ÛŒÙ† Ø¨Ø®Ø´ ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±ØªÛŒ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø´Ø±Ø· Ø¨Ø§Ù„Ø§ Ø¨Ø±Ù‚Ø±Ø§Ø± Ù†Ø¨Ø§Ø´Ø¯
      const encoder = new TextEncoder()
      const stream = new ReadableStream({
        async start(controller) {
          console.log(
            "ğŸš€ [WEB-SEARCH] Entering STREAMING web search block for model:",
            selectedModel
          )
          let usage: ChatCompletionUsage | undefined

          try {
            const transformedInput = finalMessages.map(m => {
              if (m.role === "user")
                return {
                  ...m,
                  content: [{ type: "input_text", text: m.content as string }]
                }
              if (m.role === "assistant" && typeof m.content === "string")
                return {
                  ...m,
                  content: [{ type: "output_text", text: m.content }]
                }
              return m
            })

            const oaiStream = await openai.responses.stream({
              model: selectedModel,
              input: transformedInput as any,
              tools: [{ type: "web_search" as any }],
              temperature: temp,
              max_output_tokens: maxTokens
            })

            for await (const event of oaiStream as AsyncIterable<any>) {
              // console.log("EVENT FROM OPENAI:", JSON.stringify(event, null, 2));
              if (event.type === "response.output_text.delta") {
                controller.enqueue(encoder.encode(String(event.delta || "")))
              } else if (
                event.type === "response.completed" &&
                event.response?.usage
              ) {
                const receivedUsage = event.response.usage
                usage = {
                  prompt_tokens: receivedUsage.input_tokens,
                  completion_tokens: receivedUsage.output_tokens,
                  total_tokens: receivedUsage.total_tokens
                }
                // Ø§ÛŒÙ† Ù„Ø§Ú¯ Ø­Ø§Ù„Ø§ Ø¨Ø§ÛŒØ¯ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯
                console.log("ğŸ“Š [WEB-SEARCH] Usage data received:", usage)
              }
            }

            if (usage) {
              const userCostUSD = calculateUserCostUSD(selectedModel, usage)
              if (userCostUSD > 0 && wallet) {
                console.log(
                  `ğŸ’° Ù‡Ø²ÛŒÙ†Ù‡: ${userCostUSD} | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø§ÙˆÙ„ÛŒÙ‡: ${wallet.balance}`
                )
                await supabase.rpc("deduct_credits_and_log_usage", {
                  p_user_id: userId,
                  p_model_name: selectedModel,
                  p_prompt_tokens: usage.prompt_tokens,
                  p_completion_tokens: usage.completion_tokens,
                  p_cost: userCostUSD
                })
                const { data: updatedWallet } = await supabase
                  .from("wallets")
                  .select("balance")
                  .eq("user_id", userId)
                  .single()
                console.log(
                  `âœ… Ø¹Ù…Ù„ÛŒØ§Øª Ù…ÙˆÙÙ‚! | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯: ${updatedWallet?.balance}`
                )
              }
            }
          } catch (err: any) {
            console.error("âŒ [WEB-SEARCH] Error in stream:", err)
            controller.enqueue(
              encoder.encode(
                `âŒ Ø®Ø·Ø§ Ø¯Ø± ÙˆØ¨â€ŒØ³Ø±Ú†: ${err?.message || "Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡"}`
              )
            )
          } finally {
            console.log("ğŸšª [WEB-SEARCH] Closing stream controller.")
            controller.close()
          }
        }
      })
      return new Response(stream, {
        headers: {
          "Content-Type": "text/plain; charset=utf-8",
          "Cache-Control": "no-cache, no-transform",
          "X-Accel-Buffering": "no"
        }
      })
    }
    const userPrompt = finalMessages[finalMessages.length - 1]
      ?.content as string

    // âœ¨ Ù…Ù†Ø·Ù‚ Ø§Ø³ØªØ±ÛŒÙ… Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ
    if (useStream) {
      const payload: ChatCompletionCreateParamsStreaming = {
        model: selectedModel,
        messages: finalMessages,
        stream: true,
        temperature: temp
      }

      if (MODELS_NEED_MAX_COMPLETION.has(selectedModel)) {
        ;(payload as any).max_completion_tokens = maxTokens
      } else {
        payload.max_tokens = maxTokens
      }

      const stream = await openai.chat.completions.create(payload)
      const encoder = new TextEncoder()
      const readableStream = new ReadableStream({
        async start(controller) {
          console.log(`ğŸš€ [STREAM-DEBUG] Stream started for user: ${userId}`)

          let usage: ChatCompletionUsage | undefined
          try {
            for await (const chunk of stream) {
              if (chunk.usage) usage = chunk.usage
              console.log("ğŸ“Š [STREAM-DEBUG] Usage data received:", usage)
              const delta = chunk.choices[0]?.delta?.content || ""
              if (delta) controller.enqueue(encoder.encode(delta))
            }
            console.log(
              "ğŸ [STREAM-DEBUG] Stream loop finished. Checking for usage data..."
            )
            if (usage) {
              console.log(
                "âœ… [STREAM-DEBUG] Usage data found. Proceeding with deduction logic."
              )

              const userCostUSD = calculateUserCostUSD(selectedModel, usage)
              console.log(
                `ğŸ’° Model: ${selectedModel}, UserID: ${userId}, CostUSD: ${userCostUSD}, Wallet balance before deduction: ${wallet?.balance}`
              )
              if (userCostUSD > 0 && wallet) {
                await supabase.rpc("deduct_credits_and_log_usage", {
                  p_user_id: userId,
                  p_model_name: selectedModel,
                  p_prompt_tokens: usage.prompt_tokens,
                  p_completion_tokens: usage.completion_tokens,
                  p_cost: userCostUSD
                })
                const { data: updatedWallet } = await supabase
                  .from("wallets")
                  .select("balance")
                  .eq("user_id", userId)
                  .single()

                console.log(
                  `âœ… Ø¹Ù…Ù„ÛŒØ§Øª Ù…ÙˆÙÙ‚! | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯: ${updatedWallet?.balance}`
                )
                console.log(
                  "ğŸ’° Credits deducted:",
                  userCostUSD,
                  "for user:",
                  userId
                )
              }
            } else {
              // ğŸ“Œ fallback ÙˆÙ‚ØªÛŒ usage Ø§Ø² Ø§Ø³ØªØ±ÛŒÙ… Ù†ÛŒØ§Ø¯
              console.log(
                "âš ï¸ No usage data from stream. Trying fallback non-stream request..."
              )
              const usageResponsePayload: ChatCompletionCreateParams = {
                model: selectedModel,
                messages: finalMessages,
                temperature: temp,
                stream: false
              }

              if (MODELS_NEED_MAX_COMPLETION.has(selectedModel)) {
                ;(usageResponsePayload as any).max_completion_tokens = maxTokens
              } else {
                usageResponsePayload.max_tokens = maxTokens
              }

              const usageResponse =
                await openai.chat.completions.create(usageResponsePayload)

              console.log(
                "âœ… FALLBACK RESPONSE:",
                JSON.stringify(usageResponse, null, 2)
              )
              const content = usageResponse.choices[0]?.message?.content
              if (content) {
                controller.enqueue(encoder.encode(content))
              }
              if (usageResponse.usage) {
                const userCostUSD = calculateUserCostUSD(
                  selectedModel,
                  usageResponse.usage
                )
                console.log(
                  "ğŸ“Š Usage from fallback request:",
                  usageResponse.usage
                )
                console.log(
                  `ğŸ’° [FALLBACK] Model: ${selectedModel}, UserID: ${userId}, CostUSD: ${userCostUSD}, Wallet balance before deduction: ${wallet?.balance}`
                )

                if (userCostUSD > 0 && wallet) {
                  await supabase.rpc("deduct_credits_and_log_usage", {
                    p_user_id: userId,
                    p_model_name: selectedModel,
                    p_prompt_tokens: usageResponse.usage.prompt_tokens,
                    p_completion_tokens: usageResponse.usage.completion_tokens,
                    p_cost: userCostUSD
                  })
                  console.log(
                    `âœ… Credits deducted with fallback | User: ${userId}`
                  )

                  const { data: updatedWallet } = await supabase
                    .from("wallets")
                    .select("balance")
                    .eq("user_id", userId)
                    .single()

                  console.log(
                    `âœ… [FALLBACK] Ø¹Ù…Ù„ÛŒØ§Øª Ù…ÙˆÙÙ‚! | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù‡Ø²ÛŒÙ†Ù‡: ${userCostUSD} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯: ${updatedWallet?.balance}`
                  )
                }
              }
            }
          } catch (err: any) {
            // ++ Ø§ÛŒÙ† Ø¨Ù„ÙˆÚ© CATCH Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª ++
            console.error("âŒ ERROR INSIDE STREAM/FALLBACK:", err)
            const errorMessage = `Ø®Ø·Ø§ÛŒ Ø³Ø±ÙˆØ±: ${err.message || "Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡"}`
            controller.enqueue(encoder.encode(errorMessage))
          } finally {
            // Ø´Ø±ÙˆØ¹ FINALLY
            console.log("ğŸšª [STREAM-DEBUG] Closing stream controller.")
            controller.close()
          }
        }
      })
      return new Response(readableStream, {
        headers: { "Content-Type": "text/event-stream; charset=utf-8" }
      })
    } else {
      const payload: ChatCompletionCreateParams = {
        model: selectedModel,
        messages: finalMessages,
        stream: false,
        temperature: temp
      }
      if (MODELS_NEED_MAX_COMPLETION.has(selectedModel)) {
        ;(payload as any).max_completion_tokens = maxTokens
      } else {
        payload.max_tokens = maxTokens
      }
      const response = await openai.chat.completions.create(payload)
      const content = response.choices[0].message.content ?? ""
      const usage = response.usage
      console.log("ğŸ’¡ Checking wallet and usage...")
      if (usage) {
        console.log("ğŸ’¡ Usage exists:", usage)
        const userCostUSD = calculateUserCostUSD(selectedModel, usage)
        console.log(
          `ğŸ’° Model: ${selectedModel}, UserID: ${userId}, CostUSD: ${userCostUSD}, Wallet balance before deduction: ${wallet?.balance}`
        )
        if (!wallet || wallet.balance < userCostUSD)
          return NextResponse.json(
            { message: "Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø´Ù…Ø§ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª." },
            { status: 402 }
          )
        if (userCostUSD > 0) {
          console.log("â³ Trying to deduct credits now...")
          await supabase.rpc("deduct_credits_and_log_usage", {
            p_user_id: userId,
            p_model_name: selectedModel,
            p_prompt_tokens: usage.prompt_tokens,
            p_completion_tokens: usage.completion_tokens,
            p_cost: userCostUSD
          })
          console.log(
            `âœ… Credits deducted for UserID: ${userId}, CostUSD: ${userCostUSD}`
          )
        }
      }
      return new Response(content, {
        headers: { "Content-Type": "text/plain; charset=utf-8" }
      })
    }
  } catch (error: any) {
    console.error("!!! FULL BACKEND ERROR CATCH !!!:", error)
    const errorMessage = error.message || "ÛŒÚ© Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø±Ø® Ø¯Ø§Ø¯"
    const status = error.status || 500
    return NextResponse.json({ message: errorMessage }, { status })
  }
}
