import { checkApiKey, getServerProfile } from "@/lib/server/server-chat-helpers"
import { ChatSettings, LLMID } from "@/types"
import { ServerRuntime } from "next"
import OpenAI from "openai"
import type {
  ChatCompletionCreateParams,
  ChatCompletionCreateParamsStreaming
} from "openai/resources/chat/completions"
import { NextResponse } from "next/server"
import { MODEL_PROMPTS } from "@/lib/build-prompt"

import { createClient as createSSRClient } from "@/lib/supabase/server"
import { cookies } from "next/headers"
import { OPENAI_LLM_LIST } from "@/lib/models/llm/openai-llm-list"
import { handleTTS } from "@/app/api/chat/handlers/tts"
import { modelsWithRial } from "@/app/checkout/pricing"
import { handleSTT } from "@/app/api/chat/handlers/stt"
import jwt from "jsonwebtoken"
import { createClient as createAdminClient } from "@supabase/supabase-js"
// âœ… Ø§ÛŒÙ† Ø®Ø· Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
import { encode } from "gpt-tokenizer"
import { quickResponses } from "@/lib/quick-responses"
import { StreamingTextResponse } from "ai"

// Ø§Ø² Node.js runtime Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
export const runtime: ServerRuntime = "nodejs"
export const maxDuration = 60

// --- â¬‡ï¸ ØªØºÛŒÛŒØ± Û±: Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ OpenRouter Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ ØªØ¹Ø±ÛŒÙ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ---
const OPENROUTER_GEMINI_MODEL_ID = "google/gemini-2.5-flash-image"

/**
 * Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ú©Ù†ØªØ±Ù„â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø§Ø®ØªØµØ§ØµÛŒ OpenRouter Ù‡Ø¯Ø§ÛŒØª Ø´ÙˆÙ†Ø¯.
 * Ø§ÛŒÙ† Ú©Ù†ØªØ±Ù„â€ŒÚ©Ù†Ù†Ø¯Ù‡ (/api/chat/openrouter) Ù…Ø³Ø¦ÙˆÙ„ ØªÙ…Ø§Ø³ Ø¨Ø§ API OpenRouter
 * Ùˆ Ø§Ø³ØªØ±ÛŒÙ… Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø® Ø§Ø³Øª.
 */
const OPENROUTER_MODELS = new Set([
  OPENROUTER_GEMINI_MODEL_ID,
  "gpt-5",
  "gpt-5-mini",
  "gpt-5-nano",
  "gpt-5-codex"
])
// --- â¬†ï¸ Ù¾Ø§ÛŒØ§Ù† ØªØºÛŒÛŒØ± Û± ---

function isImageRequest(prompt: string): boolean {
  const lowerCasePrompt = prompt.toLowerCase()

  // Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªØµÙˆÛŒØ±
  const imageNouns = [
    "Ø¹Ú©Ø³",
    "ØªØµÙˆÛŒØ±",
    "Ù†Ù‚Ø§Ø´ÛŒ",
    "Ø·Ø±Ø­",
    "Ù¾ÙˆØ³ØªØ±",
    "ÛŒÙ‡ Ø¹Ú©Ø³ Ø§Ø²",
    "ÛŒÙ‡ Ø¹Ú©Ø³",
    "ÛŒÚ© Ø¹Ú©Ø³"
  ]
  const createVerbs = [
    "Ø¨Ø³Ø§Ø²",
    "Ø¨Ú©Ø´",
    "Ø·Ø±Ø§Ø­ÛŒ Ú©Ù†",
    "Ø¯Ø±Ø³Øª Ú©Ù†",
    "Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†",
    "ÛŒÙ‡ Ø¹Ú©Ø³ Ø§Ø²"
  ]

  // Ø¢ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² Ø§Ø³Ù…â€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ± Ø¯Ø± Ù…ØªÙ† Ù‡Ø³ØªØŸ
  const hasImageNoun = imageNouns.some(noun => lowerCasePrompt.includes(noun))

  // Ø¢ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² ÙØ¹Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªÙ† Ø¯Ø± Ù…ØªÙ† Ù‡Ø³ØªØŸ
  const hasCreateVerb = createVerbs.some(verb => lowerCasePrompt.includes(verb))

  // Ø§Ú¯Ø± Ù‡Ø± Ø¯Ùˆ Ø´Ø±Ø· Ø¨Ø±Ù‚Ø±Ø§Ø± Ø¨ÙˆØ¯ØŒ ÛŒØ¹Ù†ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø³Ø§Ø®Øª ØªØµÙˆÛŒØ± Ø§Ø³Øª
  if (hasImageNoun && hasCreateVerb) {
    return true
  }

  // Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø±Ø§ Ù‡Ù… Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
  const englishKeywords = ["generate image", "create a picture of", "draw a"]
  if (englishKeywords.some(keyword => lowerCasePrompt.includes(keyword))) {
    return true
  }

  return false
}

// ØªØ§Ø¨Ø¹ ØªØ´Ø®ÛŒØµ ÙˆØ±ÙˆØ¯ÛŒ Ú¯ÙØªØ§Ø± Ø¨Ù‡ Ù…ØªÙ† (STT)
// Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù…ØŒ ÛŒÚ© ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±
function isSttRequest(messages: any[]): boolean {
  if (!messages || messages.length === 0) {
    return false
  }
  const lastMessage = messages[messages.length - 1]
  // Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ø¯ ÙØ±Ø§Ù†Øªâ€ŒØ§Ù†Ø¯ Ø´Ù…Ø§ØŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ ØµÙˆØªÛŒ Ú©Ø§Ø±Ø¨Ø± Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø±Ø§ Ø¯Ø§Ø±Ù†Ø¯
  return lastMessage.model === "user-audio"
}
function isMcpRequest(prompt: string): boolean {
  // Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Ù†Ø§Ù†Ùˆ Ø±Ø§ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯
  const keywords = ["mcp", "nano", "rhyno nano", "v5 nano"]
  const lowerCasePrompt = prompt.toLowerCase()

  // Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ø§ ÛŒÚ©ÛŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ (Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ : ÛŒØ§ ÙØ§ØµÙ„Ù‡) Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ÛŒØ§ Ø®ÛŒØ±
  // Ø§ÛŒÙ† Ú©Ø§Ø± Ø§Ø² ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
  return keywords.some(
    word =>
      lowerCasePrompt.startsWith(word + ":") ||
      lowerCasePrompt.startsWith(word + " ")
  )
}
function isDocgenRequest(prompt: string): boolean {
  const lowerCasePrompt = prompt.toLowerCase()

  // Ù„ÛŒØ³Øª Ø§Ù†ÙˆØ§Ø¹ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
  const docTypes = [
    "Ø§Ú©Ø³Ù„",
    "excel",
    "pdf",
    "Ù¾ÛŒ Ø¯ÛŒ Ø§Ù",
    "word",
    "ÙˆØ±Ø¯",
    "document",
    "Ø³Ù†Ø¯"
  ]

  // Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø³Ø§Ø®ØªÙ† ÛŒØ§ ØªØ¨Ø¯ÛŒÙ„
  const createKeywords = [
    "Ø¨Ø³Ø§Ø²",
    "Ú©Ù†",
    "ØªÙˆÙ„ÛŒØ¯ Ú©Ù†",
    "Ø¯Ø±Ø³Øª Ú©Ù†",
    "Ø®Ø±ÙˆØ¬ÛŒ",
    "output",
    "format",
    "Ø¯Ø± Ù‚Ø§Ù„Ø¨"
  ]

  // Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø¢ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² Ø§Ù†ÙˆØ§Ø¹ ÙØ§ÛŒÙ„ Ø¯Ø± Ù…ØªÙ† ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ
  const hasDocType = docTypes.some(doc => lowerCasePrompt.includes(doc))

  // Ùˆ Ø¢ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø³Ø§Ø®ØªÙ† Ø¯Ø± Ù…ØªÙ† ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ
  const hasCreateKeyword = createKeywords.some(keyword =>
    lowerCasePrompt.includes(keyword)
  )

  // Ø§Ú¯Ø± Ù‡Ø± Ø¯Ùˆ Ø´Ø±Ø· Ø¨Ø±Ù‚Ø±Ø§Ø± Ø¨ÙˆØ¯ØŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø³Ø§Ø®Øª ÙØ§ÛŒÙ„ Ø§Ø³Øª
  return hasDocType && hasCreateKeyword
}

type ChatCompletionUsage = {
  prompt_tokens: number
  completion_tokens: number
  total_tokens: number
}

type ExtendedChatSettings = ChatSettings & {
  maxTokens?: number
  max_tokens?: number
}

// âœ¨ Ø«Ø§Ø¨Øªâ€ŒÙ‡Ø§ Ùˆ ØªØ§Ø¨Ø¹ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡
const PROFIT_MARGIN = 1.4

function calculateUserCostUSD(
  modelId: string,
  usage: { prompt_tokens: number; completion_tokens: number }
): number {
  // Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„ Ø§Ø² modelsWithRial
  const model = modelsWithRial.find(m => m.id === modelId)
  if (!model) return 0

  const promptCost =
    (usage.prompt_tokens / 1_000_000) * model.inputPricePer1MTokenUSD
  const completionCost =
    (usage.completion_tokens / 1_000_000) * model.outputPricePer1MTokenUSD

  return (promptCost + completionCost) * PROFIT_MARGIN
}

const MODELS_NEED_MAX_COMPLETION = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-nano",
  "gpt-5-mini"
])
const MODELS_WITH_OPENAI_WEB_SEARCH = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-mini",
  "gpt-5-codex"
])
const MODELS_THAT_SHOULD_NOT_STREAM = new Set([
  "gpt-5",
  "gpt-5-mini",
  "gpt-5-codex"
])
const MODELS_WITH_AUTO_SEARCH = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-mini"
])

const MODEL_MAX_TOKENS: Record<string, number> = {
  "gpt-4o": 8192,
  "gpt-4o-mini": 4096,
  "gpt-5": 12000,
  "gpt-5-mini": 12000,
  "gpt-3.5-turbo": 4096,
  "gpt-5-nano": 5000,
  "gpt-3.5-turbo-16k": 16384
  // Ø³Ø§ÛŒØ± Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
}
const MODELS_WITH_PRIORITY_TIER = new Set([
  "gpt-5",
  "gpt-5-mini",
  "gpt-5-nano",
  "gpt-5-codex"
])

function pickMaxTokens(cs: ExtendedChatSettings, modelId: string): number {
  const requestedTokens = cs.maxTokens ?? cs.max_tokens ?? 4096
  const modelLimit = MODEL_MAX_TOKENS[modelId] ?? 4096
  // Ù…Ù‚Ø¯Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ù†Ø¨Ø§ÛŒØ¯ Ø§Ø² Ø³Ù‚Ù Ù…Ø¯Ù„ Ø¨ÛŒØ´ØªØ± Ø´ÙˆØ¯
  return Math.min(requestedTokens, modelLimit)
}
function normalizeQuickInput(input: string): string {
  return (
    input
      .trim()
      .toLowerCase()
      // Ø­Ø°Ù ØªÙ…Ø§Ù… Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ Ø±Ø§ÛŒØ¬ (ÙØ§Ø±Ø³ÛŒ Ùˆ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)
      .replace(/[.,ØŒØŸ?!]/g, "")
  )
  // Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…ÙˆØ§Ø±Ø¯ Ø¨ÛŒØ´ØªØ±ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
  // Ù…Ø«Ù„Ø§Ù‹: .replace(/ÙŠ/g, "ÛŒ").replace(/Ùƒ/g, "Ú©")
}
export async function POST(request: Request) {
  console.log("ğŸ”¥ğŸ”¥ğŸ”¥ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ API Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯! Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´... ğŸ”¥ğŸ”¥ğŸ”¥")
  try {
    const requestBody = await request.json()
    const authHeader = request.headers.get("Authorization")
    if (!authHeader || !authHeader.startsWith("Bearer ")) {
      return new NextResponse("Unauthorized: Missing Bearer token", {
        status: 401
      })
    }
    const token = authHeader.split(" ")[1]
    // const tokenCount = tokens.length;
    let userId: string

    // Û±. Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¯Ø³ØªÛŒ ØªÙˆÚ©Ù† Ø¨Ø§ JWT_SECRET
    try {
      if (!process.env.SUPABASE_JWT_SECRET) {
        throw new Error("SUPABASE_JWT_SECRET is not set on server!")
      }
      // ØªÙˆÚ©Ù† Ø±Ø§ Ø¨Ø§ Â«Ø±Ø§Ø²Â» (Secret) Ú©Ù‡ Ø¯Ø± Vercel Ø³Øª Ú©Ø±Ø¯ÛŒØ¯ØŒ Ø¨Ø§Ø² Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
      const decodedToken = jwt.verify(
        token,
        process.env.SUPABASE_JWT_SECRET
      ) as jwt.JwtPayload

      if (!decodedToken.sub) {
        throw new Error("Invalid token: No 'sub' (user ID) found.")
      }
      userId = decodedToken.sub // 'sub' (Subject) Ù‡Ù…Ø§Ù† User ID Ø§Ø³Øª
      console.log(`âœ… Token MANUALLY verified! User ID: ${userId}`)
    } catch (err: any) {
      // Ø§Ú¯Ø± Â«Ø±Ø§Ø²Â» Ø´Ù…Ø§ Ø¯Ø± Vercel Ø§Ø´ØªØ¨Ø§Ù‡ Ø¨Ø§Ø´Ø¯ØŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯
      console.error("âŒ Manual JWT Verification Failed:", err.message)
      return new NextResponse(
        `Unauthorized: Manual verification failed: ${err.message}`,
        { status: 401 }
      )
    }

    // Û². Ø³Ø§Ø®Øª Ú©Ù„Ø§ÛŒÙ†Øª Ø§Ø¯Ù…ÛŒÙ† (Admin) Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† Ø¢Ø¨Ø¬Ú©Øª Ú©Ø§Ù…Ù„ User
    // (Ø§ÛŒÙ† Ú©Ø§Ø± ØªÙ…Ø§Ù… Ø§Ø±ÙˆØ±Ù‡Ø§ÛŒ TypeScript Ù‚Ø¨Ù„ÛŒ Ø±Ø§ Ø­Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯)
    if (!process.env.SUPABASE_SERVICE_ROLE_KEY) {
      throw new Error("SUPABASE_SERVICE_ROLE_KEY is not set on server!")
    }

    // Ø§Ø² createClient Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø¨Ø§ Ú©Ù„ÛŒØ¯ SERVICE_ROLE Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    const supabaseAdmin = createAdminClient(
      process.env.NEXT_PUBLIC_SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!
    )

    // Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©Ù„Ø§ÛŒÙ†Øª Ø§Ø¯Ù…ÛŒÙ†ØŒ Ø¢Ø¨Ø¬Ú©Øª Ú©Ø§Ù…Ù„ user Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
    const {
      data: { user },
      error: adminError
    } = await supabaseAdmin.auth.admin.getUserById(userId)

    if (adminError || !user) {
      console.error("âŒ Admin client failed to get user:", adminError?.message)
      return new NextResponse(
        `Unauthorized: User not found with admin client: ${adminError?.message}`,
        { status: 401 }
      )
    }

    // âœ… Ø­Ø§Ù„Ø§ Ù…Ø§ Ø¢Ø¨Ø¬Ú©Øª User Ú©Ø§Ù…Ù„ Ø±Ø§ Ø¯Ø§Ø±ÛŒÙ… (Ø¨Ø±Ø§ÛŒ handleTTS Ùˆ...)
    console.log(`âœ… Full user object retrieved for: ${user.email}`)
    const { isUsageReport, modelId, usage } = requestBody
    if (isUsageReport === true && modelId && usage) {
      console.log(`ğŸ“Š [REALTIME-USAGE] Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„: ${modelId}`)

      const { data: wallet, error: walletError } = await supabaseAdmin
        .from("wallets")
        .select("balance")
        .eq("user_id", userId)
        .single()

      if (walletError || !wallet) {
        console.error(
          "âŒ [REALTIME-USAGE] Ø®Ø·Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ú©ÛŒÙ Ù¾ÙˆÙ„:",
          walletError?.message
        )
        return NextResponse.json(
          { message: "Wallet not found for usage report" },
          { status: 400 }
        )
      }

      const userCostUSD = calculateUserCostUSD(modelId, {
        prompt_tokens: usage.input_tokens,
        completion_tokens: usage.output_tokens
      })

      console.log(`ğŸ’° [REALTIME-USAGE] Ù‡Ø²ÛŒÙ†Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡: ${userCostUSD} USD`)

      if (userCostUSD > 0 && wallet.balance >= userCostUSD) {
        await supabaseAdmin.rpc("deduct_credits_and_log_usage", {
          p_user_id: userId,
          p_model_name: modelId,
          p_prompt_tokens: usage.input_tokens,
          p_completion_tokens: usage.output_tokens,
          p_cost: userCostUSD
        })
        console.log(`âœ… [REALTIME-USAGE] Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø³Ø± Ø´Ø¯.`)
      } else if (userCostUSD > 0) {
        console.warn(
          `âš ï¸ [REALTIME-USAGE] Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª. Ù‡Ø²ÛŒÙ†Ù‡: ${userCostUSD}, Ù…ÙˆØ¬ÙˆØ¯ÛŒ: ${wallet.balance}`
        )
      } else {
        console.log("â„¹ï¸ [REALTIME-USAGE] Ù‡Ø²ÛŒÙ†Ù‡ ØµÙØ± Ø¨ÙˆØ¯ØŒ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ú©Ø³Ø± Ù†ÛŒØ³Øª.")
      }

      // â—ï¸â—ï¸â—ï¸ Ù…Ù‡Ù…: Ø¨Ø¹Ø¯ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú¯Ø²Ø§Ø±Ø´ØŒ Ø®Ø§Ø±Ø¬ Ø´ÙˆÛŒØ¯
      return NextResponse.json(
        { success: true, message: "Usage reported." },
        { status: 200 }
      )
    }
    const {
      chatSettings,
      messages,
      enableWebSearch,
      input,
      chat_id,
      is_user_message_saved
    } = requestBody
    // console.log("--- RECEIVED MESSAGES ARRAY ---")
    // console.log(JSON.stringify(messages, null, 2))
    // console.log("-----------------------------")
    let selectedModel = (chatSettings.model || "gpt-4o-mini") as LLMID

    const cookieStore = cookies()
    const supabase = createSSRClient(cookieStore)

    console.log(`âœ… User ${userId} successfully authenticated via Supabase.`)
    // âœ… Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡: Ø§Ú¯Ø± Ú†Øª Ø¢ÛŒØ¯ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªØŒ Ú†Ú© Ú©Ù† Ú©Ù‡ Ø¢ÛŒØ§ Ù…Ø¯Ù„ Ø±ÛŒÙ„â€ŒØªØ§ÛŒÙ… Ø§Ø³Øª ÛŒØ§ Ù†Ù‡
    const modelFromSettings = chatSettings?.model || ""

    // ÙÙ‚Ø· Ø§Ú¯Ø± Ù…Ø¯Ù„ØŒ Ù…ØªÙ†ÛŒ Ø¹Ø§Ø¯ÛŒ Ø¨ÙˆØ¯ØŒ ÙˆØ¬ÙˆØ¯ messages Ø±Ø§ Ú†Ú© Ú©Ù†
    if (
      (!messages || !Array.isArray(messages) || messages.length === 0) &&
      !modelFromSettings.includes("realtime") &&
      !modelFromSettings.includes("tts")
    ) {
      console.error(
        "â›”ï¸ FATAL: 'messages' array is missing for this model type!"
      )
      return NextResponse.json(
        {
          message: "Missing 'messages' array for non-TTS/non-Realtime request."
        },
        { status: 400 }
      )
    }
    // // âœ…âœ…âœ… Ú†Ú© Ø±Ø§ Ø¨Ù‡ Ø§ÛŒÙ†Ø¬Ø§ Ù…Ù†ØªÙ‚Ù„ Ú©Ù†ÛŒØ¯ âœ…âœ…âœ…
    if (
      !modelFromSettings.includes("realtime") && // ğŸ‘ˆ Ø§ÛŒÙ† Ø´Ø±Ø· Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
      !modelFromSettings.includes("tts") && // ğŸ‘ˆ Ø§ÛŒÙ† Ø´Ø±Ø· Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
      (!messages || !Array.isArray(messages) || messages.length === 0)
    ) {
      console.error(
        "â›”ï¸ FATAL: 'messages' array is missing for this model type!"
      )
      return NextResponse.json(
        {
          message: "Missing 'messages' array for non-TTS/non-Realtime request."
        },
        { status: 400 }
      )
    }
    // âœ…âœ…âœ… Ù¾Ø§ÛŒØ§Ù† Ø§Ù†ØªÙ‚Ø§Ù„ âœ…âœ…âœ…

    console.log(`DEBUG: Processing request for chat_id: ${chat_id}`)

    // Ù…ØªØºÛŒØ±Ù‡Ø§ Ø±Ø§ Ø¨ÛŒØ±ÙˆÙ† Ø§Ø² Ø¨Ù„Ø§Ú© ØªØ¹Ø±ÛŒÙ Ú©Ù†ÛŒØ¯
    let lastUserMessage
    let userMessageContent = "" // Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
    let userImagePaths: string[] = [] // Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡

    // âœ…âœ…âœ… Ø§ÛŒÙ† Ø´Ø±Ø· Ø­ÛŒØ§ØªÛŒ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯ âœ…âœ…âœ…
    if (
      !modelFromSettings.includes("realtime") &&
      !modelFromSettings.includes("tts")
    ) {
      // --- Ø´Ø±ÙˆØ¹ Ø¨Ù„Ø§Ú© Ù…Ù†ØªÙ‚Ù„ Ø´Ø¯Ù‡ ---
      // Ø­Ø§Ù„Ø§ Ø§ÛŒÙ† Ø®Ø· Ø§Ù…Ù† Ø§Ø³Øª Ú†ÙˆÙ† Ù…ÛŒâ€ŒØ¯Ø§Ù†ÛŒÙ… messages ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
      lastUserMessage = messages[messages.length - 1]
      userMessageContent = lastUserMessage.content

      // (Ø§Ú¯Ø± Ù¾ÛŒØ§Ù… Ø­Ø§ÙˆÛŒ Ø¹Ú©Ø³ Ø§Ø³ØªØŒ ÙÙ‚Ø· Ù…ØªÙ† Ø±Ø§ Ø¬Ø¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…)
      if (typeof lastUserMessage.content === "string") {
        // Ø­Ø§Ù„Øª Ø³Ø§Ø¯Ù‡: ÙÙ‚Ø· Ù…ØªÙ†
        userMessageContent = lastUserMessage.content
        userImagePaths = []
      } else if (Array.isArray(lastUserMessage.content)) {
        // Ø­Ø§Ù„Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡: Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø§Ø² Ù…ØªÙ† Ùˆ Ø¹Ú©Ø³
        const textPart = lastUserMessage.content.find(
          (p: any) => p.type === "text"
        )
        userMessageContent = textPart ? textPart.text : ""
        userImagePaths = lastUserMessage.content
          .filter((p: any) => p.type === "image_url" && p.image_url?.url)
          .map((p: any) => p.image_url.url)
      }

      if (is_user_message_saved !== true) {
        // Û³. Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø°Ø®ÛŒØ±Ù‡ Ú©Ù†ÛŒØ¯
        if (userMessageContent || userImagePaths.length > 0) {
          try {
            console.log(
              "DEBUG: Saving user message to DB (client did not save)..."
            )
            const userSequenceNumber = messages.length - 1
            const { error: insertUserMsgError } = await supabaseAdmin
              .from("messages")
              .insert({
                chat_id: chat_id,
                user_id: userId,
                role: "user",
                content: userMessageContent,
                model: chatSettings.model,
                image_paths: userImagePaths,
                sequence_number: userSequenceNumber
              })
            if (insertUserMsgError) {
              console.error(
                "âŒ ERROR saving user message:",
                insertUserMsgError.message
              )
            } else {
              console.log("âœ… User message saved to DB.")
            }
          } catch (e: any) {
            console.error("âŒ EXCEPTION saving user message:", e.message)
          }
        }
      } else {
        console.log("DEBUG: Skipping user message save (client already saved).")
      }

      if (userMessageContent) {
        const normalizedInput = normalizeQuickInput(userMessageContent)
        const response = quickResponses[normalizedInput]
        if (response) {
          console.log(
            `âš¡ï¸ [LEVEL 0] Sending instant reply for: "${normalizedInput}"`
          )

          const encoder = new TextEncoder()
          const stream = new ReadableStream({
            start(controller) {
              controller.enqueue(encoder.encode(response))
              controller.close()
            }
          })

          return new Response(stream, {
            /* ... headers ... */
          })
        }
      }
    } // --- âœ…âœ…âœ… Ù¾Ø§ÛŒØ§Ù† Ø¨Ù„ÙˆÚ© Paste Ø´Ø¯Ù‡ ---

    const { data: wallet, error: walletError } = await supabaseAdmin
      .from("wallets")
      .select("balance")
      .eq("user_id", userId) // âœ… Ø§Ø² userId Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
      .single()
    if (walletError && walletError.code === "PGRST116") {
      return NextResponse.json(
        { message: "Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø´Ù…Ø§ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª." },
        { status: 402 }
      )
    } else if (walletError) {
      throw walletError
    }

    if (!wallet || wallet.balance <= 0) {
      return NextResponse.json(
        { message: "Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø´Ù…Ø§ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª." },
        { status: 402 }
      )
    }
    // âœ¨ Ù¾Ø§ÛŒØ§Ù† Ø¨Ø®Ø´ Ù¾Ø±Ø¯Ø§Ø®Øª Ùˆ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª

    const profile = await getServerProfile(userId, supabaseAdmin)
    checkApiKey(profile.openai_api_key, "OpenAI")
    const openai = new OpenAI({
      apiKey: profile.openai_api_key || "",
      organization: profile.openai_organization_id
    })

    if (OPENROUTER_MODELS.has(selectedModel)) {
      console.log(
        `ğŸ”„ [ROUTER] Redirecting request for model ${selectedModel} to /api/chat/openrouter...`
      )
      const openrouterUrl = new URL("/api/chat/openrouter", request.url)
      const openrouterResponse = await fetch(openrouterUrl, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          // ØªÙˆÚ©Ù† Ù…ÙˆØ¨Ø§ÛŒÙ„ Ùˆ Ú©ÙˆÚ©ÛŒ ÙˆØ¨â€ŒØ³Ø§ÛŒØª Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª Ø¨Ù‡ Ù…Ø³ÛŒØ± Ø¨Ø¹Ø¯ÛŒ Ù¾Ø§Ø³ Ø¨Ø¯Ù‡
          Authorization: request.headers.get("Authorization") || "",
          Cookie: request.headers.get("Cookie") || ""
        },
        body: JSON.stringify(requestBody)
      })

      // Ù¾Ø§Ø³Ø® (Ø§Ø³ØªØ±ÛŒÙ… ÛŒØ§ ØºÛŒØ± Ø§Ø³ØªØ±ÛŒÙ…) Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø± Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†
      return new Response(openrouterResponse.body, {
        status: openrouterResponse.status,
        headers: openrouterResponse.headers
      })
    }
    // --- â¬†ï¸ Ù¾Ø§ÛŒØ§Ù† ØªØºÛŒÛŒØ± Û² ---

    if (selectedModel === "gpt-4o-mini-tts") {
      // console.log("ğŸ”Š Ø¯Ø±Ø®ÙˆØ§Ø³Øª TTS Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯.")

      const ttsInput =
        input ||
        (messages && messages.length > 0
          ? messages[messages.length - 1]?.content
          : "") ||
        ""

      if (!ttsInput) {
        return NextResponse.json(
          { message: "Input text is required for TTS." },
          { status: 400 }
        )
      }
      const ttsBody = {
        input: ttsInput,
        voice: chatSettings.voice || "coral",
        speed: chatSettings.speed || 1.0,
        model: selectedModel,
        chat_id: chat_id
      }

      return await handleTTS({
        request, // ğŸ‘ˆ Ø§ÛŒÙ†Ùˆ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
        body: ttsBody,
        user,
        supabase: supabaseAdmin
      })
    }

    if (selectedModel.includes("realtime")) {
      const response = await fetch(
        "https://api.openai.com/v1/realtime/sessions",
        {
          method: "POST",
          headers: {
            Authorization: `Bearer ${profile.openai_api_key}`,
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            model: selectedModel,
            voice: "alloy",
            // ğŸŸ¢ ØªØºÛŒÛŒØ±Ø§Øª Ù…Ù‡Ù… Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯:
            instructions: `
  You are Rhyno, a realtime Persian-speaking assistant.
  
  CORE RULES:
  âœ… Always respond in Persian (Farsi).
  âœ… Only speak in voice (no text output).
  âœ… Introduce yourself as Rhyno when asked.
  âœ… Keep your answers short and concise.
  
  SPECIAL INSTRUCTION FOR "AZAR YORD":
  If the user asks about "Azar Yord" (Ø¢Ø°Ø±ÛŒÙˆØ±Ø¯) or "Azar Yord Tabriz", you MUST use the "web_search" tool.
  Context: Azar Yord is a specific entity found at "https://azaryourdtabriz.com/".
  Action: When searching, construct your query to specifically target this site (e.g., "site:azaryourdtabriz.com about" or "site:azaryourdtabriz.com services").
  Goal: Read the information from the search results related to that website and explain it to the user in Persian. Do not just give the link; explain what they do based on the search results.
  `,
            tools: [
              {
                type: "function",
                name: "web_search",
                description:
                  "Search the web for up-to-date information. Use this when asked about specific entities or current events.",
                parameters: {
                  type: "object",
                  properties: {
                    query: {
                      type: "string",
                      description:
                        "The search query. For specific sites, use 'site:domain.com keyword'."
                    }
                  },
                  required: ["query"]
                }
              }
            ]
          })
        }
      )
      if (!response.ok) {
        const errorBody = await response.json()
        console.error("âŒ OpenAI Realtime Error Body:", errorBody) // âœ¨ Ø§ÛŒÙ† Ø®Ø· Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
        throw new Error(
          errorBody.error?.message || "Failed to create realtime session"
        )
      }
      const session = await response.json()
      // console.log("ğŸŒ Realtime session raw response:", session)
      // console.log("ğŸ”Š Session modalities:", session.modalities)
      // console.log("ğŸ”Š Session voice:", session.voice)
      // console.log("ğŸ”Š Session instructions:", session.instructions)
      console.log(
        "ğŸŒ Realtime session raw response from OpenAI:",
        JSON.stringify(session, null, 2)
      ) // âœ¨ Ø§ÛŒÙ† Ø®Ø· Ø±Ø§ ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯!
      const { error: insertError } = await supabase
        .from("realtime_sessions")
        .insert({
          user_id: userId,
          openai_session_id: session.id // ÛŒØ§ Ù‡Ø± ÙÛŒÙ„Ø¯ÛŒ Ú©Ù‡ ID Ø¬Ù„Ø³Ù‡ Ø¯Ø± Ø¢Ù† Ø§Ø³Øª
        })

      if (insertError) {
        console.error("Failed to save realtime session to DB:", insertError)
        // Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§ÛŒÙ†Ø¬Ø§ Ø®Ø·Ø§ Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†ÛŒØ¯
      }
      return NextResponse.json(session)
    }

    function extractTextFromContent(content: any): string {
      if (!content && content !== 0) return ""
      if (typeof content === "string") return content
      // Ø§Ú¯Ø± content Ø¢Ø±Ø§ÛŒÙ‡ Ø§Ø² Ù¾Ø§Ø±Øªâ€ŒÙ‡Ø§Ø³Øª (Ù…Ø«Ù„ [{type:"input_text", text: "..."}])
      if (Array.isArray(content)) {
        return content
          .map(part => {
            if (typeof part === "string") return part
            if (part == null) return ""
            if (typeof part === "object") {
              // Ø§Ù†ÙˆØ§Ø¹ Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ú©Ù‡ Ù…Ù…Ú©Ù†Ù‡ Ø¯Ø§Ø®Ù„ Ø¨Ø§Ø´Ù†
              return (
                part.text ??
                part.content ??
                part.name ??
                JSON.stringify(part)
              ).toString()
            }
            return String(part)
          })
          .filter(Boolean)
          .join(" ")
      }
      // Ø§Ú¯Ø± Ø¢Ø¨Ø¬Ú©Øª Ø³Ø§Ø¯Ù‡â€ŒØ³Øª
      if (typeof content === "object") {
        return (
          content.text ??
          content.content ??
          JSON.stringify(content)
        ).toString()
      }
      return String(content)
    }

    // Ø³Ù¾Ø³

    if (selectedModel === "gpt-4o-transcribe") {
      // console.log("ğŸ™ï¸ Ø¯Ø±Ø®ÙˆØ§Ø³Øª STT Ø¨Ù‡ Ù…Ø³ÛŒØ± Ø§Ø´ØªØ¨Ø§Ù‡ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯Ù‡ Ø§Ø³Øª.")
      // Ø§ÛŒÙ† Ø´Ø±Ø· Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø³Ø±Ø¯Ø±Ú¯Ù…ÛŒ Ø§Ø³Øª.
      // Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ STT Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø¨Ù‡ /api/transcribe Ø§Ø±Ø³Ø§Ù„ Ø´ÙˆÙ†Ø¯.
      return NextResponse.json(
        {
          message:
            "Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú¯ÙØªØ§Ø± Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ù…Ø³ÛŒØ± /api/transcribe Ø§Ø±Ø³Ø§Ù„ Ø´ÙˆÙ†Ø¯."
        },
        { status: 400 } // Bad Request
      )
    }

    // if (isDocgenRequest(lastUserMessage)) {
    //   // console.log("ğŸ“„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø³Ø§Ø®Øª ÙØ§ÛŒÙ„ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. Ù‡Ø¯Ø§ÛŒØª Ø¨Ù‡ Ù…Ø³ÛŒØ± DocGen...")

    //   // ØªÙˆØ¬Ù‡: ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø´Ù…Ø§ ÛŒÚ© Ù…Ø³ÛŒØ± API Ø¬Ø¯ÛŒØ¯ Ø¯Ø± /api/chat/docgen Ø³Ø§Ø®ØªÙ‡â€ŒØ§ÛŒØ¯
    //   const docgenUrl = new URL("/api/chat/mcp", request.url)

    //   const docgenResponse = await fetch(docgenUrl, {
    //     method: "POST",
    //     headers: {
    //       "Content-Type": "application/json",
    //       Cookie: request.headers.get("Cookie") || ""
    //     },
    //     body: JSON.stringify({ chatSettings, messages, enableWebSearch })
    //   })

    //   // Ù¾Ø§Ø³Ø® Ø§Ø² Ø§ÛŒÙ† Ù…Ø³ÛŒØ± Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ ÛŒÚ© Ù„ÛŒÙ†Ú© Ø¯Ø§Ù†Ù„ÙˆØ¯ ÛŒØ§ Ø®ÙˆØ¯ ÙØ§ÛŒÙ„ Ø¨Ø§Ø´Ø¯
    //   return new Response(docgenResponse.body, {
    //     status: docgenResponse.status,
    //     headers: docgenResponse.headers
    //   })
    // }

    // if (selectedModel === "gpt-5-nano") {
    //   console.log("ğŸš€ Ø¯Ø±Ø®ÙˆØ§Ø³Øª gpt-5-nano Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. Ù‡Ø¯Ø§ÛŒØª Ø¨Ù‡ /api/chat/mcp...")

    //   // Ø³Ø§Ø®Øª URL Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ù…Ø³ÛŒØ± Ø¬Ø¯ÛŒØ¯
    //   const mcpUrl = new URL("/api/chat/mcp", request.url)

    //   // Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ Ù…Ø³ÛŒØ± Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ Ù‡Ù…Ø§Ù† Ø¨Ø¯Ù†Ù‡ Ùˆ Ù‡Ø¯Ø±Ù‡Ø§
    //   const mcpResponse = await fetch(mcpUrl, {
    //     method: "POST",
    //     headers: {
    //       "Content-Type": "application/json",
    //       // Ø§Ø±Ø³Ø§Ù„ Ú©ÙˆÚ©ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª Ø¯Ø± Ù…Ø³ÛŒØ± Ø¬Ø¯ÛŒØ¯
    //       Cookie: request.headers.get("Cookie") || ""
    //     },
    //     // Ø§Ø±Ø³Ø§Ù„ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ú©Ù‡ Ø§Ø² Ø¨Ø¯Ù†Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø¨ÙˆØ¯ÛŒÙ…
    //     body: JSON.stringify({ chatSettings, messages, enableWebSearch })
    //   })

    //   // Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù…Ø³ØªÙ‚ÛŒÙ… Ù¾Ø§Ø³Ø® (Ø§Ø³ØªØ±ÛŒÙ… ÛŒØ§ ØºÛŒØ± Ø§Ø³ØªØ±ÛŒÙ…) Ø§Ø² Ù…Ø³ÛŒØ± MCP Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±
    //   return new Response(mcpResponse.body, {
    //     status: mcpResponse.status,
    //     headers: mcpResponse.headers
    //   })
    // }
    // Ø§Ú¯Ø± Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ú¯ÙØªØ§Ø± Ø§Ø³ØªØŒ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ú©Ù†ØªØ±Ù„â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ù…Ø±Ø¨ÙˆØ·Ù‡ Ø¨ÙØ±Ø³Øª

    // âœ¨ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ§Ù… Ø³ÛŒØ³ØªÙ…
    const CHAT_HISTORY_LIMIT = 20

    // Ù…Ø·Ù…Ø¦Ù† Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ… Ú©Ù‡ 'messages' ÛŒÚ© Ø¢Ø±Ø§ÛŒÙ‡ Ø§Ø³Øª
    const validMessages = Array.isArray(messages) ? messages : []

    // Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø² Ø§Ù†ØªÙ‡Ø§ Ø¨Ø±Ø´ Ù…ÛŒâ€ŒØ²Ù†ÛŒÙ… ØªØ§ ÙÙ‚Ø· N ØªØ§ÛŒ Ø¢Ø®Ø± Ø¨Ø§Ù‚ÛŒ Ø¨Ù…Ø§Ù†Ù†Ø¯
    const recentMessages = validMessages.slice(-CHAT_HISTORY_LIMIT)

    // âœ¨ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ§Ù… Ø³ÛŒØ³ØªÙ…
    const finalMessages = [
      {
        role: "system",
        content:
          MODEL_PROMPTS[selectedModel] || "You are a helpful AI assistant."
      },
      ...recentMessages // âœ… Ø¨Ù‡ Ø¬Ø§ÛŒ Ú©Ù„ 'messages'ØŒ Ø§Ø² 'recentMessages' Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    ]

    if (selectedModel === "dall-e-3") {
      return NextResponse.json(
        {
          message:
            "DALL-E 3 requests should be sent to /api/chat/dalle/route.ts"
        },
        { status: 400 }
      )
    }

    const cs = chatSettings as ExtendedChatSettings
    const maxTokens = pickMaxTokens(cs, selectedModel)
    const temp = typeof cs.temperature === "number" ? cs.temperature : 1
    const hasImage = messages.some(
      (message: any) =>
        Array.isArray(message.content) &&
        message.content.some((part: any) => part.type === "image_url")
    )
    const useStream = !MODELS_THAT_SHOULD_NOT_STREAM.has(selectedModel)
    const enableSearch =
      typeof enableWebSearch === "boolean"
        ? enableWebSearch
        : MODELS_WITH_AUTO_SEARCH.has(selectedModel)
    const useOpenAIWebSearch =
      !!enableSearch &&
      MODELS_WITH_OPENAI_WEB_SEARCH.has(selectedModel) &&
      !hasImage // âœ¨

    // âœ¨ Ù…Ù†Ø·Ù‚ Web Search
    if (useOpenAIWebSearch) {
      // Ø¨Ø®Ø´ Û±: Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØºÛŒØ± Ø§Ø³ØªØ±ÛŒÙ… ÙˆØ¨â€ŒØ³Ø±Ú† (Ú©Ø¯ Ø§ØµÙ„ÛŒ Ø´Ù…Ø§)
      if (["gpt-5", "gpt-5-mini", "gpt-5-codex"].includes(selectedModel)) {
        // âœ… Ø§ØµÙ„Ø§Ø­ Ø´Ø¯
        // console.log(
        //   "ğŸš€ [WEB-SEARCH] Entering NON-streaming web search block for model:",
        //   selectedModel
        // )
        const webSearchPayload: any = {
          model: selectedModel,
          input: finalMessages.map(m =>
            m.role === "user"
              ? {
                  role: "user",
                  content: [{ type: "input_text", text: m.content as string }]
                }
              : m
          ) as any,
          tools: [{ type: "web_search" as any }],
          temperature: temp,
          max_output_tokens: maxTokens
        }

        if (MODELS_WITH_PRIORITY_TIER.has(selectedModel)) {
          webSearchPayload.service_tier = "default"
        }
        console.log(
          "ğŸš€ [PRIORITY-CHECK] Web Search Payload:",
          JSON.stringify(webSearchPayload, null, 2)
        )
        const response = await openai.responses.create(webSearchPayload)

        // âœ¨ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù†Ø·Ù‚ Ú©Ø³Ø± Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª ØºÛŒØ± Ø§Ø³ØªØ±ÛŒÙ… ÙˆØ¨â€ŒØ³Ø±Ú†
        const usage = response.usage
        if (usage) {
          const userCostUSD = calculateUserCostUSD(selectedModel, {
            prompt_tokens: usage.input_tokens, // <-- ØªØºÛŒÛŒØ± Ø§Ø² prompt_tokens
            completion_tokens: usage.output_tokens // <-- ØªØºÛŒÛŒØ± Ø§Ø² completion_tokens
          })
          // console.log(`ğŸ“Š [WEB-SEARCH] Usage data received:`, usage)
          if (userCostUSD > 0 && wallet) {
            // console.log(
            //   `ğŸ’° Ù‡Ø²ÛŒÙ†Ù‡: ${userCostUSD} | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø§ÙˆÙ„ÛŒÙ‡: ${wallet.balance}`
            // )
            await supabaseAdmin.rpc("deduct_credits_and_log_usage", {
              p_user_id: userId,
              p_model_name: selectedModel,
              p_prompt_tokens: usage.input_tokens, // <-- ØªØºÛŒÛŒØ± Ø¨Ù‡ input_tokens
              p_completion_tokens: usage.output_tokens, // <-- ØªØºÛŒÛŒØ± Ø¨Ù‡ output_tokens
              p_cost: userCostUSD
            })
            const { data: updatedWallet } = await supabase
              .from("wallets")
              .select("balance")
              .eq("user_id", userId)
              .single()
            // console.log(
            //   `âœ… Ø¹Ù…Ù„ÛŒØ§Øª Ù…ÙˆÙÙ‚! | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯: ${updatedWallet?.balance}`
            // )
          }
        }

        return new Response(response.output_text ?? "", {
          headers: { "Content-Type": "text/plain; charset=utf-8" }
        })
      }

      // Ø¨Ø®Ø´ Û²: Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ±ÛŒÙ… ÙˆØ¨â€ŒØ³Ø±Ú† (Ù…Ø«Ù„ gpt-4o-mini)
      // Ø§ÛŒÙ† Ø¨Ø®Ø´ ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±ØªÛŒ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø´Ø±Ø· Ø¨Ø§Ù„Ø§ Ø¨Ø±Ù‚Ø±Ø§Ø± Ù†Ø¨Ø§Ø´Ø¯
      const encoder = new TextEncoder()
      const stream = new ReadableStream({
        async start(controller) {
          // console.log(
          //   "ğŸš€ [WEB-SEARCH] Entering STREAMING web search block for model:",
          //   selectedModel
          // )
          let usage: ChatCompletionUsage | undefined

          try {
            const transformedInput = finalMessages.map(m => {
              if (m.role === "user")
                return {
                  ...m,
                  content: [{ type: "input_text", text: m.content as string }]
                }
              if (m.role === "assistant" && typeof m.content === "string")
                return {
                  ...m,
                  content: [{ type: "output_text", text: m.content }]
                }
              return m
            })

            const oaiStream = await openai.responses.stream({
              model: selectedModel,
              input: transformedInput as any,
              tools: [{ type: "web_search" as any }],
              temperature: temp,
              max_output_tokens: maxTokens
            })

            for await (const event of oaiStream as AsyncIterable<any>) {
              // console.log("EVENT FROM OPENAI:", JSON.stringify(event, null, 2));
              if (event.type === "response.output_text.delta") {
                controller.enqueue(encoder.encode(String(event.delta || "")))
              } else if (
                event.type === "response.completed" &&
                event.response?.usage
              ) {
                const receivedUsage = event.response.usage
                usage = {
                  prompt_tokens: receivedUsage.input_tokens,
                  completion_tokens: receivedUsage.output_tokens,
                  total_tokens: receivedUsage.total_tokens
                }
                // Ø§ÛŒÙ† Ù„Ø§Ú¯ Ø­Ø§Ù„Ø§ Ø¨Ø§ÛŒØ¯ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯
                // console.log("ğŸ“Š [WEB-SEARCH] Usage data received:", usage)
              }
            }

            if (usage) {
              const userCostUSD = calculateUserCostUSD(selectedModel, usage)
              if (userCostUSD > 0 && wallet) {
                // console.log(
                //   `ğŸ’° Ù‡Ø²ÛŒÙ†Ù‡: ${userCostUSD} | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø§ÙˆÙ„ÛŒÙ‡: ${wallet.balance}`
                // )
                await supabaseAdmin.rpc("deduct_credits_and_log_usage", {
                  p_user_id: userId,
                  p_model_name: selectedModel,
                  p_prompt_tokens: usage.prompt_tokens,
                  p_completion_tokens: usage.completion_tokens,
                  p_cost: userCostUSD
                })
                const { data: updatedWallet } = await supabase
                  .from("wallets")
                  .select("balance")
                  .eq("user_id", userId)
                  .single()
                // console.log(
                //   `âœ… Ø¹Ù…Ù„ÛŒØ§Øª Ù…ÙˆÙÙ‚! | Ú©Ø§Ø±Ø¨Ø±: ${userId} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯: ${updatedWallet?.balance}`
                // )
              }
            }
          } catch (err: any) {
            console.error("âŒ [WEB-SEARCH] Error in stream:", err)
            controller.enqueue(
              encoder.encode(
                `âŒ Ø®Ø·Ø§ Ø¯Ø± ÙˆØ¨â€ŒØ³Ø±Ú†: ${err?.message || "Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡"}`
              )
            )
          } finally {
            // console.log("ğŸšª [WEB-SEARCH] Closing stream controller.")
            controller.close()
          }
        }
      })
      return new Response(stream, {
        headers: {
          "Content-Type": "text/plain; charset=utf-8",
          "Cache-Control": "no-cache, no-transform",
          "X-Accel-Buffering": "no"
        }
      })
    }
    const userPrompt = extractTextFromContent(
      finalMessages[finalMessages.length - 1]?.content
    )
    if (useStream) {
      const payload: ChatCompletionCreateParamsStreaming = {
        model: selectedModel,
        messages: finalMessages,
        stream: true,
        temperature: temp,
        user: userId
        // ... (max_tokens, service_tier Ù…Ø«Ù„ Ù‚Ø¨Ù„)
      }
      if (MODELS_NEED_MAX_COMPLETION.has(selectedModel)) {
        ;(payload as any).max_completion_tokens = maxTokens
      } else {
        payload.max_tokens = maxTokens
      }
      if (MODELS_WITH_PRIORITY_TIER.has(selectedModel)) {
        ;(payload as any).service_tier = "default" // ÛŒØ§ "default" Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÛŒØ§Ø²
      }

      const stream = await openai.chat.completions.create(payload)
      const encoder = new TextEncoder()
      let usage: ChatCompletionUsage | undefined // Ù…ØªØºÛŒØ± usage Ø¨ÛŒØ±ÙˆÙ† Ø­Ù„Ù‚Ù‡ ØªØ¹Ø±ÛŒÙ Ø´ÙˆØ¯
      let fullAssistantResponse = ""
      let calculated_prompt_tokens = 0
      try {
        // Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ù…Ø­ØªÙˆØ§ÛŒ ØªÙ…Ø§Ù… Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø´Ù…Ø§Ø±ÛŒÙ…
        for (const message of finalMessages) {
          // Ø§Ø² ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ú©Ù‡ Ø®ÙˆØ¯ØªØ§Ù† Ù†ÙˆØ´ØªÙ‡ Ø¨ÙˆØ¯ÛŒØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
          const content = extractTextFromContent(message.content)
          calculated_prompt_tokens += encode(content).length
        }
        console.log(
          `ğŸ“Š [TIKTOKEN] Calculated Prompt Tokens: ${calculated_prompt_tokens}`
        )
      } catch (e: any) {
        console.error(
          "âŒ [TIKTOKEN] Error calculating prompt tokens:",
          e.message
        )
        // Ø§Ú¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯ØŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØµÙØ± Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… ØªØ§ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù…ØªÙˆÙ‚Ù Ù†Ø´ÙˆØ¯
      }
      const readableStream = new ReadableStream({
        async start(controller) {
          console.log(`ğŸš€ [STREAM-DEBUG] Stream started for user: ${userId}`)

          try {
            // --- 1. Ø­Ù„Ù‚Ù‡ Stream (Ú©Ø¯ Ø§ØµÙ„ÛŒ Ùˆ ØµØ­ÛŒØ­ Ø´Ù…Ø§) ---
            for await (const chunk of stream) {
              // Ø§ÛŒÙ† Ù„Ø§Ú¯ Ø±Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø­Ø°Ù Ú©Ù†ÛŒØ¯ØŒ Ú†ÙˆÙ† Ù…ÛŒâ€ŒØ¯Ø§Ù†ÛŒÙ… 'usage' Ø§ÛŒÙ†Ø¬Ø§ Ù†ÛŒØ³Øª
              // if (chunk.usage) {
              //   usage = chunk.usage
              //   console.log("ğŸ“Š [STREAM-DEBUG] Potential Usage data:", usage)
              // }

              const delta = chunk.choices[0]?.delta?.content || ""
              if (delta) {
                // Ø§Ø±Ø³Ø§Ù„ ØªÚ©Ù‡ Ù…ØªÙ† Ø¨Ù‡ Ú©Ù„Ø§ÛŒÙ†Øª
                fullAssistantResponse += delta
                console.log(`â¡ï¸ [STREAM-SENDING] Delta: "${delta}"`)
                controller.enqueue(encoder.encode(delta))
              }
            }
            // --- ğŸ‘† Ù¾Ø§ÛŒØ§Ù† Ø­Ù„Ù‚Ù‡ Stream ---

            console.log("ğŸ [STREAM-DEBUG] Stream loop finished.")
          } catch (err: any) {
            console.error("âŒ ERROR DURING STREAM PROCESSING:", err)
            controller.enqueue(
              encoder.encode(
                `\nâŒ Ø®Ø·Ø§ÛŒ Ø³Ø±ÙˆØ±: ${err.message || "Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡"}`
              )
            )
          } finally {
            console.log("ğŸšª [STREAM-DEBUG] Closing stream controller.")
            // Û±. Ø§Ø³ØªØ±ÛŒÙ… Ø±Ø§ Ø¨Ù‡ Ú©Ù„Ø§ÛŒÙ†Øª Ù…ÛŒâ€ŒØ¨Ù†Ø¯ÛŒÙ…
            controller.close()

            // Û². ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ (Completion) Ø±Ø§ Ù…ÛŒâ€ŒØ´Ù…Ø§Ø±ÛŒÙ…
            let calculated_completion_tokens = 0
            try {
              if (fullAssistantResponse.trim().length > 0) {
                calculated_completion_tokens = encode(
                  // âœ…âœ…âœ… Ø§ØµÙ„Ø§Ø­ Ø´Ø¯
                  fullAssistantResponse.trim()
                ).length
                console.log(
                  `ğŸ“Š [TIKTOKEN] Calculated Completion Tokens: ${calculated_completion_tokens}`
                )
              }
            } catch (e: any) {
              console.error(
                "âŒ [TIKTOKEN] Error calculating completion tokens:",
                e.message
              )
            }

            // Û³. Ø¢Ø¨Ø¬Ú©Øª 'usage' Ø±Ø§ Ø®ÙˆØ¯Ù…Ø§Ù† Ù…ÛŒâ€ŒØ³Ø§Ø²ÛŒÙ…
            // (Ø§Ø² 'calculated_prompt_tokens' Ú©Ù‡ Ø¨ÛŒØ±ÙˆÙ† Ø§Ø³ØªØ±ÛŒÙ… Ø­Ø³Ø§Ø¨ Ú©Ø±Ø¯ÛŒÙ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…)
            const usage = {
              prompt_tokens: calculated_prompt_tokens,
              completion_tokens: calculated_completion_tokens,
              total_tokens:
                calculated_prompt_tokens + calculated_completion_tokens
            }

            // Û´. Ú©Ø³Ø± Ù‡Ø²ÛŒÙ†Ù‡ (Ø­Ø§Ù„Ø§ Ù‡Ù…ÛŒØ´Ù‡ 'usage' Ø±Ø§ Ø¯Ø§Ø±ÛŒÙ…)
            if (usage.prompt_tokens > 0 || usage.completion_tokens > 0) {
              console.log(
                "âœ… [TIKTOKEN-FINAL] Usage data available. Proceeding with deduction."
              )
              const userCostUSD = calculateUserCostUSD(selectedModel, usage)
              if (userCostUSD > 0 && wallet) {
                // ... (Ú©Ø¯ Ú©Ø³Ø± Ù‡Ø²ÛŒÙ†Ù‡ Ø´Ù…Ø§ Ø¨Ø§ supabaseAdmin.rpc) ...
                await supabaseAdmin.rpc("deduct_credits_and_log_usage", {
                  p_user_id: userId,
                  p_model_name: selectedModel,
                  p_prompt_tokens: usage.prompt_tokens,
                  p_completion_tokens: usage.completion_tokens,
                  p_cost: userCostUSD
                })
                console.log(
                  `âœ… Cost deducted: ${userCostUSD} for user ${userId}`
                )
              }
            } else {
              console.warn("âš ï¸ Usage was zero. Skipping deduction.")
            }

            // Ûµ. Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ§Ù… Ø¯Ø³ØªÛŒØ§Ø± (Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ Ø´Ù…Ø§)
            if (is_user_message_saved !== true) {
              // ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ¨Ø§ÛŒÙ„
              if (fullAssistantResponse.trim().length > 0) {
                try {
                  console.log(
                    "DEBUG: Saving assistant message to DB (Mobile client)..."
                  )
                  const { error: insertAsstMsgError } = await supabaseAdmin
                    .from("messages")
                    .insert({
                      chat_id: chat_id,
                      user_id: userId,
                      role: "assistant",
                      content: fullAssistantResponse.trim(),
                      model: selectedModel,
                      prompt_tokens: usage?.prompt_tokens || 0,
                      completion_tokens: usage?.completion_tokens || 0,
                      image_paths: [],
                      sequence_number: messages.length
                    })
                  if (insertAsstMsgError) {
                    console.error(
                      "âŒ ERROR saving assistant message:",
                      insertAsstMsgError.message
                    )
                  } else {
                    console.log(
                      "âœ… Assistant message saved to DB (Mobile client)."
                    )
                  }
                } catch (e: any) {
                  console.error(
                    "âŒ EXCEPTION saving assistant message:",
                    e.message
                  )
                }
              }
            } else {
              console.log(
                "DEBUG: Skipping assistant message save (Web client will save)."
              )
            }
          }
        }
      })

      // âœ…âœ…âœ… Ø±Ø§Ù‡ Ø­Ù„ Ù†Ù‡Ø§ÛŒÛŒ: ÙÙ‚Ø· Ø²Ù…Ø§Ù†ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ú©Ù† Ú©Ù‡ Ú©Ù„Ø§ÛŒÙ†Øª Ø®ÙˆØ¯Ø´ Ø°Ø®ÛŒØ±Ù‡ Ù†Ú©Ø±Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
      //   if (is_user_message_saved !== true) {
      //     if (fullAssistantResponse.trim().length > 0) {
      //       try {
      //         console.log(
      //           "DEBUG: Saving assistant message to DB (Mobile client)..."
      //         ) // Ù„Ø§Ú¯ Ø±Ø§ Ø¢Ù¾Ø¯ÛŒØª Ú©Ø±Ø¯Ù…
      //         const { error: insertAsstMsgError } = await supabaseAdmin
      //           .from("messages")
      //           .insert({
      //             chat_id: chat_id,
      //             user_id: userId,
      //             role: "assistant",
      //             content: fullAssistantResponse.trim(),
      //             model: selectedModel,
      //             prompt_tokens: usage?.prompt_tokens || 0,
      //             completion_tokens: usage?.completion_tokens || 0,
      //             image_paths: [],
      //             sequence_number: messages.length
      //           })
      //         if (insertAsstMsgError) {
      //           console.error(
      //             "âŒ ERROR saving assistant message:",
      //             insertAsstMsgError.message
      //           )
      //         } else {
      //           console.log(
      //             "âœ… Assistant message saved to DB (Mobile client)."
      //           )
      //         }
      //       } catch (e: any) {
      //         console.error(
      //           "âŒ EXCEPTION saving assistant message:",
      //           e.message
      //         )
      //       }
      //     } else {
      //       console.warn(
      //         "âš ï¸ Assistant response was empty, not saving to DB."
      //       )
      //     }
      //   } else {
      //     console.log(
      //       "DEBUG: Skipping assistant message save (Web client will save)."
      //     )
      //   }

      // Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Stream Ø¨Ù‡ Ú©Ù„Ø§ÛŒÙ†Øª
      // Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Stream Ø¨Ù‡ Ú©Ù„Ø§ÛŒÙ†Øª
      return new Response(readableStream, {
        headers: {
          "Content-Type": "text/plain; charset=utf-8",
          "Cache-Control": "no-cache, no-transform",
          "X-Accel-Buffering": "no"
        }
      })
    } else {
      const isNewOpenAIModel = [
        "gpt-5",
        "gpt-5-mini",
        "gpt-5-nano",
        "gpt-5-codex"
      ].includes(selectedModel)
      const userInputText = finalMessages
        .map(m =>
          typeof m.content === "string"
            ? m.content
            : extractTextFromContent(m.content)
        )
        .join("\n")
      if (isNewOpenAIModel) {
        // âœ… Ù…Ø¯Ù„ Ø¬Ø¯ÛŒØ¯: Ø§Ø² v1/responses Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        const response = await openai.responses.create({
          model: selectedModel,
          input: userInputText,
          temperature: temp,
          max_output_tokens: maxTokens,
          ...(MODELS_WITH_PRIORITY_TIER.has(selectedModel)
            ? { service_tier: "default" }
            : {})
        })
        console.log(
          "ğŸš€ [PRIORITY-CHECK] Non-Stream Response Payload (v1/responses):",
          JSON.stringify(response, null, 2)
        )
        const content = response.output_text ?? ""
        return new Response(content, {
          headers: { "Content-Type": "text/plain; charset=utf-8" }
        })
      } else {
        // âœ… Ù…Ø¯Ù„ Ù‚Ø¯ÛŒÙ…ÛŒ: Ø§Ø² chat.completions.create Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        try {
          const payload: ChatCompletionCreateParams = {
            model: selectedModel,
            messages: finalMessages,
            stream: false,
            temperature: temp,
            user: userId
          }
          if (MODELS_NEED_MAX_COMPLETION.has(selectedModel)) {
            ;(payload as any).max_completion_tokens = maxTokens
          } else {
            payload.max_tokens = maxTokens
          }
          if (MODELS_WITH_PRIORITY_TIER.has(selectedModel)) {
            ;(payload as any).service_tier = "default"
          }
          const response = await openai.chat.completions.create(payload)
          const content = response.choices[0].message.content ?? ""
          const usage = response.usage
          console.log("ğŸ’¡ Checking wallet and usage...")
          if (usage) {
            console.log("ğŸ’¡ Usage exists:", usage)
            const userCostUSD = calculateUserCostUSD(selectedModel, usage)
            console.log(
              `ğŸ’° Model: ${selectedModel}, UserID: ${userId}, CostUSD: ${userCostUSD}, Wallet balance before deduction: ${wallet?.balance}`
            )
            if (!wallet || wallet.balance < userCostUSD)
              return NextResponse.json(
                { message: "Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø´Ù…Ø§ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª." },
                { status: 402 }
              )
            if (userCostUSD > 0) {
              console.log("â³ Trying to deduct credits now...")
              await supabaseAdmin.rpc("deduct_credits_and_log_usage", {
                p_user_id: userId,
                p_model_name: selectedModel,
                p_prompt_tokens: usage.prompt_tokens,
                p_completion_tokens: usage.completion_tokens,
                p_cost: userCostUSD
              })
              console.log(
                `âœ… Credits deducted for UserID: ${userId}, CostUSD: ${userCostUSD}`
              )
            }
          }
          return new Response(content, {
            headers: { "Content-Type": "text/plain; charset=utf-8" }
          })
        } catch (error: any) {
          // â¬…ï¸ Ø¬Ø§Ø¨Ø¬Ø§ÛŒÛŒ 1: Ø¨Ù„Ø§Ú© CATCH Ø¨Ù‡ Ø§ÛŒÙ†Ø¬Ø§ Ù…Ù†ØªÙ‚Ù„ Ø´Ø¯ (Ø¯Ø§Ø®Ù„ ELSE)
          // â¬…ï¸ Ø§Ú©Ù†ÙˆÙ† Ø§ÛŒÙ† 'catch' Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª
          console.error("!!! FULL BACKEND ERROR CATCH !!!:", error)
          const errorMessage = error.message || "ÛŒÚ© Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø±Ø® Ø¯Ø§Ø¯"
          const status = error.status || 500
          return NextResponse.json({ message: errorMessage }, { status })
        }
      } // (Ø®Ø· 1082 Ø¯Ø± Ú©Ø¯ Ø§ØµÙ„ÛŒ) - Ø§ÛŒÙ† Ø¢Ú©ÙˆÙ„Ø§Ø¯ØŒ ELSE Ù…Ø¯Ù„ Ù‚Ø¯ÛŒÙ…ÛŒ Ø±Ø§ Ù…ÛŒâ€ŒØ¨Ù†Ø¯Ø¯
    } // (Ø®Ø· 1090 Ø¯Ø± Ú©Ø¯ Ø§ØµÙ„ÛŒ) - Ø§ÛŒÙ† Ø¢Ú©ÙˆÙ„Ø§Ø¯ØŒ ELSE ØºÛŒØ± Ø§Ø³ØªØ±ÛŒÙ… Ø±Ø§ Ù…ÛŒâ€ŒØ¨Ù†Ø¯Ø¯
  } catch (error: any) {
    // (Ø®Ø· 1093 Ø¯Ø± Ú©Ø¯ Ø§ØµÙ„ÛŒ) - Ø§ÛŒÙ† CATCH Ø§ØµÙ„ÛŒ Ø§Ø³Øª
    // (Ø®Ø· 1092 Ø¯Ø± Ú©Ø¯ Ø§ØµÙ„ÛŒ) â¬…ï¸ Ø¬Ø§Ø¨Ø¬Ø§ÛŒÛŒ 2: Ø¢Ú©ÙˆÙ„Ø§Ø¯ Ø§Ø¶Ø§ÙÙ‡ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø­Ø°Ù Ø´Ø¯
    // â¬…ï¸ Ø§Ú©Ù†ÙˆÙ† Ø§ÛŒÙ† 'catch' Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª
    console.error("!!! FULL BACKEND ERROR CATCH !!!:", error)
    const errorMessage = error.message || "ÛŒÚ© Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø±Ø® Ø¯Ø§Ø¯"
    const status = error.status || 500
    return NextResponse.json({ message: errorMessage }, { status })
  }
}
