// ğŸ“ app/api/chat/mcp/route.ts

import { createServerClient } from "@supabase/ssr"
import { cookies } from "next/headers"
import { NextResponse } from "next/server"
import OpenAI from "openai"
import type {
  ChatCompletionCreateParams,
  ChatCompletionCreateParamsStreaming,
  ChatCompletionMessageParam
} from "openai/resources/chat/completions"
import { MODEL_PROMPTS } from "@/lib/build-prompt"
// --- ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ùˆ Ø«Ø§Ø¨Øªâ€ŒÙ‡Ø§ ---
import { getServerProfile } from "@/lib/server/server-chat-helpers"
import { ChatSettings, LLMID } from "@/types"
import { OPENAI_LLM_LIST } from "@/lib/models/llm/openai-llm-list"

export const runtime = "nodejs"

// --- ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ù…Ù† JSON ---
function safeJSONParse(jsonString: string | undefined): any {
  if (!jsonString) return {}
  try {
    return JSON.parse(jsonString)
  } catch (error) {
    console.warn("JSON.parse failed, attempting to fix string:", jsonString)
    try {
      const fixedString = jsonString
        .replace(/(?<!\\)'/g, '"') // ØªØ¨Ø¯ÛŒÙ„ single-quote Ø¨Ù‡ double-quote
        .replace(/([{,]\s*)(\w+)\s*:/g, '$1"$2":') // Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ú©ÙˆØªÛŒØ´Ù† Ø¨Ù‡ Ú©Ù„ÛŒØ¯Ù‡Ø§
        .replace(/,\s*([}\]])/g, "$1") // Ø­Ø°Ù ÙˆÛŒØ±Ú¯ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø¯Ø± Ø§Ù†ØªÙ‡Ø§
      return JSON.parse(fixedString)
    } catch (finalError) {
      console.error("Critical JSON parse error after fix:", finalError)
      return {} // Ø§Ú¯Ø± Ø¨Ø§Ø² Ù‡Ù… Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯ØŒ ÛŒÚ© Ø¢Ø¨Ø¬Ú©Øª Ø®Ø§Ù„ÛŒ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†
    }
  }
}
// ... (Ø¨Ù‚ÛŒÙ‡ ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø´Ù…Ø§) ...
type ExtendedChatSettings = ChatSettings & {
  maxTokens?: number
  max_tokens?: number
}
const pricingMap = new Map(
  OPENAI_LLM_LIST.map(llm => [llm.modelId, llm.pricing])
)
const PROFIT_MARGIN = 1.4
function calculateUserCostUSD(
  modelId: LLMID,
  usage: { prompt_tokens: number; completion_tokens: number }
): number {
  const pricing = pricingMap.get(modelId)
  if (!pricing?.inputCost || !pricing.outputCost) return 0
  const promptCost = (usage.prompt_tokens / 1_000_000) * pricing.inputCost
  const completionCost =
    (usage.completion_tokens / 1_000_000) * pricing.outputCost
  return (promptCost + completionCost) * PROFIT_MARGIN
}
const MODELS_NEED_MAX_COMPLETION = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-nano",
  "gpt-5-mini"
])
const MODELS_THAT_SHOULD_NOT_STREAM = new Set(["gpt-5", "gpt-5-mini"])
const MODEL_MAX_TOKENS: Record<string, number> = {
  "gpt-4o": 8192,
  "gpt-4o-mini": 4096,
  "gpt-5": 12000,
  "gpt-5-mini": 12000
}
function pickMaxTokens(cs: ExtendedChatSettings, modelId: string): number {
  const requestedTokens = cs.maxTokens ?? cs.max_tokens ?? 4096
  const modelLimit = MODEL_MAX_TOKENS[modelId] ?? 4096
  return Math.min(requestedTokens, modelLimit)
}

export async function POST(request: Request) {
  console.log("âœ… Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ Ù…Ø³ÛŒØ± MCP Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯.")
  try {
    const { chatSettings, messages } = await request.json()

    // ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø§ÛŒÙ† Ø¢Ø¯Ø±Ø³ Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ .env.local Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯
    // Ù…Ø«Ø§Ù„: FILE_SERVER_URL=http://your-ip-or-domain:3000
    const fileServerUrl =
      process.env.FILE_SERVER_URL || "http://65.109.206.118:3000"

    const cookieStore = cookies()
    const supabase = createServerClient(
      process.env.NEXT_PUBLIC_SUPABASE_URL!,
      process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
      { cookies: { get: (name: string) => cookieStore.get(name)?.value } }
    )

    const {
      data: { user }
    } = await supabase.auth.getUser()
    if (!user) return new NextResponse("Unauthorized", { status: 401 })

    const { data: wallet } = await supabase
      .from("wallets")
      .select("balance")
      .eq("user_id", user.id)
      .single()
    if (!wallet || wallet.balance <= 0) {
      return NextResponse.json(
        { message: "Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø´Ù…Ø§ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª." },
        { status: 402 }
      )
    }

    const profile = await getServerProfile()
    const openai = new OpenAI({
      apiKey: profile.openai_api_key || "",
      organization: profile.openai_organization_id
    })

    const selectedModel = "gpt-5-nano" as LLMID

    const tools: OpenAI.Chat.Completions.ChatCompletionTool[] = [
      {
        type: "function",
        function: {
          name: "generate_pdf",
          description: "ÛŒÚ© ÙØ§ÛŒÙ„ PDF Ø¨Ø§ ÛŒÚ© Ø¹Ù†ÙˆØ§Ù† Ùˆ ÛŒÚ© Ù…ØªÙ† Ù…Ø­ØªÙˆØ§ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.",
          parameters: {
            type: "object",
            properties: {
              filename: {
                type: "string",
                description: "Ù†Ø§Ù… ÙØ§ÛŒÙ„. Ù…Ø«Ø§Ù„: report.pdf"
              },
              title: {
                type: "string",
                description:
                  "Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ ÙØ§ÛŒÙ„ Ú©Ù‡ Ø¨Ø§ ÙÙˆÙ†Øª Ø¨Ø²Ø±Ú¯ Ùˆ ÙˆØ³Ø·â€ŒÚ†ÛŒÙ† Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯."
              },
              content: {
                type: "string",
                description:
                  "Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ ÙØ§ÛŒÙ„ Ú©Ù‡ Ø¨Ø§ ÙÙˆÙ†Øª Ú©ÙˆÚ†Ú©â€ŒØªØ± Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯."
              },
              titleFontSize: {
                type: "number",
                description: "Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙÙˆÙ†Øª Ø¨Ø±Ø§ÛŒ Ø¹Ù†ÙˆØ§Ù†. Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 25"
              },
              contentFontSize: {
                type: "number",
                description: "Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙÙˆÙ†Øª Ø¨Ø±Ø§ÛŒ Ù…Ø­ØªÙˆØ§. Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 14"
              }
            },
            required: ["filename"]
          }
        }
      },

      {
        type: "function",
        function: {
          name: "generate_word",
          description:
            "ÛŒÚ© ÙØ§ÛŒÙ„ Word (.docx) Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† Ùˆ Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ø´Ø®Øµ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.",
          parameters: {
            type: "object",
            properties: {
              title: { type: "string", description: "Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ ÙØ§ÛŒÙ„ Word" },
              content: { type: "string", description: "Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ ÙØ§ÛŒÙ„ Word" },
              filename: {
                type: "string",
                description: "Ù†Ø§Ù… ÙØ§ÛŒÙ„. Ù…Ø«Ø§Ù„: document.docx"
              }
            },
            required: ["filename"]
          }
        }
      },

      // Replace the generate_excel tool definition in route.ts with this

      {
        type: "function",
        function: {
          name: "generate_excel",
          description:
            "ÛŒÚ© ÙØ§ÛŒÙ„ Excel (.xlsx) Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø®ØµØŒ Ø§Ø³ØªØ§ÛŒÙ„ Ùˆ ÙØ±Ù…ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§ÛŒÙ†Ø§Ù…ÛŒÚ© Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.",
          parameters: {
            type: "object",
            properties: {
              filename: {
                type: "string",
                description: "Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ. Ù…Ø«Ø§Ù„: report.xlsx"
              },
              sheetNames: {
                type: "array",
                description: "Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø§Ø² Ù†Ø§Ù… Ø´ÛŒØªâ€ŒÙ‡Ø§.",
                items: { type: "string" }
              },
              headers: {
                type: "array",
                description:
                  "Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø§Ø² Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ú©Ù‡ Ø¯Ø± Ø±Ø¯ÛŒÙ Ø§ÙˆÙ„ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.",
                items: { type: "string" }
              },
              data: {
                type: "array",
                description:
                  "Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø± Ú©Ø±Ø¯Ù† Ø´ÛŒØª. Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø§Ø² Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ Ø§Ø³Øª Ú©Ù‡ Ù‡Ø± Ø±Ø¯ÛŒÙ Ø®ÙˆØ¯ ÛŒÚ© Ø¢Ø±Ø§ÛŒÙ‡ Ø§Ø² Ù…Ù‚Ø§Ø¯ÛŒØ± Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§Ø³Øª. Ù…Ø«Ø§Ù„: [['Ú©Ø§Ù„Ø§ÛŒ Ø§ÙˆÙ„', 10], ['Ú©Ø§Ù„Ø§ÛŒ Ø¯ÙˆÙ…', 25]]",
                items: {
                  type: "array",
                  items: {
                    anyOf: [{ type: "string" }, { type: "number" }]
                  }
                }
              },
              formulas: {
                type: "array",
                description: "Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø§Ø² ÙØ±Ù…ÙˆÙ„â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø¹Ù…Ø§Ù„ Ø¨Ù‡ Ø´ÛŒØª.",
                items: {
                  type: "object",
                  properties: {
                    type: {
                      type: "string",
                      enum: ["SUM"],
                      description: "Ù†ÙˆØ¹ ÙØ±Ù…ÙˆÙ„."
                    },
                    columnHeader: {
                      type: "string",
                      description: "Ù†Ø§Ù… Ø³ØªÙˆÙ†ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¬Ù…Ø¹ Ø²Ø¯Ù‡ Ø´ÙˆØ¯."
                    },
                    label: {
                      type: "string",
                      description: "Ø¨Ø±Ú†Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Ø¬Ù…Ø¹ Ú©Ù„."
                    },
                    labelColumnHeader: {
                      type: "string",
                      description: "Ù†Ø§Ù… Ø³ØªÙˆÙ†ÛŒ Ú©Ù‡ Ø¨Ø±Ú†Ø³Ø¨ Ø¯Ø± Ø¢Ù† Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯."
                    }
                  }
                }
              },
              conditionalFormatting: {
                type: "array",
                description: "Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø§Ø² Ù‚ÙˆØ§Ù†ÛŒÙ† ÙØ±Ù…Øªâ€ŒØ¯Ù‡ÛŒ Ø´Ø±Ø·ÛŒ.",
                items: {
                  type: "object",
                  properties: {
                    columnHeader: {
                      type: "string",
                      description: "Ù†Ø§Ù… Ø³ØªÙˆÙ†ÛŒ Ú©Ù‡ Ù‚Ø§Ù†ÙˆÙ† Ø±ÙˆÛŒ Ø¢Ù† Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯."
                    },
                    type: {
                      type: "string",
                      enum: ["greaterThan", "lessThan"],
                      description: "Ù†ÙˆØ¹ Ø´Ø±Ø·."
                    },
                    value: {
                      type: "number",
                      description: "Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡."
                    },
                    color: {
                      type: "string",
                      description:
                        "Ú©Ø¯ Ù‡Ú¯Ø² Ø±Ù†Ú¯ Ø¨Ø±Ø§ÛŒ Ù‡Ø§ÛŒÙ„Ø§ÛŒØª. Ù…Ø«Ø§Ù„: 'FFFF0000' Ø¨Ø±Ø§ÛŒ Ù‚Ø±Ù…Ø²."
                    }
                  }
                }
              }
            },
            required: ["filename", "headers"]
          }
        }
      }
    ]

    const finalMessages: ChatCompletionMessageParam[] = [
      {
        role: "system",
        content:
          "Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ ÙÙ‚Ø· Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯. ÙˆØ¸ÛŒÙÙ‡ Ø´Ù…Ø§ Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§Ø³Øª. ÙˆÙ‚ØªÛŒ Ø§Ø¨Ø²Ø§Ø±ÛŒ Ø±Ø§ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØŒ Ù†ØªÛŒØ¬Ù‡ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© Ù¾Ø§Ø³Ø® Ø±ÙˆØ§Ù† Ùˆ Ú©ÙˆØªØ§Ù‡ Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø± Ø§Ø¹Ù„Ø§Ù… Ú©Ù†. Ù‡Ø±Ú¯Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… JSON Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ù†Ø¯Ù‡. **Ù‚Ø§Ù†ÙˆÙ† Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù…: ÙˆÙ‚ØªÛŒ Ø®ÙˆØ¯Øª Ù…Ø­ØªÙˆØ§ÛŒ ÛŒÚ© ÙØ§ÛŒÙ„ (Ù…Ø§Ù†Ù†Ø¯ Ù†Ø§Ù…Ù‡ ÛŒØ§ Ú¯Ø²Ø§Ø±Ø´) Ø±Ø§ Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³ÛŒØŒ Ø¨Ø§ÛŒØ¯ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù‡Ù…Ø§Ù† Ù…ØªÙ† Ø±Ø§ Ø¯Ø± Ù¾Ø§Ø±Ø§Ù…ØªØ± `content` Ø§Ø¨Ø²Ø§Ø± Ù…Ø±Ø¨ÙˆØ·Ù‡ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒ. Ù¾Ø§Ø±Ø§Ù…ØªØ± `title` Ø±Ø§ Ù‡Ù… Ø§Ø² Ù…ÙˆØ¶ÙˆØ¹ Ø§ØµÙ„ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†.**"
      },
      ...(Array.isArray(messages) ? messages : [])
    ]

    const cs = chatSettings as ExtendedChatSettings
    const maxTokens = pickMaxTokens(cs, selectedModel)
    const temp = typeof cs.temperature === "number" ? cs.temperature : 1
    const useStream = !MODELS_THAT_SHOULD_NOT_STREAM.has(selectedModel)

    if (useStream) {
      const payload: ChatCompletionCreateParamsStreaming = {
        model: selectedModel,
        messages: finalMessages,
        stream: true,
        temperature: temp,
        tools: tools,
        tool_choice: "auto",
        ...(MODELS_NEED_MAX_COMPLETION.has(selectedModel)
          ? { max_completion_tokens: maxTokens }
          : { max_tokens: maxTokens })
      }

      const encoder = new TextEncoder()
      const readableStream = new ReadableStream({
        async start(controller) {
          try {
            const stream = await openai.chat.completions.create(payload)

            let hasToolCall = false
            const toolCalls: OpenAI.Chat.Completions.ChatCompletionMessageToolCall[] =
              []

            for await (const chunk of stream) {
              const delta = chunk.choices[0]?.delta
              if (delta?.content) {
                controller.enqueue(encoder.encode(delta.content))
              }

              if (delta?.tool_calls) {
                hasToolCall = true
                for (const toolCallDelta of delta.tool_calls) {
                  const index = toolCallDelta.index
                  // âœ…âœ…âœ… Ø§ØµÙ„Ø§Ø­ÛŒÙ‡ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø®Ø·Ø§ÛŒ ØªØ§ÛŒÙ¾â€ŒØ§Ø³Ú©Ø±ÛŒÙ¾Øª Ø§ÛŒÙ†Ø¬Ø§Ø³Øª âœ…âœ…âœ…
                  if (!toolCalls[index]) {
                    toolCalls[index] = {
                      id: "",
                      type: "function",
                      function: { name: "", arguments: "" }
                    }
                  }

                  const currentTool = toolCalls[index]
                  if (currentTool.type === "function") {
                    if (toolCallDelta.id) currentTool.id = toolCallDelta.id
                    if (toolCallDelta.function?.name)
                      currentTool.function.name = toolCallDelta.function.name
                    if (toolCallDelta.function?.arguments)
                      currentTool.function.arguments +=
                        toolCallDelta.function.arguments
                  }
                }
              }
            }

            if (hasToolCall) {
              const newMessages: ChatCompletionMessageParam[] = [
                ...finalMessages,
                { role: "assistant", tool_calls: toolCalls }
              ]

              for (const toolCall of toolCalls) {
                if (toolCall.type === "function") {
                  const functionName = toolCall.function.name
                  const functionArgs = safeJSONParse(
                    toolCall.function.arguments
                  )

                  const apiResponse = await fetch(
                    `${fileServerUrl}/tools/${functionName}`,
                    {
                      method: "POST",
                      headers: { "Content-Type": "application/json" },
                      body: JSON.stringify(functionArgs)
                    }
                  )

                  if (!apiResponse.ok) {
                    throw new Error(
                      `Error from file server: ${await apiResponse.text()}`
                    )
                  }

                  const result = await apiResponse.json()

                  newMessages.push({
                    tool_call_id: toolCall.id,
                    role: "tool",
                    content: JSON.stringify(result)
                  })
                }
              }

              const finalStream = await openai.chat.completions.create({
                model: selectedModel,
                messages: newMessages,
                stream: true
              })

              for await (const finalChunk of finalStream) {
                const content = finalChunk.choices[0]?.delta?.content || ""
                if (content) {
                  controller.enqueue(encoder.encode(content))
                }
              }
            }
          } catch (err: any) {
            console.error("âŒ ERROR INSIDE MCP STREAM:", err)
            controller.enqueue(encoder.encode(`Ø®Ø·Ø§ÛŒ Ø³Ø±ÙˆØ±: ${err.message}`))
          } finally {
            controller.close()
          }
        }
      })
      return new Response(readableStream, {
        headers: { "Content-Type": "text/event-stream; charset=utf-8" }
      })
    } else {
      // --- Ø­Ø§Ù„Øª ØºÛŒØ± Ø§Ø³ØªØ±ÛŒÙ… ---
      const payload: ChatCompletionCreateParams = {
        model: selectedModel,
        messages: finalMessages,
        stream: false,
        temperature: temp,
        tools: tools,
        tool_choice: "auto",
        ...(MODELS_NEED_MAX_COMPLETION.has(selectedModel)
          ? { max_completion_tokens: maxTokens }
          : { max_tokens: maxTokens })
      }

      const response = await openai.chat.completions.create(payload)
      const responseMessage = response.choices[0].message

      if (responseMessage.tool_calls) {
        const newMessages: ChatCompletionMessageParam[] = [
          ...finalMessages,
          responseMessage
        ]

        for (const toolCall of responseMessage.tool_calls) {
          if (toolCall.type === "function") {
            const functionName = toolCall.function.name
            const functionArgs = safeJSONParse(toolCall.function.arguments)

            console.log("ğŸ“Œ Function Name:", functionName)
            console.log("ğŸ“¦ Function Args:", functionArgs)
            console.log("ğŸ“¦ RAW Function Args:", toolCall.function?.arguments)

            const apiResponse = await fetch(
              `${fileServerUrl}/tools/${functionName}`,
              {
                method: "POST",
                headers: { "Content-Type": "application/json" },
                body: JSON.stringify(functionArgs)
              }
            )

            if (!apiResponse.ok) {
              throw new Error(
                `Error from file server: ${await apiResponse.text()}`
              )
            }

            const result = await apiResponse.json()

            newMessages.push({
              tool_call_id: toolCall.id,
              role: "tool",
              content: JSON.stringify(result)
            })
          }
        }

        const finalResponse = await openai.chat.completions.create({
          model: selectedModel,
          messages: newMessages
        })
        const finalContent =
          finalResponse.choices[0].message.content ?? "Ø§Ø¨Ø²Ø§Ø± Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¬Ø±Ø§ Ø´Ø¯."
        const finalUsage = finalResponse.usage

        if (finalUsage) {
          const userCostUSD = calculateUserCostUSD(selectedModel, finalUsage)
          if (userCostUSD > 0 && wallet && wallet.balance >= userCostUSD) {
            await supabase.rpc("deduct_credits_and_log_usage", {
              p_user_id: user.id,
              p_model_name: selectedModel,
              p_prompt_tokens: finalUsage.prompt_tokens,
              p_completion_tokens: finalUsage.completion_tokens,
              p_cost: userCostUSD
            })
          }
        }
        return new Response(finalContent, {
          headers: { "Content-Type": "text/plain; charset=utf-8" }
        })
      } else {
        const content = response.choices[0].message.content ?? ""
        const usage = response.usage
        if (usage) {
          const userCostUSD = calculateUserCostUSD(selectedModel, usage)
          if (userCostUSD > 0 && wallet && wallet.balance >= userCostUSD) {
            await supabase.rpc("deduct_credits_and_log_usage", {
              p_user_id: user.id,
              p_model_name: selectedModel,
              p_prompt_tokens: usage.prompt_tokens,
              p_completion_tokens: usage.completion_tokens,
              p_cost: userCostUSD
            })
          }
        }
        return new Response(content, {
          headers: { "Content-Type": "text/plain; charset=utf-8" }
        })
      }
    }
  } catch (error: any) {
    console.error("!!! MCP ROUTE ERROR CATCH !!!:", error)
    const errorMessage = error.message || "ÛŒÚ© Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¯Ø± Ù…Ø³ÛŒØ± MCP Ø±Ø® Ø¯Ø§Ø¯"
    const status = error.status || 500
    return NextResponse.json({ message: errorMessage }, { status })
  }
}
