// ğŸ“ app/api/chat/mcp/route.ts

import { createClient as createSSRClient } from "@/lib/supabase/server"
import { cookies } from "next/headers"

import OpenAI from "openai"
import type {
  ChatCompletionCreateParamsStreaming,
  ChatCompletionMessageParam
} from "openai/resources/chat/completions"

// --- Your Helper Functions ---
import { getServerProfile } from "@/lib/server/server-chat-helpers"
import { ChatSettings, LLMID } from "@/types"
import { OPENAI_LLM_LIST } from "@/lib/models/llm/openai-llm-list"
import { modelsWithRial } from "@/app/checkout/pricing"
import { NextRequest, NextResponse } from "next/server"

import { createClient } from "@supabase/supabase-js"

import jwt from "jsonwebtoken"
export const runtime = "nodejs"

function safeJSONParse(jsonString: string | undefined): any {
  if (!jsonString) return {}
  try {
    return JSON.parse(jsonString)
  } catch (error) {
    console.warn("JSON.parse failed, attempting to fix string:", jsonString)
    try {
      const fixedString = jsonString
        .replace(/(?<!\\)'/g, '"')
        .replace(/([{,]\s*)(\w+)\s*:/g, '$1"$2":')
        .replace(/,\s*([}\]])/g, "$1")
      return JSON.parse(fixedString)
    } catch (finalError) {
      console.error("Critical JSON parse error after fix:", finalError)
      return {}
    }
  }
}
type ExtendedChatSettings = ChatSettings & {
  maxTokens?: number
  max_tokens?: number
}
const pricingMap = new Map(
  OPENAI_LLM_LIST.map(llm => [llm.modelId, llm.pricing])
)
const PROFIT_MARGIN = 1.4

function calculateUserCostUSD(
  modelId: string,
  usage: { prompt_tokens: number; completion_tokens: number }
): number {
  const model = modelsWithRial.find(m => m.id === modelId)
  if (!model) return 0

  const promptCost =
    (usage.prompt_tokens / 1_000_000) * model.inputPricePer1MTokenUSD
  const completionCost =
    (usage.completion_tokens / 1_000_000) * model.outputPricePer1MTokenUSD

  return (promptCost + completionCost) * PROFIT_MARGIN
}
const MODELS_NEED_MAX_COMPLETION = new Set([
  "gpt-4o",
  "gpt-4o-mini",
  "gpt-5",
  "gpt-5-nano",
  "gpt-5-mini"
])
const MODEL_MAX_TOKENS: Record<string, number> = {
  "gpt-4o": 8192,
  "gpt-4o-mini": 4096,
  "gpt-5": 12000,
  "gpt-5-mini": 12000
}
function pickMaxTokens(cs: ExtendedChatSettings, modelId: string): number {
  const requestedTokens = cs.maxTokens ?? cs.max_tokens ?? 4096
  const modelLimit = MODEL_MAX_TOKENS[modelId] ?? 4096
  return Math.min(requestedTokens, modelLimit)
}

export async function POST(request: Request) {
  console.log("âœ… Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ Ù…Ø³ÛŒØ± MCP Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯.")
  try {
    const contentType = request.headers.get("Content-Type") || ""
    let chatSettings: ChatSettings
    let messages: ChatCompletionMessageParam[]
    let file: File | null = null

    if (contentType.includes("multipart/form-data")) {
      console.log("Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø±Ø®ÙˆØ§Ø³Øª multipart/form-data...")
      const formData = await request.formData()
      chatSettings = JSON.parse(formData.get("chatSettings") as string)
      messages = JSON.parse(formData.get("messages") as string)
      file = formData.get("file") as File | null
    } else if (contentType.includes("application/json")) {
      console.log("Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø±Ø®ÙˆØ§Ø³Øª application/json...")
      const body = await request.json()
      chatSettings = body.chatSettings
      messages = body.messages
    } else {
      return new NextResponse(`Unsupported Content-Type: ${contentType}`, {
        status: 415
      })
    }

    const fileServerUrl =
      process.env.FILE_SERVER_URL || "http://65.109.206.118:3000"
    const authHeader = request.headers.get("Authorization")
    if (!authHeader || !authHeader.startsWith("Bearer ")) {
      return new NextResponse("Unauthorized: Missing Bearer token", {
        status: 401
      })
    }
    const token = authHeader.split(" ")[1]

    let userId: string

    // Û±. Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¯Ø³ØªÛŒ ØªÙˆÚ©Ù† Ø¨Ø§ JWT_SECRET
    try {
      if (!process.env.SUPABASE_JWT_SECRET) {
        throw new Error("SUPABASE_JWT_SECRET is not set on server!")
      }
      const decodedToken = jwt.verify(
        token,
        process.env.SUPABASE_JWT_SECRET
      ) as jwt.JwtPayload

      if (!decodedToken.sub) {
        throw new Error("Invalid token: No 'sub' (user ID) found.")
      }
      userId = decodedToken.sub // 'sub' Ù‡Ù…Ø§Ù† User ID Ø§Ø³Øª
      console.log(`[Agent] âœ… Token MANUALLY verified! User ID: ${userId}`)
    } catch (err: any) {
      console.error("[Agent] âŒ Manual JWT Verification Failed:", err.message)
      return new NextResponse(
        `Unauthorized: Manual verification failed: ${err.message}`,
        { status: 401 }
      )
    }

    // Û². Ø³Ø§Ø®Øª Ú©Ù„Ø§ÛŒÙ†Øª Ø§Ø¯Ù…ÛŒÙ† (Admin) Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† Ø¢Ø¨Ø¬Ú©Øª Ú©Ø§Ù…Ù„ User
    if (!process.env.SUPABASE_SERVICE_ROLE_KEY) {
      throw new Error("SUPABASE_SERVICE_ROLE_KEY is not set on server!")
    }

    const supabaseAdmin = createClient(
      process.env.NEXT_PUBLIC_SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!
    )

    const {
      data: { user },
      error: adminError
    } = await supabaseAdmin.auth.admin.getUserById(userId)

    if (adminError || !user) {
      console.error(
        "[Agent] âŒ Admin client failed to get user:",
        adminError?.message
      )
      return new NextResponse(
        `Unauthorized: User not found with admin client: ${adminError?.message}`,
        { status: 401 }
      )
    }
    console.log(`[Agent] âœ… Full user object retrieved for: ${user.email}`)
    const cookieStore = cookies()
    const supabase = createSSRClient(cookieStore)

    const { data: wallet } = await supabase
      .from("wallets")
      .select("balance")
      .eq("user_id", user.id)
      .single()
    if (!wallet || wallet.balance <= 0) {
      return NextResponse.json(
        { message: "Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø´Ù…Ø§ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª." },
        { status: 402 }
      )
    }

    const profile = await getServerProfile(userId, supabaseAdmin) // âœ… Ø§ØµÙ„Ø§Ø­ userId
    const openai = new OpenAI({ apiKey: profile.openai_api_key || "" })

    const selectedModel = "gpt-5-nano" as LLMID

    if (file) {
      const fileBuffer = Buffer.from(await file.arrayBuffer())
      const base64Image = fileBuffer.toString("base64")
      const mimeType = file.type
      const lastUserMessage = messages[messages.length - 1]
      if (lastUserMessage && lastUserMessage.role === "user") {
        const newContent: any[] = []
        if (
          lastUserMessage.content &&
          typeof lastUserMessage.content === "string"
        ) {
          newContent.push({ type: "text", text: lastUserMessage.content })
        }
        newContent.push({
          type: "image_url",
          image_url: { url: `data:${mimeType};base64,${base64Image}` }
        })
        lastUserMessage.content = newContent
      }
    }

    // âœ…âœ…âœ… FINAL AND CORRECTED TOOL DEFINITIONS âœ…âœ…âœ…
    const tools: OpenAI.Chat.Completions.ChatCompletionTool[] = [
      {
        type: "function",
        function: {
          name: "generate_pdf",
          description: "ÛŒÚ© ÙØ§ÛŒÙ„ PDF Ø¨Ø§ ÛŒÚ© Ø¹Ù†ÙˆØ§Ù† Ùˆ ÛŒÚ© Ù…ØªÙ† Ù…Ø­ØªÙˆØ§ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.",
          parameters: {
            type: "object",
            properties: {
              filename: {
                type: "string",
                description: "Ù†Ø§Ù… ÙØ§ÛŒÙ„. Ù…Ø«Ø§Ù„: report.pdf"
              },
              title: {
                type: "string",
                description:
                  "Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ ÙØ§ÛŒÙ„ Ú©Ù‡ Ø¨Ø§ ÙÙˆÙ†Øª Ø¨Ø²Ø±Ú¯ Ùˆ ÙˆØ³Ø·â€ŒÚ†ÛŒÙ† Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯."
              },
              content: {
                type: "string",
                description:
                  "Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ ÙØ§ÛŒÙ„ Ú©Ù‡ Ø¨Ø§ ÙÙˆÙ†Øª Ú©ÙˆÚ†Ú©â€ŒØªØ± Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯."
              },
              titleFontSize: {
                type: "number",
                description: "Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙÙˆÙ†Øª Ø¨Ø±Ø§ÛŒ Ø¹Ù†ÙˆØ§Ù†. Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 25"
              },
              contentFontSize: {
                type: "number",
                description: "Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙÙˆÙ†Øª Ø¨Ø±Ø§ÛŒ Ù…Ø­ØªÙˆØ§. Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 14"
              }
            },
            required: ["filename"]
          }
        }
      },

      {
        type: "function",
        function: {
          name: "generate_word",
          description:
            "ÛŒÚ© ÙØ§ÛŒÙ„ Word (.docx) Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† Ùˆ Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ø´Ø®Øµ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.",
          parameters: {
            type: "object",
            properties: {
              title: { type: "string", description: "Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ ÙØ§ÛŒÙ„ Word" },
              content: { type: "string", description: "Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ ÙØ§ÛŒÙ„ Word" },
              filename: {
                type: "string",
                description: "Ù†Ø§Ù… ÙØ§ÛŒÙ„. Ù…Ø«Ø§Ù„: document.docx"
              }
            },
            required: ["filename"]
          }
        }
      },

      // Replace the generate_excel tool definition in route.ts with this

      {
        type: "function",
        function: {
          name: "generate_excel",
          description:
            "ÛŒÚ© ÙØ§ÛŒÙ„ Excel (.xlsx) Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø®ØµØŒ Ø§Ø³ØªØ§ÛŒÙ„ Ùˆ ÙØ±Ù…ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§ÛŒÙ†Ø§Ù…ÛŒÚ© Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.",
          parameters: {
            type: "object",
            properties: {
              filename: {
                type: "string",
                description: "Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ. Ù…Ø«Ø§Ù„: report.xlsx"
              },
              sheetNames: {
                type: "array",
                description: "Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø§Ø² Ù†Ø§Ù… Ø´ÛŒØªâ€ŒÙ‡Ø§.",
                items: { type: "string" }
              },
              headers: {
                type: "array",
                description:
                  "Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø§Ø² Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ú©Ù‡ Ø¯Ø± Ø±Ø¯ÛŒÙ Ø§ÙˆÙ„ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.",
                items: { type: "string" }
              },
              data: {
                type: "array",
                description:
                  "Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø± Ú©Ø±Ø¯Ù† Ø´ÛŒØª. Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø§Ø² Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ Ø§Ø³Øª Ú©Ù‡ Ù‡Ø± Ø±Ø¯ÛŒÙ Ø®ÙˆØ¯ ÛŒÚ© Ø¢Ø±Ø§ÛŒÙ‡ Ø§Ø² Ù…Ù‚Ø§Ø¯ÛŒØ± Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§Ø³Øª. Ù…Ø«Ø§Ù„: [['Ú©Ø§Ù„Ø§ÛŒ Ø§ÙˆÙ„', 10], ['Ú©Ø§Ù„Ø§ÛŒ Ø¯ÙˆÙ…', 25]]",
                items: {
                  type: "array",
                  items: {
                    anyOf: [{ type: "string" }, { type: "number" }]
                  }
                }
              },
              formulas: {
                type: "array",
                description: "Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø§Ø² ÙØ±Ù…ÙˆÙ„â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø¹Ù…Ø§Ù„ Ø¨Ù‡ Ø´ÛŒØª.",
                items: {
                  type: "object",
                  properties: {
                    type: {
                      type: "string",
                      enum: ["SUM"],
                      description: "Ù†ÙˆØ¹ ÙØ±Ù…ÙˆÙ„."
                    },
                    columnHeader: {
                      type: "string",
                      description: "Ù†Ø§Ù… Ø³ØªÙˆÙ†ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¬Ù…Ø¹ Ø²Ø¯Ù‡ Ø´ÙˆØ¯."
                    },
                    label: {
                      type: "string",
                      description: "Ø¨Ø±Ú†Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Ø¬Ù…Ø¹ Ú©Ù„."
                    },
                    labelColumnHeader: {
                      type: "string",
                      description: "Ù†Ø§Ù… Ø³ØªÙˆÙ†ÛŒ Ú©Ù‡ Ø¨Ø±Ú†Ø³Ø¨ Ø¯Ø± Ø¢Ù† Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯."
                    }
                  }
                }
              },
              conditionalFormatting: {
                type: "array",
                description: "Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø§Ø² Ù‚ÙˆØ§Ù†ÛŒÙ† ÙØ±Ù…Øªâ€ŒØ¯Ù‡ÛŒ Ø´Ø±Ø·ÛŒ.",
                items: {
                  type: "object",
                  properties: {
                    columnHeader: {
                      type: "string",
                      description: "Ù†Ø§Ù… Ø³ØªÙˆÙ†ÛŒ Ú©Ù‡ Ù‚Ø§Ù†ÙˆÙ† Ø±ÙˆÛŒ Ø¢Ù† Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯."
                    },
                    type: {
                      type: "string",
                      enum: ["greaterThan", "lessThan"],
                      description: "Ù†ÙˆØ¹ Ø´Ø±Ø·."
                    },
                    value: {
                      type: "number",
                      description: "Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡."
                    },
                    color: {
                      type: "string",
                      description:
                        "Ú©Ø¯ Ù‡Ú¯Ø² Ø±Ù†Ú¯ Ø¨Ø±Ø§ÛŒ Ù‡Ø§ÛŒÙ„Ø§ÛŒØª. Ù…Ø«Ø§Ù„: 'FFFF0000' Ø¨Ø±Ø§ÛŒ Ù‚Ø±Ù…Ø²."
                    }
                  }
                }
              }
            },
            required: ["filename", "headers"]
          }
        }
      }
    ]

    const finalMessages: ChatCompletionMessageParam[] = [
      {
        role: "system",
        content:
          "Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ùˆ Ú†Ù†Ø¯ÙˆØ¬Ù‡ÛŒ (multi-modal) Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ ÙÙ‚Ø· Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯. ÙˆØ¸ÛŒÙÙ‡ Ø´Ù…Ø§ Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù…ØªÙ†ÛŒ Ùˆ ØªØµÙˆÛŒØ±ÛŒ Ú©Ø§Ø±Ø¨Ø± Ø§Ø³Øª. Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± ØªØµÙˆÛŒØ±ÛŒ Ø§Ø±Ø³Ø§Ù„ Ú©Ø±Ø¯ØŒ Ø§Ø² Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ø®ÙˆØ¯ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù† Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¨Ù‡ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§Ø¨Ø²Ø§Ø± Ù…Ù†Ø§Ø³Ø¨ (Ù…Ø§Ù†Ù†Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ± `data` Ø¯Ø± Ø§Ø¨Ø²Ø§Ø± Ø§Ú©Ø³Ù„) Ø§Ø±Ø³Ø§Ù„ Ú©Ù†. Ù‡Ù…ÛŒØ´Ù‡ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ùˆ Ø¨Ø¯ÙˆÙ† Ø³ÙˆØ§Ù„ ØªØ§ÛŒÛŒØ¯ÛŒ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ú©Ù†."
      },
      ...messages
    ]

    const cs = chatSettings as ExtendedChatSettings
    const maxTokens = pickMaxTokens(cs, selectedModel)
    const temp = typeof cs.temperature === "number" ? cs.temperature : 1
    let usage:
      | {
          prompt_tokens: number
          completion_tokens: number
          total_tokens?: number
        }
      | undefined

    const payload: ChatCompletionCreateParamsStreaming = {
      model: selectedModel,
      messages: finalMessages,
      stream: true,
      temperature: temp,
      tools: tools,
      tool_choice: "auto",
      ...(MODELS_NEED_MAX_COMPLETION.has(selectedModel)
        ? { max_completion_tokens: maxTokens }
        : { max_tokens: maxTokens })
    }

    const encoder = new TextEncoder()
    const readableStream = new ReadableStream({
      async start(controller) {
        try {
          const stream = await openai.chat.completions.create(payload)

          let hasToolCall = false
          const toolCalls: OpenAI.Chat.Completions.ChatCompletionMessageToolCall[] =
            []

          for await (const chunk of stream) {
            const delta = chunk.choices[0]?.delta

            if (delta?.content) {
              controller.enqueue(encoder.encode(delta.content))
            }
            if (chunk.usage) {
              usage = {
                prompt_tokens: chunk.usage.prompt_tokens,
                completion_tokens: chunk.usage.completion_tokens,
                total_tokens: chunk.usage.total_tokens
              }
            }

            if (delta?.tool_calls) {
              hasToolCall = true
              for (const toolCallDelta of delta.tool_calls) {
                const index = toolCallDelta.index
                if (!toolCalls[index]) {
                  toolCalls[index] = {
                    id: "",
                    type: "function",
                    function: { name: "", arguments: "" }
                  }
                }
                const currentTool = toolCalls[index]
                if (currentTool.type === "function") {
                  if (toolCallDelta.id) currentTool.id = toolCallDelta.id
                  if (toolCallDelta.function?.name)
                    currentTool.function.name = toolCallDelta.function.name
                  if (toolCallDelta.function?.arguments)
                    currentTool.function.arguments +=
                      toolCallDelta.function.arguments
                }
              }
            }
          }
          if (!usage) {
            console.log(
              "âš ï¸ No usage from stream. Using fallback non-stream request for cost calculation."
            )
            const fallbackResponse = await openai.chat.completions.create({
              model: selectedModel,
              messages: finalMessages,
              temperature: temp,
              stream: false
            })
            if (fallbackResponse.usage) {
              usage = {
                prompt_tokens: fallbackResponse.usage.prompt_tokens,
                completion_tokens: fallbackResponse.usage.completion_tokens,
                total_tokens: fallbackResponse.usage.total_tokens
              }

              // ğŸ”¥ Log AFTER usage is available
              console.log(
                `ğŸ’° [BEFORE DEDUCT] User ${user.id} balance: ${wallet.balance}, cost to deduct: $${calculateUserCostUSD(selectedModel, usage)}`
              )
            }
          }

          if (hasToolCall) {
            const newMessages: ChatCompletionMessageParam[] = [
              ...finalMessages,
              { role: "assistant", tool_calls: toolCalls }
            ]
            for (const toolCall of toolCalls) {
              if (toolCall.type === "function") {
                const functionName = toolCall.function.name
                const functionArgs = safeJSONParse(toolCall.function.arguments)
                const apiResponse = await fetch(
                  `${fileServerUrl}/tools/${functionName}`,
                  {
                    method: "POST",
                    headers: { "Content-Type": "application/json" },
                    body: JSON.stringify(functionArgs)
                  }
                )
                if (!apiResponse.ok) {
                  throw new Error(
                    `Error from file server: ${await apiResponse.text()}`
                  )
                }
                const result = await apiResponse.json()
                newMessages.push({
                  tool_call_id: toolCall.id,
                  role: "tool",
                  content: JSON.stringify(result)
                })
              }
            }
            const finalStream = await openai.chat.completions.create({
              model: selectedModel,
              messages: newMessages,
              stream: true
            })
            for await (const finalChunk of finalStream) {
              const content = finalChunk.choices[0]?.delta?.content || ""
              if (content) {
                controller.enqueue(encoder.encode(content))
              }
            }
            // Ø§Ú¯Ø± usage Ù†ÛŒØ§Ù…Ø¯ØŒ fallback Ø¨Ø²Ù†
            if (usage && wallet) {
              const userCostUSD = calculateUserCostUSD(selectedModel, usage)
              if (userCostUSD > 0) {
                await supabase.rpc("deduct_credits_and_log_usage", {
                  p_user_id: user.id,
                  p_model_name: selectedModel,
                  p_prompt_tokens: usage.prompt_tokens,
                  p_completion_tokens: usage.completion_tokens,
                  p_cost: userCostUSD
                })

                const { data: newWallet, error } = await supabase
                  .from("wallets")
                  .select("balance")
                  .eq("user_id", user.id)
                  .single()

                if (error)
                  console.error("âŒ Error fetching new balance:", error)
                else
                  console.log(
                    `âœ… [AFTER DEDUCT] User ${user.id} new balance: ${newWallet.balance}`
                  )
              }
            }
          }
        } catch (err: any) {
          console.error("âŒ ERROR INSIDE MCP STREAM:", err)
          controller.enqueue(encoder.encode(`Ø®Ø·Ø§ÛŒ Ø³Ø±ÙˆØ±: ${err.message}`))
        } finally {
          controller.close()
        }
      }
    })
    return new Response(readableStream, {
      headers: { "Content-Type": "text/event-stream; charset=utf-8" }
    })
  } catch (error: any) {
    console.error("!!! MCP ROUTE ERROR CATCH !!!:", error)
    const errorMessage = error.message || "ÛŒÚ© Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¯Ø± Ù…Ø³ÛŒØ± MCP Ø±Ø® Ø¯Ø§Ø¯"
    const status = error.status || 500
    return NextResponse.json({ message: errorMessage }, { status })
  }
}
